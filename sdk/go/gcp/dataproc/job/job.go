// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

// nolint: lll
package job

import (
	"reflect"

	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/go/pulumi"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobHadoopConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobHadoopConfigLoggingConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobHiveConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobPigConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobPigConfigLoggingConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobPlacement"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobPysparkConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobPysparkConfigLoggingConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobReference"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobScheduling"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobSparkConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobSparkConfigLoggingConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobSparksqlConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobSparksqlConfigLoggingConfig"
	"https:/github.com/pulumi/pulumi-gcp/dataproc/JobStatus"
)

// Manages a job resource within a Dataproc cluster within GCE. For more information see
// [the official dataproc documentation](https://cloud.google.com/dataproc/).
// 
// !> **Note:** This resource does not support 'update' and changing any attributes will cause the resource to be recreated.
// 
// > This content is derived from https://github.com/terraform-providers/terraform-provider-google/blob/master/website/docs/r/dataproc_job.html.markdown.
type Job struct {
	pulumi.CustomResourceState

	// If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri pulumi.StringOutput `pulumi:"driverControlsFilesUri"`
	// A URI pointing to the location of the stdout of the job's driver program.
	DriverOutputResourceUri pulumi.StringOutput `pulumi:"driverOutputResourceUri"`
	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrOutput `pulumi:"forceDelete"`
	HadoopConfig dataprocJobHadoopConfig.JobHadoopConfigPtrOutput `pulumi:"hadoopConfig"`
	HiveConfig dataprocJobHiveConfig.JobHiveConfigPtrOutput `pulumi:"hiveConfig"`
	// The list of labels (key/value pairs) to add to the job.
	Labels pulumi.StringMapOutput `pulumi:"labels"`
	PigConfig dataprocJobPigConfig.JobPigConfigPtrOutput `pulumi:"pigConfig"`
	Placement dataprocJobPlacement.JobPlacementOutput `pulumi:"placement"`
	// The project in which the `cluster` can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project pulumi.StringOutput `pulumi:"project"`
	PysparkConfig dataprocJobPysparkConfig.JobPysparkConfigPtrOutput `pulumi:"pysparkConfig"`
	Reference dataprocJobReference.JobReferenceOutput `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to `global`.
	Region pulumi.StringPtrOutput `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling dataprocJobScheduling.JobSchedulingPtrOutput `pulumi:"scheduling"`
	SparkConfig dataprocJobSparkConfig.JobSparkConfigPtrOutput `pulumi:"sparkConfig"`
	SparksqlConfig dataprocJobSparksqlConfig.JobSparksqlConfigPtrOutput `pulumi:"sparksqlConfig"`
	Status dataprocJobStatus.JobStatusOutput `pulumi:"status"`
}

// NewJob registers a new resource with the given unique name, arguments, and options.
func NewJob(ctx *pulumi.Context,
	name string, args *JobArgs, opts ...pulumi.ResourceOption) (*Job, error) {
	if args == nil || args.Placement == nil {
		return nil, errors.New("missing required argument 'Placement'")
	}
	if args == nil {
		args = &JobArgs{}
	}
	var resource Job
	err := ctx.RegisterResource("gcp:dataproc/job:Job", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetJob gets an existing Job resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetJob(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *JobState, opts ...pulumi.ResourceOption) (*Job, error) {
	var resource Job
	err := ctx.ReadResource("gcp:dataproc/job:Job", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Job resources.
type jobState struct {
	// If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri *string `pulumi:"driverControlsFilesUri"`
	// A URI pointing to the location of the stdout of the job's driver program.
	DriverOutputResourceUri *string `pulumi:"driverOutputResourceUri"`
	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete *bool `pulumi:"forceDelete"`
	HadoopConfig *dataprocJobHadoopConfig.JobHadoopConfig `pulumi:"hadoopConfig"`
	HiveConfig *dataprocJobHiveConfig.JobHiveConfig `pulumi:"hiveConfig"`
	// The list of labels (key/value pairs) to add to the job.
	Labels map[string]string `pulumi:"labels"`
	PigConfig *dataprocJobPigConfig.JobPigConfig `pulumi:"pigConfig"`
	Placement *dataprocJobPlacement.JobPlacement `pulumi:"placement"`
	// The project in which the `cluster` can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project *string `pulumi:"project"`
	PysparkConfig *dataprocJobPysparkConfig.JobPysparkConfig `pulumi:"pysparkConfig"`
	Reference *dataprocJobReference.JobReference `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to `global`.
	Region *string `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling *dataprocJobScheduling.JobScheduling `pulumi:"scheduling"`
	SparkConfig *dataprocJobSparkConfig.JobSparkConfig `pulumi:"sparkConfig"`
	SparksqlConfig *dataprocJobSparksqlConfig.JobSparksqlConfig `pulumi:"sparksqlConfig"`
	Status *dataprocJobStatus.JobStatus `pulumi:"status"`
}

type JobState struct {
	// If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri pulumi.StringPtrInput
	// A URI pointing to the location of the stdout of the job's driver program.
	DriverOutputResourceUri pulumi.StringPtrInput
	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrInput
	HadoopConfig dataprocJobHadoopConfig.JobHadoopConfigPtrInput
	HiveConfig dataprocJobHiveConfig.JobHiveConfigPtrInput
	// The list of labels (key/value pairs) to add to the job.
	Labels pulumi.StringMapInput
	PigConfig dataprocJobPigConfig.JobPigConfigPtrInput
	Placement dataprocJobPlacement.JobPlacementPtrInput
	// The project in which the `cluster` can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project pulumi.StringPtrInput
	PysparkConfig dataprocJobPysparkConfig.JobPysparkConfigPtrInput
	Reference dataprocJobReference.JobReferencePtrInput
	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to `global`.
	Region pulumi.StringPtrInput
	// Optional. Job scheduling configuration.
	Scheduling dataprocJobScheduling.JobSchedulingPtrInput
	SparkConfig dataprocJobSparkConfig.JobSparkConfigPtrInput
	SparksqlConfig dataprocJobSparksqlConfig.JobSparksqlConfigPtrInput
	Status dataprocJobStatus.JobStatusPtrInput
}

func (JobState) ElementType() reflect.Type {
	return reflect.TypeOf((*jobState)(nil)).Elem()
}

type jobArgs struct {
	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete *bool `pulumi:"forceDelete"`
	HadoopConfig *dataprocJobHadoopConfig.JobHadoopConfig `pulumi:"hadoopConfig"`
	HiveConfig *dataprocJobHiveConfig.JobHiveConfig `pulumi:"hiveConfig"`
	// The list of labels (key/value pairs) to add to the job.
	Labels map[string]string `pulumi:"labels"`
	PigConfig *dataprocJobPigConfig.JobPigConfig `pulumi:"pigConfig"`
	Placement dataprocJobPlacement.JobPlacement `pulumi:"placement"`
	// The project in which the `cluster` can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project *string `pulumi:"project"`
	PysparkConfig *dataprocJobPysparkConfig.JobPysparkConfig `pulumi:"pysparkConfig"`
	Reference *dataprocJobReference.JobReference `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to `global`.
	Region *string `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling *dataprocJobScheduling.JobScheduling `pulumi:"scheduling"`
	SparkConfig *dataprocJobSparkConfig.JobSparkConfig `pulumi:"sparkConfig"`
	SparksqlConfig *dataprocJobSparksqlConfig.JobSparksqlConfig `pulumi:"sparksqlConfig"`
}

// The set of arguments for constructing a Job resource.
type JobArgs struct {
	// By default, you can only delete inactive jobs within
	// Dataproc. Setting this to true, and calling destroy, will ensure that the
	// job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrInput
	HadoopConfig dataprocJobHadoopConfig.JobHadoopConfigPtrInput
	HiveConfig dataprocJobHiveConfig.JobHiveConfigPtrInput
	// The list of labels (key/value pairs) to add to the job.
	Labels pulumi.StringMapInput
	PigConfig dataprocJobPigConfig.JobPigConfigPtrInput
	Placement dataprocJobPlacement.JobPlacementInput
	// The project in which the `cluster` can be found and jobs
	// subsequently run against. If it is not provided, the provider project is used.
	Project pulumi.StringPtrInput
	PysparkConfig dataprocJobPysparkConfig.JobPysparkConfigPtrInput
	Reference dataprocJobReference.JobReferencePtrInput
	// The Cloud Dataproc region. This essentially determines which clusters are available
	// for this job to be submitted to. If not specified, defaults to `global`.
	Region pulumi.StringPtrInput
	// Optional. Job scheduling configuration.
	Scheduling dataprocJobScheduling.JobSchedulingPtrInput
	SparkConfig dataprocJobSparkConfig.JobSparkConfigPtrInput
	SparksqlConfig dataprocJobSparksqlConfig.JobSparksqlConfigPtrInput
}

func (JobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*jobArgs)(nil)).Elem()
}

