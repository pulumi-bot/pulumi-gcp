# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Any, Dict, List, Optional, Tuple, Union
from .. import _utilities, _tables
from . import outputs

__all__ = [
    'AutoscalingPolicyBasicAlgorithm',
    'AutoscalingPolicyBasicAlgorithmYarnConfig',
    'AutoscalingPolicySecondaryWorkerConfig',
    'AutoscalingPolicyWorkerConfig',
    'ClusterClusterConfig',
    'ClusterClusterConfigAutoscalingConfig',
    'ClusterClusterConfigEncryptionConfig',
    'ClusterClusterConfigEndpointConfig',
    'ClusterClusterConfigGceClusterConfig',
    'ClusterClusterConfigInitializationAction',
    'ClusterClusterConfigLifecycleConfig',
    'ClusterClusterConfigMasterConfig',
    'ClusterClusterConfigMasterConfigAccelerator',
    'ClusterClusterConfigMasterConfigDiskConfig',
    'ClusterClusterConfigPreemptibleWorkerConfig',
    'ClusterClusterConfigPreemptibleWorkerConfigDiskConfig',
    'ClusterClusterConfigSecurityConfig',
    'ClusterClusterConfigSecurityConfigKerberosConfig',
    'ClusterClusterConfigSoftwareConfig',
    'ClusterClusterConfigWorkerConfig',
    'ClusterClusterConfigWorkerConfigAccelerator',
    'ClusterClusterConfigWorkerConfigDiskConfig',
    'ClusterIAMBindingCondition',
    'ClusterIAMMemberCondition',
    'JobHadoopConfig',
    'JobHadoopConfigLoggingConfig',
    'JobHiveConfig',
    'JobIAMBindingCondition',
    'JobIAMMemberCondition',
    'JobPigConfig',
    'JobPigConfigLoggingConfig',
    'JobPlacement',
    'JobPysparkConfig',
    'JobPysparkConfigLoggingConfig',
    'JobReference',
    'JobScheduling',
    'JobSparkConfig',
    'JobSparkConfigLoggingConfig',
    'JobSparksqlConfig',
    'JobSparksqlConfigLoggingConfig',
    'JobStatus',
]

@pulumi.output_type
class AutoscalingPolicyBasicAlgorithm(dict):
    cooldown_period: Optional[str] = pulumi.output_property("cooldownPeriod")
    """
    Duration between scaling events. A scaling period starts after the
    update operation from the previous event has completed.
    Bounds: [2m, 1d]. Default: 2m.
    """
    yarn_config: 'outputs.AutoscalingPolicyBasicAlgorithmYarnConfig' = pulumi.output_property("yarnConfig")
    """
    YARN autoscaling configuration.  Structure is documented below.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class AutoscalingPolicyBasicAlgorithmYarnConfig(dict):
    graceful_decommission_timeout: str = pulumi.output_property("gracefulDecommissionTimeout")
    """
    Timeout for YARN graceful decommissioning of Node Managers. Specifies the
    duration to wait for jobs to complete before forcefully removing workers
    (and potentially interrupting jobs). Only applicable to downscaling operations.
    Bounds: [0s, 1d].
    """
    scale_down_factor: float = pulumi.output_property("scaleDownFactor")
    """
    Fraction of average pending memory in the last cooldown period for which to
    remove workers. A scale-down factor of 1 will result in scaling down so that there
    is no available memory remaining after the update (more aggressive scaling).
    A scale-down factor of 0 disables removing workers, which can be beneficial for
    autoscaling a single job.
    Bounds: [0.0, 1.0].
    """
    scale_down_min_worker_fraction: Optional[float] = pulumi.output_property("scaleDownMinWorkerFraction")
    """
    Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
    For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
    recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
    means the autoscaler will scale down on any recommended change.
    Bounds: [0.0, 1.0]. Default: 0.0.
    """
    scale_up_factor: float = pulumi.output_property("scaleUpFactor")
    """
    Fraction of average pending memory in the last cooldown period for which to
    add workers. A scale-up factor of 1.0 will result in scaling up so that there
    is no pending memory remaining after the update (more aggressive scaling).
    A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
    (less aggressive scaling).
    Bounds: [0.0, 1.0].
    """
    scale_up_min_worker_fraction: Optional[float] = pulumi.output_property("scaleUpMinWorkerFraction")
    """
    Minimum scale-up threshold as a fraction of total cluster size before scaling
    occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
    must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
    0 means the autoscaler will scale up on any recommended change.
    Bounds: [0.0, 1.0]. Default: 0.0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class AutoscalingPolicySecondaryWorkerConfig(dict):
    max_instances: Optional[float] = pulumi.output_property("maxInstances")
    """
    Maximum number of instances for this group. Note that by default, clusters will not use
    secondary workers. Required for secondary workers if the minimum secondary instances is set.
    Bounds: [minInstances, ). Defaults to 0.
    """
    min_instances: Optional[float] = pulumi.output_property("minInstances")
    """
    Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
    """
    weight: Optional[float] = pulumi.output_property("weight")
    """
    Weight for the instance group, which is used to determine the fraction of total workers
    in the cluster from this instance group. For example, if primary workers have weight 2,
    and secondary workers have weight 1, the cluster will have approximately 2 primary workers
    for each secondary worker.
    The cluster may not reach the specified balance if constrained by min/max bounds or other
    autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
    primary workers will be added. The cluster can also be out of balance when created.
    If weight is not set on any instance group, the cluster will default to equal weight for
    all groups: the cluster will attempt to maintain an equal number of workers in each group
    within the configured size bounds for each group. If weight is set for one group only,
    the cluster will default to zero weight on the unset group. For example if weight is set
    only on primary workers, the cluster will use primary workers only and no secondary workers.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class AutoscalingPolicyWorkerConfig(dict):
    max_instances: float = pulumi.output_property("maxInstances")
    """
    Maximum number of instances for this group. Note that by default, clusters will not use
    secondary workers. Required for secondary workers if the minimum secondary instances is set.
    Bounds: [minInstances, ). Defaults to 0.
    """
    min_instances: Optional[float] = pulumi.output_property("minInstances")
    """
    Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
    """
    weight: Optional[float] = pulumi.output_property("weight")
    """
    Weight for the instance group, which is used to determine the fraction of total workers
    in the cluster from this instance group. For example, if primary workers have weight 2,
    and secondary workers have weight 1, the cluster will have approximately 2 primary workers
    for each secondary worker.
    The cluster may not reach the specified balance if constrained by min/max bounds or other
    autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
    primary workers will be added. The cluster can also be out of balance when created.
    If weight is not set on any instance group, the cluster will default to equal weight for
    all groups: the cluster will attempt to maintain an equal number of workers in each group
    within the configured size bounds for each group. If weight is set for one group only,
    the cluster will default to zero weight on the unset group. For example if weight is set
    only on primary workers, the cluster will use primary workers only and no secondary workers.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfig(dict):
    autoscaling_config: Optional['outputs.ClusterClusterConfigAutoscalingConfig'] = pulumi.output_property("autoscalingConfig")
    """
    The autoscaling policy config associated with the cluster.
    Structure defined below.
    """
    bucket: Optional[str] = pulumi.output_property("bucket")
    encryption_config: Optional['outputs.ClusterClusterConfigEncryptionConfig'] = pulumi.output_property("encryptionConfig")
    """
    The Customer managed encryption keys settings for the cluster.
    Structure defined below.
    """
    endpoint_config: Optional['outputs.ClusterClusterConfigEndpointConfig'] = pulumi.output_property("endpointConfig")
    """
    The config settings for port access on the cluster.
    Structure defined below.
    - - -
    """
    gce_cluster_config: Optional['outputs.ClusterClusterConfigGceClusterConfig'] = pulumi.output_property("gceClusterConfig")
    """
    Common config settings for resources of Google Compute Engine cluster
    instances, applicable to all instances in the cluster. Structure defined below.
    """
    initialization_actions: Optional[List['outputs.ClusterClusterConfigInitializationAction']] = pulumi.output_property("initializationActions")
    """
    Commands to execute on each node after config is completed.
    You can specify multiple versions of these. Structure defined below.
    """
    lifecycle_config: Optional['outputs.ClusterClusterConfigLifecycleConfig'] = pulumi.output_property("lifecycleConfig")
    """
    The settings for auto deletion cluster schedule.
    Structure defined below.
    """
    master_config: Optional['outputs.ClusterClusterConfigMasterConfig'] = pulumi.output_property("masterConfig")
    """
    The Google Compute Engine config settings for the master instances
    in a cluster.. Structure defined below.
    """
    preemptible_worker_config: Optional['outputs.ClusterClusterConfigPreemptibleWorkerConfig'] = pulumi.output_property("preemptibleWorkerConfig")
    """
    The Google Compute Engine config settings for the additional (aka
    preemptible) instances in a cluster. Structure defined below.
    """
    security_config: Optional['outputs.ClusterClusterConfigSecurityConfig'] = pulumi.output_property("securityConfig")
    """
    Security related configuration. Structure defined below.
    """
    software_config: Optional['outputs.ClusterClusterConfigSoftwareConfig'] = pulumi.output_property("softwareConfig")
    """
    The config settings for software inside the cluster.
    Structure defined below.
    """
    staging_bucket: Optional[str] = pulumi.output_property("stagingBucket")
    """
    The Cloud Storage staging bucket used to stage files,
    such as Hadoop jars, between client machines and the cluster.
    Note: If you don't explicitly specify a `staging_bucket`
    then GCP will auto create / assign one for you. However, you are not guaranteed
    an auto generated bucket which is solely dedicated to your cluster; it may be shared
    with other clusters in the same region/zone also choosing to use the auto generation
    option.
    """
    worker_config: Optional['outputs.ClusterClusterConfigWorkerConfig'] = pulumi.output_property("workerConfig")
    """
    The Google Compute Engine config settings for the worker instances
    in a cluster.. Structure defined below.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigAutoscalingConfig(dict):
    policy_uri: str = pulumi.output_property("policyUri")
    """
    The autoscaling policy used by the cluster.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigEncryptionConfig(dict):
    kms_key_name: str = pulumi.output_property("kmsKeyName")
    """
    The Cloud KMS key name to use for PD disk encryption for
    all instances in the cluster.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigEndpointConfig(dict):
    enable_http_port_access: bool = pulumi.output_property("enableHttpPortAccess")
    """
    The flag to enable http access to specific ports
    on the cluster from external sources (aka Component Gateway). Defaults to false.
    """
    http_ports: Optional[Dict[str, Any]] = pulumi.output_property("httpPorts")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigGceClusterConfig(dict):
    internal_ip_only: Optional[bool] = pulumi.output_property("internalIpOnly")
    """
    By default, clusters are not restricted to internal IP addresses, 
    and will have ephemeral external IP addresses assigned to each instance. If set to true, all
    instances in the cluster will only have internal IP addresses. Note: Private Google Access
    (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
    will be launched in.
    """
    metadata: Optional[Dict[str, str]] = pulumi.output_property("metadata")
    """
    A map of the Compute Engine metadata entries to add to all instances
    (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
    """
    network: Optional[str] = pulumi.output_property("network")
    """
    The name or self_link of the Google Compute Engine
    network to the cluster will be part of. Conflicts with `subnetwork`.
    If neither is specified, this defaults to the "default" network.
    """
    service_account: Optional[str] = pulumi.output_property("serviceAccount")
    """
    The service account to be used by the Node VMs.
    If not specified, the "default" service account is used.
    """
    service_account_scopes: Optional[List[str]] = pulumi.output_property("serviceAccountScopes")
    """
    The set of Google API scopes
    to be made available on all of the node VMs under the `service_account`
    specified. These can be	either FQDNs, or scope aliases. The following scopes
    must be set if any other scopes are set. They're necessary to ensure the
    correct functioning ofthe cluster, and are set automatically by the API:
    """
    subnetwork: Optional[str] = pulumi.output_property("subnetwork")
    """
    The name or self_link of the Google Compute Engine
    subnetwork the cluster will be part of. Conflicts with `network`.
    """
    tags: Optional[List[str]] = pulumi.output_property("tags")
    """
    The list of instance tags applied to instances in the cluster.
    Tags are used to identify valid sources or targets for network firewalls.
    """
    zone: Optional[str] = pulumi.output_property("zone")
    """
    The GCP zone where your data is stored and used (i.e. where
    the master and the worker nodes will be created in). If `region` is set to 'global' (default)
    then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
    to determine this automatically for you.
    Note: This setting additionally determines and restricts
    which computing resources are available for use with other configs such as
    `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigInitializationAction(dict):
    script: str = pulumi.output_property("script")
    """
    The script to be executed during initialization of the cluster.
    The script must be a GCS file with a gs:// prefix.
    """
    timeout_sec: Optional[float] = pulumi.output_property("timeoutSec")
    """
    The maximum duration (in seconds) which `script` is
    allowed to take to execute its action. GCP will default to a predetermined
    computed value if not set (currently 300).
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigLifecycleConfig(dict):
    auto_delete_time: Optional[str] = pulumi.output_property("autoDeleteTime")
    """
    The time when cluster will be auto-deleted.
    A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
    Example: "2014-10-02T15:01:23.045123456Z".
    """
    idle_delete_ttl: Optional[str] = pulumi.output_property("idleDeleteTtl")
    """
    The duration to keep the cluster alive while idling
    (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
    """
    idle_start_time: Optional[str] = pulumi.output_property("idleStartTime")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigMasterConfig(dict):
    accelerators: Optional[List['outputs.ClusterClusterConfigMasterConfigAccelerator']] = pulumi.output_property("accelerators")
    """
    The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    """
    disk_config: Optional['outputs.ClusterClusterConfigMasterConfigDiskConfig'] = pulumi.output_property("diskConfig")
    """
    Disk Config
    """
    image_uri: Optional[str] = pulumi.output_property("imageUri")
    """
    The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
    for more information.
    """
    instance_names: Optional[List[str]] = pulumi.output_property("instanceNames")
    machine_type: Optional[str] = pulumi.output_property("machineType")
    """
    The name of a Google Compute Engine machine type
    to create for the worker nodes. If not specified, GCP will default to a predetermined
    computed value (currently `n1-standard-4`).
    """
    min_cpu_platform: Optional[str] = pulumi.output_property("minCpuPlatform")
    """
    The name of a minimum generation of CPU family
    for the master. If not specified, GCP will default to a predetermined computed value
    for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for details about which CPU families are available (and defaulted) for each zone.
    """
    num_instances: Optional[float] = pulumi.output_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigMasterConfigAccelerator(dict):
    accelerator_count: float = pulumi.output_property("acceleratorCount")
    """
    The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
    """
    accelerator_type: str = pulumi.output_property("acceleratorType")
    """
    The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigMasterConfigDiskConfig(dict):
    boot_disk_size_gb: Optional[float] = pulumi.output_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[str] = pulumi.output_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[float] = pulumi.output_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigPreemptibleWorkerConfig(dict):
    disk_config: Optional['outputs.ClusterClusterConfigPreemptibleWorkerConfigDiskConfig'] = pulumi.output_property("diskConfig")
    """
    Disk Config
    """
    instance_names: Optional[List[str]] = pulumi.output_property("instanceNames")
    num_instances: Optional[float] = pulumi.output_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigPreemptibleWorkerConfigDiskConfig(dict):
    boot_disk_size_gb: Optional[float] = pulumi.output_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[str] = pulumi.output_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[float] = pulumi.output_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigSecurityConfig(dict):
    kerberos_config: 'outputs.ClusterClusterConfigSecurityConfigKerberosConfig' = pulumi.output_property("kerberosConfig")
    """
    Kerberos Configuration
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigSecurityConfigKerberosConfig(dict):
    cross_realm_trust_admin_server: Optional[str] = pulumi.output_property("crossRealmTrustAdminServer")
    """
    The admin server (IP or hostname) for the
    remote trusted realm in a cross realm trust relationship.
    """
    cross_realm_trust_kdc: Optional[str] = pulumi.output_property("crossRealmTrustKdc")
    """
    The KDC (IP or hostname) for the
    remote trusted realm in a cross realm trust relationship.
    """
    cross_realm_trust_realm: Optional[str] = pulumi.output_property("crossRealmTrustRealm")
    """
    The remote realm the Dataproc on-cluster KDC will
    trust, should the user enable cross realm trust.
    """
    cross_realm_trust_shared_password_uri: Optional[str] = pulumi.output_property("crossRealmTrustSharedPasswordUri")
    """
    The Cloud Storage URI of a KMS
    encrypted file containing the shared password between the on-cluster Kerberos realm
    and the remote trusted realm, in a cross realm trust relationship.
    """
    enable_kerberos: Optional[bool] = pulumi.output_property("enableKerberos")
    """
    Flag to indicate whether to Kerberize the cluster.
    """
    kdc_db_key_uri: Optional[str] = pulumi.output_property("kdcDbKeyUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the master key of the KDC database.
    """
    key_password_uri: Optional[str] = pulumi.output_property("keyPasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the password to the user provided key. For the self-signed certificate, this password
    is generated by Dataproc.
    """
    keystore_password_uri: Optional[str] = pulumi.output_property("keystorePasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the password to the user provided keystore. For the self-signed certificated, the password
    is generated by Dataproc.
    """
    keystore_uri: Optional[str] = pulumi.output_property("keystoreUri")
    """
    The Cloud Storage URI of the keystore file used for SSL encryption.
    If not provided, Dataproc will provide a self-signed certificate.
    """
    kms_key_uri: str = pulumi.output_property("kmsKeyUri")
    """
    The URI of the KMS key used to encrypt various sensitive files.
    """
    realm: Optional[str] = pulumi.output_property("realm")
    """
    The name of the on-cluster Kerberos realm. If not specified, the
    uppercased domain of hostnames will be the realm.
    """
    root_principal_password_uri: str = pulumi.output_property("rootPrincipalPasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file
    containing the root principal password.
    """
    tgt_lifetime_hours: Optional[float] = pulumi.output_property("tgtLifetimeHours")
    """
    The lifetime of the ticket granting ticket, in hours.
    """
    truststore_password_uri: Optional[str] = pulumi.output_property("truststorePasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file
    containing the password to the user provided truststore. For the self-signed
    certificate, this password is generated by Dataproc.
    """
    truststore_uri: Optional[str] = pulumi.output_property("truststoreUri")
    """
    The Cloud Storage URI of the truststore file used for
    SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigSoftwareConfig(dict):
    image_version: Optional[str] = pulumi.output_property("imageVersion")
    """
    The Cloud Dataproc image version to use
    for the cluster - this controls the sets of software versions
    installed onto the nodes when you create clusters. If not specified, defaults to the
    latest version. For a list of valid versions see
    [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
    """
    optional_components: Optional[List[str]] = pulumi.output_property("optionalComponents")
    """
    The set of optional components to activate on the cluster. 
    Accepted values are:
    * ANACONDA
    * DRUID
    * HBASE
    * HIVE_WEBHCAT
    * JUPYTER
    * KERBEROS
    * PRESTO
    * RANGER
    * SOLR
    * ZEPPELIN
    * ZOOKEEPER
    """
    override_properties: Optional[Dict[str, str]] = pulumi.output_property("overrideProperties")
    """
    A list of override and additional properties (key/value pairs)
    used to modify various aspects of the common configuration files used when creating
    a cluster. For a list of valid properties please see
    [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
    """
    properties: Optional[Dict[str, Any]] = pulumi.output_property("properties")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigWorkerConfig(dict):
    accelerators: Optional[List['outputs.ClusterClusterConfigWorkerConfigAccelerator']] = pulumi.output_property("accelerators")
    """
    The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    """
    disk_config: Optional['outputs.ClusterClusterConfigWorkerConfigDiskConfig'] = pulumi.output_property("diskConfig")
    """
    Disk Config
    """
    image_uri: Optional[str] = pulumi.output_property("imageUri")
    """
    The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
    for more information.
    """
    instance_names: Optional[List[str]] = pulumi.output_property("instanceNames")
    machine_type: Optional[str] = pulumi.output_property("machineType")
    """
    The name of a Google Compute Engine machine type
    to create for the worker nodes. If not specified, GCP will default to a predetermined
    computed value (currently `n1-standard-4`).
    """
    min_cpu_platform: Optional[str] = pulumi.output_property("minCpuPlatform")
    """
    The name of a minimum generation of CPU family
    for the master. If not specified, GCP will default to a predetermined computed value
    for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for details about which CPU families are available (and defaulted) for each zone.
    """
    num_instances: Optional[float] = pulumi.output_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigWorkerConfigAccelerator(dict):
    accelerator_count: float = pulumi.output_property("acceleratorCount")
    """
    The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
    """
    accelerator_type: str = pulumi.output_property("acceleratorType")
    """
    The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterClusterConfigWorkerConfigDiskConfig(dict):
    boot_disk_size_gb: Optional[float] = pulumi.output_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[str] = pulumi.output_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[float] = pulumi.output_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterIAMBindingCondition(dict):
    description: Optional[str] = pulumi.output_property("description")
    expression: str = pulumi.output_property("expression")
    title: str = pulumi.output_property("title")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class ClusterIAMMemberCondition(dict):
    description: Optional[str] = pulumi.output_property("description")
    expression: str = pulumi.output_property("expression")
    title: str = pulumi.output_property("title")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobHadoopConfig(dict):
    archive_uris: Optional[List[str]] = pulumi.output_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[List[str]] = pulumi.output_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[List[str]] = pulumi.output_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional['outputs.JobHadoopConfigLoggingConfig'] = pulumi.output_property("loggingConfig")
    main_class: Optional[str] = pulumi.output_property("mainClass")
    """
    The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
    """
    main_jar_file_uri: Optional[str] = pulumi.output_property("mainJarFileUri")
    """
    The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
    """
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobHadoopConfigLoggingConfig(dict):
    driver_log_levels: Dict[str, str] = pulumi.output_property("driverLogLevels")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobHiveConfig(dict):
    continue_on_failure: Optional[bool] = pulumi.output_property("continueOnFailure")
    """
    Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    """
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[str] = pulumi.output_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[List[str]] = pulumi.output_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[Dict[str, str]] = pulumi.output_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobIAMBindingCondition(dict):
    description: Optional[str] = pulumi.output_property("description")
    expression: str = pulumi.output_property("expression")
    title: str = pulumi.output_property("title")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobIAMMemberCondition(dict):
    description: Optional[str] = pulumi.output_property("description")
    expression: str = pulumi.output_property("expression")
    title: str = pulumi.output_property("title")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobPigConfig(dict):
    continue_on_failure: Optional[bool] = pulumi.output_property("continueOnFailure")
    """
    Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    """
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional['outputs.JobPigConfigLoggingConfig'] = pulumi.output_property("loggingConfig")
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[str] = pulumi.output_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[List[str]] = pulumi.output_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[Dict[str, str]] = pulumi.output_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobPigConfigLoggingConfig(dict):
    driver_log_levels: Dict[str, str] = pulumi.output_property("driverLogLevels")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobPlacement(dict):
    cluster_name: str = pulumi.output_property("clusterName")
    cluster_uuid: Optional[str] = pulumi.output_property("clusterUuid")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobPysparkConfig(dict):
    archive_uris: Optional[List[str]] = pulumi.output_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[List[str]] = pulumi.output_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[List[str]] = pulumi.output_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional['outputs.JobPysparkConfigLoggingConfig'] = pulumi.output_property("loggingConfig")
    main_python_file_uri: str = pulumi.output_property("mainPythonFileUri")
    """
    The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    """
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    python_file_uris: Optional[List[str]] = pulumi.output_property("pythonFileUris")
    """
    HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobPysparkConfigLoggingConfig(dict):
    driver_log_levels: Dict[str, str] = pulumi.output_property("driverLogLevels")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobReference(dict):
    job_id: Optional[str] = pulumi.output_property("jobId")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobScheduling(dict):
    max_failures_per_hour: float = pulumi.output_property("maxFailuresPerHour")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobSparkConfig(dict):
    archive_uris: Optional[List[str]] = pulumi.output_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[List[str]] = pulumi.output_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[List[str]] = pulumi.output_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional['outputs.JobSparkConfigLoggingConfig'] = pulumi.output_property("loggingConfig")
    main_class: Optional[str] = pulumi.output_property("mainClass")
    """
    The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
    """
    main_jar_file_uri: Optional[str] = pulumi.output_property("mainJarFileUri")
    """
    The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
    """
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobSparkConfigLoggingConfig(dict):
    driver_log_levels: Dict[str, str] = pulumi.output_property("driverLogLevels")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobSparksqlConfig(dict):
    jar_file_uris: Optional[List[str]] = pulumi.output_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional['outputs.JobSparksqlConfigLoggingConfig'] = pulumi.output_property("loggingConfig")
    properties: Optional[Dict[str, str]] = pulumi.output_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[str] = pulumi.output_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[List[str]] = pulumi.output_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[Dict[str, str]] = pulumi.output_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobSparksqlConfigLoggingConfig(dict):
    driver_log_levels: Dict[str, str] = pulumi.output_property("driverLogLevels")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


@pulumi.output_type
class JobStatus(dict):
    details: Optional[str] = pulumi.output_property("details")
    state: Optional[str] = pulumi.output_property("state")
    state_start_time: Optional[str] = pulumi.output_property("stateStartTime")
    substate: Optional[str] = pulumi.output_property("substate")

    def _translate_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop


