# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Any, Dict, List, Optional, Tuple, Union
from .. import _utilities, _tables

__all__ = [
    'AutoscalingPolicyBasicAlgorithmArgs',
    'AutoscalingPolicyBasicAlgorithmYarnConfigArgs',
    'AutoscalingPolicySecondaryWorkerConfigArgs',
    'AutoscalingPolicyWorkerConfigArgs',
    'ClusterClusterConfigArgs',
    'ClusterClusterConfigAutoscalingConfigArgs',
    'ClusterClusterConfigEncryptionConfigArgs',
    'ClusterClusterConfigEndpointConfigArgs',
    'ClusterClusterConfigGceClusterConfigArgs',
    'ClusterClusterConfigInitializationActionArgs',
    'ClusterClusterConfigLifecycleConfigArgs',
    'ClusterClusterConfigMasterConfigArgs',
    'ClusterClusterConfigMasterConfigAcceleratorArgs',
    'ClusterClusterConfigMasterConfigDiskConfigArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs',
    'ClusterClusterConfigSecurityConfigArgs',
    'ClusterClusterConfigSecurityConfigKerberosConfigArgs',
    'ClusterClusterConfigSoftwareConfigArgs',
    'ClusterClusterConfigWorkerConfigArgs',
    'ClusterClusterConfigWorkerConfigAcceleratorArgs',
    'ClusterClusterConfigWorkerConfigDiskConfigArgs',
    'ClusterIAMBindingConditionArgs',
    'ClusterIAMMemberConditionArgs',
    'JobHadoopConfigArgs',
    'JobHadoopConfigLoggingConfigArgs',
    'JobHiveConfigArgs',
    'JobIAMBindingConditionArgs',
    'JobIAMMemberConditionArgs',
    'JobPigConfigArgs',
    'JobPigConfigLoggingConfigArgs',
    'JobPlacementArgs',
    'JobPysparkConfigArgs',
    'JobPysparkConfigLoggingConfigArgs',
    'JobReferenceArgs',
    'JobSchedulingArgs',
    'JobSparkConfigArgs',
    'JobSparkConfigLoggingConfigArgs',
    'JobSparksqlConfigArgs',
    'JobSparksqlConfigLoggingConfigArgs',
    'JobStatusArgs',
]

@pulumi.input_type
class AutoscalingPolicyBasicAlgorithmArgs:
    yarn_config: pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs'] = pulumi.input_property("yarnConfig")
    """
    YARN autoscaling configuration.  Structure is documented below.
    """
    cooldown_period: Optional[pulumi.Input[str]] = pulumi.input_property("cooldownPeriod")
    """
    Duration between scaling events. A scaling period starts after the
    update operation from the previous event has completed.
    Bounds: [2m, 1d]. Default: 2m.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, yarn_config: pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs'], cooldown_period: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs'] yarn_config: YARN autoscaling configuration.  Structure is documented below.
        :param pulumi.Input[str] cooldown_period: Duration between scaling events. A scaling period starts after the
               update operation from the previous event has completed.
               Bounds: [2m, 1d]. Default: 2m.
        """
        __self__.yarn_config = yarn_config
        __self__.cooldown_period = cooldown_period

@pulumi.input_type
class AutoscalingPolicyBasicAlgorithmYarnConfigArgs:
    graceful_decommission_timeout: pulumi.Input[str] = pulumi.input_property("gracefulDecommissionTimeout")
    """
    Timeout for YARN graceful decommissioning of Node Managers. Specifies the
    duration to wait for jobs to complete before forcefully removing workers
    (and potentially interrupting jobs). Only applicable to downscaling operations.
    Bounds: [0s, 1d].
    """
    scale_down_factor: pulumi.Input[float] = pulumi.input_property("scaleDownFactor")
    """
    Fraction of average pending memory in the last cooldown period for which to
    remove workers. A scale-down factor of 1 will result in scaling down so that there
    is no available memory remaining after the update (more aggressive scaling).
    A scale-down factor of 0 disables removing workers, which can be beneficial for
    autoscaling a single job.
    Bounds: [0.0, 1.0].
    """
    scale_up_factor: pulumi.Input[float] = pulumi.input_property("scaleUpFactor")
    """
    Fraction of average pending memory in the last cooldown period for which to
    add workers. A scale-up factor of 1.0 will result in scaling up so that there
    is no pending memory remaining after the update (more aggressive scaling).
    A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
    (less aggressive scaling).
    Bounds: [0.0, 1.0].
    """
    scale_down_min_worker_fraction: Optional[pulumi.Input[float]] = pulumi.input_property("scaleDownMinWorkerFraction")
    """
    Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
    For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
    recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
    means the autoscaler will scale down on any recommended change.
    Bounds: [0.0, 1.0]. Default: 0.0.
    """
    scale_up_min_worker_fraction: Optional[pulumi.Input[float]] = pulumi.input_property("scaleUpMinWorkerFraction")
    """
    Minimum scale-up threshold as a fraction of total cluster size before scaling
    occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
    must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
    0 means the autoscaler will scale up on any recommended change.
    Bounds: [0.0, 1.0]. Default: 0.0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, graceful_decommission_timeout: pulumi.Input[str], scale_down_factor: pulumi.Input[float], scale_up_factor: pulumi.Input[float], scale_down_min_worker_fraction: Optional[pulumi.Input[float]] = None, scale_up_min_worker_fraction: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[str] graceful_decommission_timeout: Timeout for YARN graceful decommissioning of Node Managers. Specifies the
               duration to wait for jobs to complete before forcefully removing workers
               (and potentially interrupting jobs). Only applicable to downscaling operations.
               Bounds: [0s, 1d].
        :param pulumi.Input[float] scale_down_factor: Fraction of average pending memory in the last cooldown period for which to
               remove workers. A scale-down factor of 1 will result in scaling down so that there
               is no available memory remaining after the update (more aggressive scaling).
               A scale-down factor of 0 disables removing workers, which can be beneficial for
               autoscaling a single job.
               Bounds: [0.0, 1.0].
        :param pulumi.Input[float] scale_up_factor: Fraction of average pending memory in the last cooldown period for which to
               add workers. A scale-up factor of 1.0 will result in scaling up so that there
               is no pending memory remaining after the update (more aggressive scaling).
               A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
               (less aggressive scaling).
               Bounds: [0.0, 1.0].
        :param pulumi.Input[float] scale_down_min_worker_fraction: Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
               For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
               recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
               means the autoscaler will scale down on any recommended change.
               Bounds: [0.0, 1.0]. Default: 0.0.
        :param pulumi.Input[float] scale_up_min_worker_fraction: Minimum scale-up threshold as a fraction of total cluster size before scaling
               occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
               must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
               0 means the autoscaler will scale up on any recommended change.
               Bounds: [0.0, 1.0]. Default: 0.0.
        """
        __self__.graceful_decommission_timeout = graceful_decommission_timeout
        __self__.scale_down_factor = scale_down_factor
        __self__.scale_up_factor = scale_up_factor
        __self__.scale_down_min_worker_fraction = scale_down_min_worker_fraction
        __self__.scale_up_min_worker_fraction = scale_up_min_worker_fraction

@pulumi.input_type
class AutoscalingPolicySecondaryWorkerConfigArgs:
    max_instances: Optional[pulumi.Input[float]] = pulumi.input_property("maxInstances")
    """
    Maximum number of instances for this group. Note that by default, clusters will not use
    secondary workers. Required for secondary workers if the minimum secondary instances is set.
    Bounds: [minInstances, ). Defaults to 0.
    """
    min_instances: Optional[pulumi.Input[float]] = pulumi.input_property("minInstances")
    """
    Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
    """
    weight: Optional[pulumi.Input[float]] = pulumi.input_property("weight")
    """
    Weight for the instance group, which is used to determine the fraction of total workers
    in the cluster from this instance group. For example, if primary workers have weight 2,
    and secondary workers have weight 1, the cluster will have approximately 2 primary workers
    for each secondary worker.
    The cluster may not reach the specified balance if constrained by min/max bounds or other
    autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
    primary workers will be added. The cluster can also be out of balance when created.
    If weight is not set on any instance group, the cluster will default to equal weight for
    all groups: the cluster will attempt to maintain an equal number of workers in each group
    within the configured size bounds for each group. If weight is set for one group only,
    the cluster will default to zero weight on the unset group. For example if weight is set
    only on primary workers, the cluster will use primary workers only and no secondary workers.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_instances: Optional[pulumi.Input[float]] = None, min_instances: Optional[pulumi.Input[float]] = None, weight: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[float] max_instances: Maximum number of instances for this group. Note that by default, clusters will not use
               secondary workers. Required for secondary workers if the minimum secondary instances is set.
               Bounds: [minInstances, ). Defaults to 0.
        :param pulumi.Input[float] min_instances: Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
        :param pulumi.Input[float] weight: Weight for the instance group, which is used to determine the fraction of total workers
               in the cluster from this instance group. For example, if primary workers have weight 2,
               and secondary workers have weight 1, the cluster will have approximately 2 primary workers
               for each secondary worker.
               The cluster may not reach the specified balance if constrained by min/max bounds or other
               autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
               primary workers will be added. The cluster can also be out of balance when created.
               If weight is not set on any instance group, the cluster will default to equal weight for
               all groups: the cluster will attempt to maintain an equal number of workers in each group
               within the configured size bounds for each group. If weight is set for one group only,
               the cluster will default to zero weight on the unset group. For example if weight is set
               only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        __self__.max_instances = max_instances
        __self__.min_instances = min_instances
        __self__.weight = weight

@pulumi.input_type
class AutoscalingPolicyWorkerConfigArgs:
    max_instances: pulumi.Input[float] = pulumi.input_property("maxInstances")
    """
    Maximum number of instances for this group. Note that by default, clusters will not use
    secondary workers. Required for secondary workers if the minimum secondary instances is set.
    Bounds: [minInstances, ). Defaults to 0.
    """
    min_instances: Optional[pulumi.Input[float]] = pulumi.input_property("minInstances")
    """
    Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
    """
    weight: Optional[pulumi.Input[float]] = pulumi.input_property("weight")
    """
    Weight for the instance group, which is used to determine the fraction of total workers
    in the cluster from this instance group. For example, if primary workers have weight 2,
    and secondary workers have weight 1, the cluster will have approximately 2 primary workers
    for each secondary worker.
    The cluster may not reach the specified balance if constrained by min/max bounds or other
    autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
    primary workers will be added. The cluster can also be out of balance when created.
    If weight is not set on any instance group, the cluster will default to equal weight for
    all groups: the cluster will attempt to maintain an equal number of workers in each group
    within the configured size bounds for each group. If weight is set for one group only,
    the cluster will default to zero weight on the unset group. For example if weight is set
    only on primary workers, the cluster will use primary workers only and no secondary workers.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_instances: pulumi.Input[float], min_instances: Optional[pulumi.Input[float]] = None, weight: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[float] max_instances: Maximum number of instances for this group. Note that by default, clusters will not use
               secondary workers. Required for secondary workers if the minimum secondary instances is set.
               Bounds: [minInstances, ). Defaults to 0.
        :param pulumi.Input[float] min_instances: Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
        :param pulumi.Input[float] weight: Weight for the instance group, which is used to determine the fraction of total workers
               in the cluster from this instance group. For example, if primary workers have weight 2,
               and secondary workers have weight 1, the cluster will have approximately 2 primary workers
               for each secondary worker.
               The cluster may not reach the specified balance if constrained by min/max bounds or other
               autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
               primary workers will be added. The cluster can also be out of balance when created.
               If weight is not set on any instance group, the cluster will default to equal weight for
               all groups: the cluster will attempt to maintain an equal number of workers in each group
               within the configured size bounds for each group. If weight is set for one group only,
               the cluster will default to zero weight on the unset group. For example if weight is set
               only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        __self__.max_instances = max_instances
        __self__.min_instances = min_instances
        __self__.weight = weight

@pulumi.input_type
class ClusterClusterConfigArgs:
    autoscaling_config: Optional[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs']] = pulumi.input_property("autoscalingConfig")
    """
    The autoscaling policy config associated with the cluster.
    Structure defined below.
    """
    bucket: Optional[pulumi.Input[str]] = pulumi.input_property("bucket")
    encryption_config: Optional[pulumi.Input['ClusterClusterConfigEncryptionConfigArgs']] = pulumi.input_property("encryptionConfig")
    """
    The Customer managed encryption keys settings for the cluster.
    Structure defined below.
    """
    endpoint_config: Optional[pulumi.Input['ClusterClusterConfigEndpointConfigArgs']] = pulumi.input_property("endpointConfig")
    """
    The config settings for port access on the cluster.
    Structure defined below.
    - - -
    """
    gce_cluster_config: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigArgs']] = pulumi.input_property("gceClusterConfig")
    """
    Common config settings for resources of Google Compute Engine cluster
    instances, applicable to all instances in the cluster. Structure defined below.
    """
    initialization_actions: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]]] = pulumi.input_property("initializationActions")
    """
    Commands to execute on each node after config is completed.
    You can specify multiple versions of these. Structure defined below.
    """
    lifecycle_config: Optional[pulumi.Input['ClusterClusterConfigLifecycleConfigArgs']] = pulumi.input_property("lifecycleConfig")
    """
    The settings for auto deletion cluster schedule.
    Structure defined below.
    """
    master_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigArgs']] = pulumi.input_property("masterConfig")
    """
    The Google Compute Engine config settings for the master instances
    in a cluster.. Structure defined below.
    """
    preemptible_worker_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs']] = pulumi.input_property("preemptibleWorkerConfig")
    """
    The Google Compute Engine config settings for the additional (aka
    preemptible) instances in a cluster. Structure defined below.
    """
    security_config: Optional[pulumi.Input['ClusterClusterConfigSecurityConfigArgs']] = pulumi.input_property("securityConfig")
    """
    Security related configuration. Structure defined below.
    """
    software_config: Optional[pulumi.Input['ClusterClusterConfigSoftwareConfigArgs']] = pulumi.input_property("softwareConfig")
    """
    The config settings for software inside the cluster.
    Structure defined below.
    """
    staging_bucket: Optional[pulumi.Input[str]] = pulumi.input_property("stagingBucket")
    """
    The Cloud Storage staging bucket used to stage files,
    such as Hadoop jars, between client machines and the cluster.
    Note: If you don't explicitly specify a `staging_bucket`
    then GCP will auto create / assign one for you. However, you are not guaranteed
    an auto generated bucket which is solely dedicated to your cluster; it may be shared
    with other clusters in the same region/zone also choosing to use the auto generation
    option.
    """
    worker_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigArgs']] = pulumi.input_property("workerConfig")
    """
    The Google Compute Engine config settings for the worker instances
    in a cluster.. Structure defined below.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, autoscaling_config: Optional[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs']] = None, bucket: Optional[pulumi.Input[str]] = None, encryption_config: Optional[pulumi.Input['ClusterClusterConfigEncryptionConfigArgs']] = None, endpoint_config: Optional[pulumi.Input['ClusterClusterConfigEndpointConfigArgs']] = None, gce_cluster_config: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigArgs']] = None, initialization_actions: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]]] = None, lifecycle_config: Optional[pulumi.Input['ClusterClusterConfigLifecycleConfigArgs']] = None, master_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigArgs']] = None, preemptible_worker_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs']] = None, security_config: Optional[pulumi.Input['ClusterClusterConfigSecurityConfigArgs']] = None, software_config: Optional[pulumi.Input['ClusterClusterConfigSoftwareConfigArgs']] = None, staging_bucket: Optional[pulumi.Input[str]] = None, worker_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigArgs']] = None) -> None:
        """
        :param pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs'] autoscaling_config: The autoscaling policy config associated with the cluster.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigEncryptionConfigArgs'] encryption_config: The Customer managed encryption keys settings for the cluster.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigEndpointConfigArgs'] endpoint_config: The config settings for port access on the cluster.
               Structure defined below.
               - - -
        :param pulumi.Input['ClusterClusterConfigGceClusterConfigArgs'] gce_cluster_config: Common config settings for resources of Google Compute Engine cluster
               instances, applicable to all instances in the cluster. Structure defined below.
        :param pulumi.Input[List[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]] initialization_actions: Commands to execute on each node after config is completed.
               You can specify multiple versions of these. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigLifecycleConfigArgs'] lifecycle_config: The settings for auto deletion cluster schedule.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigMasterConfigArgs'] master_config: The Google Compute Engine config settings for the master instances
               in a cluster.. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs'] preemptible_worker_config: The Google Compute Engine config settings for the additional (aka
               preemptible) instances in a cluster. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigSecurityConfigArgs'] security_config: Security related configuration. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigSoftwareConfigArgs'] software_config: The config settings for software inside the cluster.
               Structure defined below.
        :param pulumi.Input[str] staging_bucket: The Cloud Storage staging bucket used to stage files,
               such as Hadoop jars, between client machines and the cluster.
               Note: If you don't explicitly specify a `staging_bucket`
               then GCP will auto create / assign one for you. However, you are not guaranteed
               an auto generated bucket which is solely dedicated to your cluster; it may be shared
               with other clusters in the same region/zone also choosing to use the auto generation
               option.
        :param pulumi.Input['ClusterClusterConfigWorkerConfigArgs'] worker_config: The Google Compute Engine config settings for the worker instances
               in a cluster.. Structure defined below.
        """
        __self__.autoscaling_config = autoscaling_config
        __self__.bucket = bucket
        __self__.encryption_config = encryption_config
        __self__.endpoint_config = endpoint_config
        __self__.gce_cluster_config = gce_cluster_config
        __self__.initialization_actions = initialization_actions
        __self__.lifecycle_config = lifecycle_config
        __self__.master_config = master_config
        __self__.preemptible_worker_config = preemptible_worker_config
        __self__.security_config = security_config
        __self__.software_config = software_config
        __self__.staging_bucket = staging_bucket
        __self__.worker_config = worker_config

@pulumi.input_type
class ClusterClusterConfigAutoscalingConfigArgs:
    policy_uri: pulumi.Input[str] = pulumi.input_property("policyUri")
    """
    The autoscaling policy used by the cluster.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, policy_uri: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] policy_uri: The autoscaling policy used by the cluster.
        """
        __self__.policy_uri = policy_uri

@pulumi.input_type
class ClusterClusterConfigEncryptionConfigArgs:
    kms_key_name: pulumi.Input[str] = pulumi.input_property("kmsKeyName")
    """
    The Cloud KMS key name to use for PD disk encryption for
    all instances in the cluster.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, kms_key_name: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] kms_key_name: The Cloud KMS key name to use for PD disk encryption for
               all instances in the cluster.
        """
        __self__.kms_key_name = kms_key_name

@pulumi.input_type
class ClusterClusterConfigEndpointConfigArgs:
    enable_http_port_access: pulumi.Input[bool] = pulumi.input_property("enableHttpPortAccess")
    """
    The flag to enable http access to specific ports
    on the cluster from external sources (aka Component Gateway). Defaults to false.
    """
    http_ports: Optional[pulumi.Input[Dict[str, Any]]] = pulumi.input_property("httpPorts")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enable_http_port_access: pulumi.Input[bool], http_ports: Optional[pulumi.Input[Dict[str, Any]]] = None) -> None:
        """
        :param pulumi.Input[bool] enable_http_port_access: The flag to enable http access to specific ports
               on the cluster from external sources (aka Component Gateway). Defaults to false.
        """
        __self__.enable_http_port_access = enable_http_port_access
        __self__.http_ports = http_ports

@pulumi.input_type
class ClusterClusterConfigGceClusterConfigArgs:
    internal_ip_only: Optional[pulumi.Input[bool]] = pulumi.input_property("internalIpOnly")
    """
    By default, clusters are not restricted to internal IP addresses, 
    and will have ephemeral external IP addresses assigned to each instance. If set to true, all
    instances in the cluster will only have internal IP addresses. Note: Private Google Access
    (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
    will be launched in.
    """
    metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("metadata")
    """
    A map of the Compute Engine metadata entries to add to all instances
    (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
    """
    network: Optional[pulumi.Input[str]] = pulumi.input_property("network")
    """
    The name or self_link of the Google Compute Engine
    network to the cluster will be part of. Conflicts with `subnetwork`.
    If neither is specified, this defaults to the "default" network.
    """
    service_account: Optional[pulumi.Input[str]] = pulumi.input_property("serviceAccount")
    """
    The service account to be used by the Node VMs.
    If not specified, the "default" service account is used.
    """
    service_account_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("serviceAccountScopes")
    """
    The set of Google API scopes
    to be made available on all of the node VMs under the `service_account`
    specified. These can be	either FQDNs, or scope aliases. The following scopes
    must be set if any other scopes are set. They're necessary to ensure the
    correct functioning ofthe cluster, and are set automatically by the API:
    """
    subnetwork: Optional[pulumi.Input[str]] = pulumi.input_property("subnetwork")
    """
    The name or self_link of the Google Compute Engine
    subnetwork the cluster will be part of. Conflicts with `network`.
    """
    tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("tags")
    """
    The list of instance tags applied to instances in the cluster.
    Tags are used to identify valid sources or targets for network firewalls.
    """
    zone: Optional[pulumi.Input[str]] = pulumi.input_property("zone")
    """
    The GCP zone where your data is stored and used (i.e. where
    the master and the worker nodes will be created in). If `region` is set to 'global' (default)
    then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
    to determine this automatically for you.
    Note: This setting additionally determines and restricts
    which computing resources are available for use with other configs such as
    `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, internal_ip_only: Optional[pulumi.Input[bool]] = None, metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, network: Optional[pulumi.Input[str]] = None, service_account: Optional[pulumi.Input[str]] = None, service_account_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, subnetwork: Optional[pulumi.Input[str]] = None, tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, zone: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[bool] internal_ip_only: By default, clusters are not restricted to internal IP addresses, 
               and will have ephemeral external IP addresses assigned to each instance. If set to true, all
               instances in the cluster will only have internal IP addresses. Note: Private Google Access
               (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
               will be launched in.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] metadata: A map of the Compute Engine metadata entries to add to all instances
               (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        :param pulumi.Input[str] network: The name or self_link of the Google Compute Engine
               network to the cluster will be part of. Conflicts with `subnetwork`.
               If neither is specified, this defaults to the "default" network.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
        :param pulumi.Input[List[pulumi.Input[str]]] service_account_scopes: The set of Google API scopes
               to be made available on all of the node VMs under the `service_account`
               specified. These can be	either FQDNs, or scope aliases. The following scopes
               must be set if any other scopes are set. They're necessary to ensure the
               correct functioning ofthe cluster, and are set automatically by the API:
        :param pulumi.Input[str] subnetwork: The name or self_link of the Google Compute Engine
               subnetwork the cluster will be part of. Conflicts with `network`.
        :param pulumi.Input[List[pulumi.Input[str]]] tags: The list of instance tags applied to instances in the cluster.
               Tags are used to identify valid sources or targets for network firewalls.
        :param pulumi.Input[str] zone: The GCP zone where your data is stored and used (i.e. where
               the master and the worker nodes will be created in). If `region` is set to 'global' (default)
               then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
               to determine this automatically for you.
               Note: This setting additionally determines and restricts
               which computing resources are available for use with other configs such as
               `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
        """
        __self__.internal_ip_only = internal_ip_only
        __self__.metadata = metadata
        __self__.network = network
        __self__.service_account = service_account
        __self__.service_account_scopes = service_account_scopes
        __self__.subnetwork = subnetwork
        __self__.tags = tags
        __self__.zone = zone

@pulumi.input_type
class ClusterClusterConfigInitializationActionArgs:
    script: pulumi.Input[str] = pulumi.input_property("script")
    """
    The script to be executed during initialization of the cluster.
    The script must be a GCS file with a gs:// prefix.
    """
    timeout_sec: Optional[pulumi.Input[float]] = pulumi.input_property("timeoutSec")
    """
    The maximum duration (in seconds) which `script` is
    allowed to take to execute its action. GCP will default to a predetermined
    computed value if not set (currently 300).
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, script: pulumi.Input[str], timeout_sec: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[str] script: The script to be executed during initialization of the cluster.
               The script must be a GCS file with a gs:// prefix.
        :param pulumi.Input[float] timeout_sec: The maximum duration (in seconds) which `script` is
               allowed to take to execute its action. GCP will default to a predetermined
               computed value if not set (currently 300).
        """
        __self__.script = script
        __self__.timeout_sec = timeout_sec

@pulumi.input_type
class ClusterClusterConfigLifecycleConfigArgs:
    auto_delete_time: Optional[pulumi.Input[str]] = pulumi.input_property("autoDeleteTime")
    """
    The time when cluster will be auto-deleted.
    A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
    Example: "2014-10-02T15:01:23.045123456Z".
    """
    idle_delete_ttl: Optional[pulumi.Input[str]] = pulumi.input_property("idleDeleteTtl")
    """
    The duration to keep the cluster alive while idling
    (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
    """
    idle_start_time: Optional[pulumi.Input[str]] = pulumi.input_property("idleStartTime")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, auto_delete_time: Optional[pulumi.Input[str]] = None, idle_delete_ttl: Optional[pulumi.Input[str]] = None, idle_start_time: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] auto_delete_time: The time when cluster will be auto-deleted.
               A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
               Example: "2014-10-02T15:01:23.045123456Z".
        :param pulumi.Input[str] idle_delete_ttl: The duration to keep the cluster alive while idling
               (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
        """
        __self__.auto_delete_time = auto_delete_time
        __self__.idle_delete_ttl = idle_delete_ttl
        __self__.idle_start_time = idle_start_time

@pulumi.input_type
class ClusterClusterConfigMasterConfigArgs:
    accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]]] = pulumi.input_property("accelerators")
    """
    The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    """
    disk_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs']] = pulumi.input_property("diskConfig")
    """
    Disk Config
    """
    image_uri: Optional[pulumi.Input[str]] = pulumi.input_property("imageUri")
    """
    The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
    for more information.
    """
    instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("instanceNames")
    machine_type: Optional[pulumi.Input[str]] = pulumi.input_property("machineType")
    """
    The name of a Google Compute Engine machine type
    to create for the worker nodes. If not specified, GCP will default to a predetermined
    computed value (currently `n1-standard-4`).
    """
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    """
    The name of a minimum generation of CPU family
    for the master. If not specified, GCP will default to a predetermined computed value
    for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for details about which CPU families are available (and defaulted) for each zone.
    """
    num_instances: Optional[pulumi.Input[float]] = pulumi.input_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]]] = None, disk_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs']] = None, image_uri: Optional[pulumi.Input[str]] = None, instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, machine_type: Optional[pulumi.Input[str]] = None, min_cpu_platform: Optional[pulumi.Input[str]] = None, num_instances: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
        :param pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[str] image_uri: The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
               for more information.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type
               to create for the worker nodes. If not specified, GCP will default to a predetermined
               computed value (currently `n1-standard-4`).
        :param pulumi.Input[str] min_cpu_platform: The name of a minimum generation of CPU family
               for the master. If not specified, GCP will default to a predetermined computed value
               for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for details about which CPU families are available (and defaulted) for each zone.
        :param pulumi.Input[float] num_instances: Specifies the number of preemptible nodes to create.
               Defaults to 0.
        """
        __self__.accelerators = accelerators
        __self__.disk_config = disk_config
        __self__.image_uri = image_uri
        __self__.instance_names = instance_names
        __self__.machine_type = machine_type
        __self__.min_cpu_platform = min_cpu_platform
        __self__.num_instances = num_instances

@pulumi.input_type
class ClusterClusterConfigMasterConfigAcceleratorArgs:
    accelerator_count: pulumi.Input[float] = pulumi.input_property("acceleratorCount")
    """
    The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
    """
    accelerator_type: pulumi.Input[str] = pulumi.input_property("acceleratorType")
    """
    The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, accelerator_count: pulumi.Input[float], accelerator_type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[float] accelerator_count: The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
        :param pulumi.Input[str] accelerator_type: The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        __self__.accelerator_count = accelerator_count
        __self__.accelerator_type = accelerator_type

@pulumi.input_type
class ClusterClusterConfigMasterConfigDiskConfigArgs:
    boot_disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[pulumi.Input[float]] = pulumi.input_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_size_gb: Optional[pulumi.Input[float]] = None, boot_disk_type: Optional[pulumi.Input[str]] = None, num_local_ssds: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[float] boot_disk_size_gb: Size of the primary disk attached to each preemptible worker node, specified
               in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each preemptible worker node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[float] num_local_ssds: The amount of local SSD disks that will be
               attached to each preemptible worker node. Defaults to 0.
        """
        __self__.boot_disk_size_gb = boot_disk_size_gb
        __self__.boot_disk_type = boot_disk_type
        __self__.num_local_ssds = num_local_ssds

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigArgs:
    disk_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs']] = pulumi.input_property("diskConfig")
    """
    Disk Config
    """
    instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("instanceNames")
    num_instances: Optional[pulumi.Input[float]] = pulumi.input_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disk_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs']] = None, instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, num_instances: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[float] num_instances: Specifies the number of preemptible nodes to create.
               Defaults to 0.
        """
        __self__.disk_config = disk_config
        __self__.instance_names = instance_names
        __self__.num_instances = num_instances

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs:
    boot_disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[pulumi.Input[float]] = pulumi.input_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_size_gb: Optional[pulumi.Input[float]] = None, boot_disk_type: Optional[pulumi.Input[str]] = None, num_local_ssds: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[float] boot_disk_size_gb: Size of the primary disk attached to each preemptible worker node, specified
               in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each preemptible worker node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[float] num_local_ssds: The amount of local SSD disks that will be
               attached to each preemptible worker node. Defaults to 0.
        """
        __self__.boot_disk_size_gb = boot_disk_size_gb
        __self__.boot_disk_type = boot_disk_type
        __self__.num_local_ssds = num_local_ssds

@pulumi.input_type
class ClusterClusterConfigSecurityConfigArgs:
    kerberos_config: pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs'] = pulumi.input_property("kerberosConfig")
    """
    Kerberos Configuration
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, kerberos_config: pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs']) -> None:
        """
        :param pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs'] kerberos_config: Kerberos Configuration
        """
        __self__.kerberos_config = kerberos_config

@pulumi.input_type
class ClusterClusterConfigSecurityConfigKerberosConfigArgs:
    kms_key_uri: pulumi.Input[str] = pulumi.input_property("kmsKeyUri")
    """
    The URI of the KMS key used to encrypt various sensitive files.
    """
    root_principal_password_uri: pulumi.Input[str] = pulumi.input_property("rootPrincipalPasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file
    containing the root principal password.
    """
    cross_realm_trust_admin_server: Optional[pulumi.Input[str]] = pulumi.input_property("crossRealmTrustAdminServer")
    """
    The admin server (IP or hostname) for the
    remote trusted realm in a cross realm trust relationship.
    """
    cross_realm_trust_kdc: Optional[pulumi.Input[str]] = pulumi.input_property("crossRealmTrustKdc")
    """
    The KDC (IP or hostname) for the
    remote trusted realm in a cross realm trust relationship.
    """
    cross_realm_trust_realm: Optional[pulumi.Input[str]] = pulumi.input_property("crossRealmTrustRealm")
    """
    The remote realm the Dataproc on-cluster KDC will
    trust, should the user enable cross realm trust.
    """
    cross_realm_trust_shared_password_uri: Optional[pulumi.Input[str]] = pulumi.input_property("crossRealmTrustSharedPasswordUri")
    """
    The Cloud Storage URI of a KMS
    encrypted file containing the shared password between the on-cluster Kerberos realm
    and the remote trusted realm, in a cross realm trust relationship.
    """
    enable_kerberos: Optional[pulumi.Input[bool]] = pulumi.input_property("enableKerberos")
    """
    Flag to indicate whether to Kerberize the cluster.
    """
    kdc_db_key_uri: Optional[pulumi.Input[str]] = pulumi.input_property("kdcDbKeyUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the master key of the KDC database.
    """
    key_password_uri: Optional[pulumi.Input[str]] = pulumi.input_property("keyPasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the password to the user provided key. For the self-signed certificate, this password
    is generated by Dataproc.
    """
    keystore_password_uri: Optional[pulumi.Input[str]] = pulumi.input_property("keystorePasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file containing
    the password to the user provided keystore. For the self-signed certificated, the password
    is generated by Dataproc.
    """
    keystore_uri: Optional[pulumi.Input[str]] = pulumi.input_property("keystoreUri")
    """
    The Cloud Storage URI of the keystore file used for SSL encryption.
    If not provided, Dataproc will provide a self-signed certificate.
    """
    realm: Optional[pulumi.Input[str]] = pulumi.input_property("realm")
    """
    The name of the on-cluster Kerberos realm. If not specified, the
    uppercased domain of hostnames will be the realm.
    """
    tgt_lifetime_hours: Optional[pulumi.Input[float]] = pulumi.input_property("tgtLifetimeHours")
    """
    The lifetime of the ticket granting ticket, in hours.
    """
    truststore_password_uri: Optional[pulumi.Input[str]] = pulumi.input_property("truststorePasswordUri")
    """
    The Cloud Storage URI of a KMS encrypted file
    containing the password to the user provided truststore. For the self-signed
    certificate, this password is generated by Dataproc.
    """
    truststore_uri: Optional[pulumi.Input[str]] = pulumi.input_property("truststoreUri")
    """
    The Cloud Storage URI of the truststore file used for
    SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, kms_key_uri: pulumi.Input[str], root_principal_password_uri: pulumi.Input[str], cross_realm_trust_admin_server: Optional[pulumi.Input[str]] = None, cross_realm_trust_kdc: Optional[pulumi.Input[str]] = None, cross_realm_trust_realm: Optional[pulumi.Input[str]] = None, cross_realm_trust_shared_password_uri: Optional[pulumi.Input[str]] = None, enable_kerberos: Optional[pulumi.Input[bool]] = None, kdc_db_key_uri: Optional[pulumi.Input[str]] = None, key_password_uri: Optional[pulumi.Input[str]] = None, keystore_password_uri: Optional[pulumi.Input[str]] = None, keystore_uri: Optional[pulumi.Input[str]] = None, realm: Optional[pulumi.Input[str]] = None, tgt_lifetime_hours: Optional[pulumi.Input[float]] = None, truststore_password_uri: Optional[pulumi.Input[str]] = None, truststore_uri: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] kms_key_uri: The URI of the KMS key used to encrypt various sensitive files.
        :param pulumi.Input[str] root_principal_password_uri: The Cloud Storage URI of a KMS encrypted file
               containing the root principal password.
        :param pulumi.Input[str] cross_realm_trust_admin_server: The admin server (IP or hostname) for the
               remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_kdc: The KDC (IP or hostname) for the
               remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_realm: The remote realm the Dataproc on-cluster KDC will
               trust, should the user enable cross realm trust.
        :param pulumi.Input[str] cross_realm_trust_shared_password_uri: The Cloud Storage URI of a KMS
               encrypted file containing the shared password between the on-cluster Kerberos realm
               and the remote trusted realm, in a cross realm trust relationship.
        :param pulumi.Input[bool] enable_kerberos: Flag to indicate whether to Kerberize the cluster.
        :param pulumi.Input[str] kdc_db_key_uri: The Cloud Storage URI of a KMS encrypted file containing
               the master key of the KDC database.
        :param pulumi.Input[str] key_password_uri: The Cloud Storage URI of a KMS encrypted file containing
               the password to the user provided key. For the self-signed certificate, this password
               is generated by Dataproc.
        :param pulumi.Input[str] keystore_password_uri: The Cloud Storage URI of a KMS encrypted file containing
               the password to the user provided keystore. For the self-signed certificated, the password
               is generated by Dataproc.
        :param pulumi.Input[str] keystore_uri: The Cloud Storage URI of the keystore file used for SSL encryption.
               If not provided, Dataproc will provide a self-signed certificate.
        :param pulumi.Input[str] realm: The name of the on-cluster Kerberos realm. If not specified, the
               uppercased domain of hostnames will be the realm.
        :param pulumi.Input[float] tgt_lifetime_hours: The lifetime of the ticket granting ticket, in hours.
        :param pulumi.Input[str] truststore_password_uri: The Cloud Storage URI of a KMS encrypted file
               containing the password to the user provided truststore. For the self-signed
               certificate, this password is generated by Dataproc.
        :param pulumi.Input[str] truststore_uri: The Cloud Storage URI of the truststore file used for
               SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        """
        __self__.kms_key_uri = kms_key_uri
        __self__.root_principal_password_uri = root_principal_password_uri
        __self__.cross_realm_trust_admin_server = cross_realm_trust_admin_server
        __self__.cross_realm_trust_kdc = cross_realm_trust_kdc
        __self__.cross_realm_trust_realm = cross_realm_trust_realm
        __self__.cross_realm_trust_shared_password_uri = cross_realm_trust_shared_password_uri
        __self__.enable_kerberos = enable_kerberos
        __self__.kdc_db_key_uri = kdc_db_key_uri
        __self__.key_password_uri = key_password_uri
        __self__.keystore_password_uri = keystore_password_uri
        __self__.keystore_uri = keystore_uri
        __self__.realm = realm
        __self__.tgt_lifetime_hours = tgt_lifetime_hours
        __self__.truststore_password_uri = truststore_password_uri
        __self__.truststore_uri = truststore_uri

@pulumi.input_type
class ClusterClusterConfigSoftwareConfigArgs:
    image_version: Optional[pulumi.Input[str]] = pulumi.input_property("imageVersion")
    """
    The Cloud Dataproc image version to use
    for the cluster - this controls the sets of software versions
    installed onto the nodes when you create clusters. If not specified, defaults to the
    latest version. For a list of valid versions see
    [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
    """
    optional_components: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("optionalComponents")
    """
    The set of optional components to activate on the cluster. 
    Accepted values are:
    * ANACONDA
    * DRUID
    * HBASE
    * HIVE_WEBHCAT
    * JUPYTER
    * KERBEROS
    * PRESTO
    * RANGER
    * SOLR
    * ZEPPELIN
    * ZOOKEEPER
    """
    override_properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("overrideProperties")
    """
    A list of override and additional properties (key/value pairs)
    used to modify various aspects of the common configuration files used when creating
    a cluster. For a list of valid properties please see
    [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
    """
    properties: Optional[pulumi.Input[Dict[str, Any]]] = pulumi.input_property("properties")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, image_version: Optional[pulumi.Input[str]] = None, optional_components: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, override_properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, properties: Optional[pulumi.Input[Dict[str, Any]]] = None) -> None:
        """
        :param pulumi.Input[str] image_version: The Cloud Dataproc image version to use
               for the cluster - this controls the sets of software versions
               installed onto the nodes when you create clusters. If not specified, defaults to the
               latest version. For a list of valid versions see
               [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
        :param pulumi.Input[List[pulumi.Input[str]]] optional_components: The set of optional components to activate on the cluster. 
               Accepted values are:
               * ANACONDA
               * DRUID
               * HBASE
               * HIVE_WEBHCAT
               * JUPYTER
               * KERBEROS
               * PRESTO
               * RANGER
               * SOLR
               * ZEPPELIN
               * ZOOKEEPER
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] override_properties: A list of override and additional properties (key/value pairs)
               used to modify various aspects of the common configuration files used when creating
               a cluster. For a list of valid properties please see
               [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
        """
        __self__.image_version = image_version
        __self__.optional_components = optional_components
        __self__.override_properties = override_properties
        __self__.properties = properties

@pulumi.input_type
class ClusterClusterConfigWorkerConfigArgs:
    accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]]] = pulumi.input_property("accelerators")
    """
    The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    """
    disk_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs']] = pulumi.input_property("diskConfig")
    """
    Disk Config
    """
    image_uri: Optional[pulumi.Input[str]] = pulumi.input_property("imageUri")
    """
    The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
    for more information.
    """
    instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("instanceNames")
    machine_type: Optional[pulumi.Input[str]] = pulumi.input_property("machineType")
    """
    The name of a Google Compute Engine machine type
    to create for the worker nodes. If not specified, GCP will default to a predetermined
    computed value (currently `n1-standard-4`).
    """
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    """
    The name of a minimum generation of CPU family
    for the master. If not specified, GCP will default to a predetermined computed value
    for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for details about which CPU families are available (and defaulted) for each zone.
    """
    num_instances: Optional[pulumi.Input[float]] = pulumi.input_property("numInstances")
    """
    Specifies the number of preemptible nodes to create.
    Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]]] = None, disk_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs']] = None, image_uri: Optional[pulumi.Input[str]] = None, instance_names: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, machine_type: Optional[pulumi.Input[str]] = None, min_cpu_platform: Optional[pulumi.Input[str]] = None, num_instances: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
        :param pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[str] image_uri: The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
               for more information.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type
               to create for the worker nodes. If not specified, GCP will default to a predetermined
               computed value (currently `n1-standard-4`).
        :param pulumi.Input[str] min_cpu_platform: The name of a minimum generation of CPU family
               for the master. If not specified, GCP will default to a predetermined computed value
               for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for details about which CPU families are available (and defaulted) for each zone.
        :param pulumi.Input[float] num_instances: Specifies the number of preemptible nodes to create.
               Defaults to 0.
        """
        __self__.accelerators = accelerators
        __self__.disk_config = disk_config
        __self__.image_uri = image_uri
        __self__.instance_names = instance_names
        __self__.machine_type = machine_type
        __self__.min_cpu_platform = min_cpu_platform
        __self__.num_instances = num_instances

@pulumi.input_type
class ClusterClusterConfigWorkerConfigAcceleratorArgs:
    accelerator_count: pulumi.Input[float] = pulumi.input_property("acceleratorCount")
    """
    The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
    """
    accelerator_type: pulumi.Input[str] = pulumi.input_property("acceleratorType")
    """
    The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, accelerator_count: pulumi.Input[float], accelerator_type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[float] accelerator_count: The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
        :param pulumi.Input[str] accelerator_type: The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        __self__.accelerator_count = accelerator_count
        __self__.accelerator_type = accelerator_type

@pulumi.input_type
class ClusterClusterConfigWorkerConfigDiskConfigArgs:
    boot_disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("bootDiskSizeGb")
    """
    Size of the primary disk attached to each preemptible worker node, specified
    in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
    computed value if not set (currently 500GB). Note: If SSDs are not
    attached, it also contains the HDFS data blocks and Hadoop working directories.
    """
    boot_disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskType")
    """
    The disk type of the primary disk attached to each preemptible worker node.
    One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
    """
    num_local_ssds: Optional[pulumi.Input[float]] = pulumi.input_property("numLocalSsds")
    """
    The amount of local SSD disks that will be
    attached to each preemptible worker node. Defaults to 0.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_size_gb: Optional[pulumi.Input[float]] = None, boot_disk_type: Optional[pulumi.Input[str]] = None, num_local_ssds: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[float] boot_disk_size_gb: Size of the primary disk attached to each preemptible worker node, specified
               in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each preemptible worker node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[float] num_local_ssds: The amount of local SSD disks that will be
               attached to each preemptible worker node. Defaults to 0.
        """
        __self__.boot_disk_size_gb = boot_disk_size_gb
        __self__.boot_disk_type = boot_disk_type
        __self__.num_local_ssds = num_local_ssds

@pulumi.input_type
class ClusterIAMBindingConditionArgs:
    expression: pulumi.Input[str] = pulumi.input_property("expression")
    title: pulumi.Input[str] = pulumi.input_property("title")
    description: Optional[pulumi.Input[str]] = pulumi.input_property("description")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, expression: pulumi.Input[str], title: pulumi.Input[str], description: Optional[pulumi.Input[str]] = None) -> None:
        __self__.expression = expression
        __self__.title = title
        __self__.description = description

@pulumi.input_type
class ClusterIAMMemberConditionArgs:
    expression: pulumi.Input[str] = pulumi.input_property("expression")
    title: pulumi.Input[str] = pulumi.input_property("title")
    description: Optional[pulumi.Input[str]] = pulumi.input_property("description")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, expression: pulumi.Input[str], title: pulumi.Input[str], description: Optional[pulumi.Input[str]] = None) -> None:
        __self__.expression = expression
        __self__.title = title
        __self__.description = description

@pulumi.input_type
class JobHadoopConfigArgs:
    archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional[pulumi.Input['JobHadoopConfigLoggingConfigArgs']] = pulumi.input_property("loggingConfig")
    main_class: Optional[pulumi.Input[str]] = pulumi.input_property("mainClass")
    """
    The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
    """
    main_jar_file_uri: Optional[pulumi.Input[str]] = pulumi.input_property("mainJarFileUri")
    """
    The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
    """
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, logging_config: Optional[pulumi.Input['JobHadoopConfigLoggingConfigArgs']] = None, main_class: Optional[pulumi.Input[str]] = None, main_jar_file_uri: Optional[pulumi.Input[str]] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[List[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[List[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[str] main_class: The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        """
        __self__.archive_uris = archive_uris
        __self__.args = args
        __self__.file_uris = file_uris
        __self__.jar_file_uris = jar_file_uris
        __self__.logging_config = logging_config
        __self__.main_class = main_class
        __self__.main_jar_file_uri = main_jar_file_uri
        __self__.properties = properties

@pulumi.input_type
class JobHadoopConfigLoggingConfigArgs:
    driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]] = pulumi.input_property("driverLogLevels")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]]) -> None:
        __self__.driver_log_levels = driver_log_levels

@pulumi.input_type
class JobHiveConfigArgs:
    continue_on_failure: Optional[pulumi.Input[bool]] = pulumi.input_property("continueOnFailure")
    """
    Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    """
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[pulumi.Input[str]] = pulumi.input_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, continue_on_failure: Optional[pulumi.Input[bool]] = None, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, query_file_uri: Optional[pulumi.Input[str]] = None, query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
               Conflicts with `query_list`
        :param pulumi.Input[List[pulumi.Input[str]]] query_lists: The list of SQL queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
        __self__.continue_on_failure = continue_on_failure
        __self__.jar_file_uris = jar_file_uris
        __self__.properties = properties
        __self__.query_file_uri = query_file_uri
        __self__.query_lists = query_lists
        __self__.script_variables = script_variables

@pulumi.input_type
class JobIAMBindingConditionArgs:
    expression: pulumi.Input[str] = pulumi.input_property("expression")
    title: pulumi.Input[str] = pulumi.input_property("title")
    description: Optional[pulumi.Input[str]] = pulumi.input_property("description")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, expression: pulumi.Input[str], title: pulumi.Input[str], description: Optional[pulumi.Input[str]] = None) -> None:
        __self__.expression = expression
        __self__.title = title
        __self__.description = description

@pulumi.input_type
class JobIAMMemberConditionArgs:
    expression: pulumi.Input[str] = pulumi.input_property("expression")
    title: pulumi.Input[str] = pulumi.input_property("title")
    description: Optional[pulumi.Input[str]] = pulumi.input_property("description")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, expression: pulumi.Input[str], title: pulumi.Input[str], description: Optional[pulumi.Input[str]] = None) -> None:
        __self__.expression = expression
        __self__.title = title
        __self__.description = description

@pulumi.input_type
class JobPigConfigArgs:
    continue_on_failure: Optional[pulumi.Input[bool]] = pulumi.input_property("continueOnFailure")
    """
    Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    """
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional[pulumi.Input['JobPigConfigLoggingConfigArgs']] = pulumi.input_property("loggingConfig")
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[pulumi.Input[str]] = pulumi.input_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, continue_on_failure: Optional[pulumi.Input[bool]] = None, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, logging_config: Optional[pulumi.Input['JobPigConfigLoggingConfigArgs']] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, query_file_uri: Optional[pulumi.Input[str]] = None, query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
               Conflicts with `query_list`
        :param pulumi.Input[List[pulumi.Input[str]]] query_lists: The list of SQL queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
        __self__.continue_on_failure = continue_on_failure
        __self__.jar_file_uris = jar_file_uris
        __self__.logging_config = logging_config
        __self__.properties = properties
        __self__.query_file_uri = query_file_uri
        __self__.query_lists = query_lists
        __self__.script_variables = script_variables

@pulumi.input_type
class JobPigConfigLoggingConfigArgs:
    driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]] = pulumi.input_property("driverLogLevels")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]]) -> None:
        __self__.driver_log_levels = driver_log_levels

@pulumi.input_type
class JobPlacementArgs:
    cluster_name: pulumi.Input[str] = pulumi.input_property("clusterName")
    cluster_uuid: Optional[pulumi.Input[str]] = pulumi.input_property("clusterUuid")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, cluster_name: pulumi.Input[str], cluster_uuid: Optional[pulumi.Input[str]] = None) -> None:
        __self__.cluster_name = cluster_name
        __self__.cluster_uuid = cluster_uuid

@pulumi.input_type
class JobPysparkConfigArgs:
    main_python_file_uri: pulumi.Input[str] = pulumi.input_property("mainPythonFileUri")
    """
    The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    """
    archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional[pulumi.Input['JobPysparkConfigLoggingConfigArgs']] = pulumi.input_property("loggingConfig")
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    python_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("pythonFileUris")
    """
    HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, main_python_file_uri: pulumi.Input[str], archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, logging_config: Optional[pulumi.Input['JobPysparkConfigLoggingConfigArgs']] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, python_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[str] main_python_file_uri: The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        :param pulumi.Input[List[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[List[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[List[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        :param pulumi.Input[List[pulumi.Input[str]]] python_file_uris: HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
        __self__.main_python_file_uri = main_python_file_uri
        __self__.archive_uris = archive_uris
        __self__.args = args
        __self__.file_uris = file_uris
        __self__.jar_file_uris = jar_file_uris
        __self__.logging_config = logging_config
        __self__.properties = properties
        __self__.python_file_uris = python_file_uris

@pulumi.input_type
class JobPysparkConfigLoggingConfigArgs:
    driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]] = pulumi.input_property("driverLogLevels")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]]) -> None:
        __self__.driver_log_levels = driver_log_levels

@pulumi.input_type
class JobReferenceArgs:
    job_id: Optional[pulumi.Input[str]] = pulumi.input_property("jobId")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, job_id: Optional[pulumi.Input[str]] = None) -> None:
        __self__.job_id = job_id

@pulumi.input_type
class JobSchedulingArgs:
    max_failures_per_hour: pulumi.Input[float] = pulumi.input_property("maxFailuresPerHour")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_failures_per_hour: pulumi.Input[float]) -> None:
        __self__.max_failures_per_hour = max_failures_per_hour

@pulumi.input_type
class JobSparkConfigArgs:
    archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("archiveUris")
    """
    HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    """
    args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("args")
    """
    The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    """
    file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("fileUris")
    """
    HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    """
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional[pulumi.Input['JobSparkConfigLoggingConfigArgs']] = pulumi.input_property("loggingConfig")
    main_class: Optional[pulumi.Input[str]] = pulumi.input_property("mainClass")
    """
    The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
    """
    main_jar_file_uri: Optional[pulumi.Input[str]] = pulumi.input_property("mainJarFileUri")
    """
    The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
    """
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, archive_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, args: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, logging_config: Optional[pulumi.Input['JobSparkConfigLoggingConfigArgs']] = None, main_class: Optional[pulumi.Input[str]] = None, main_jar_file_uri: Optional[pulumi.Input[str]] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[List[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[List[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[str] main_class: The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        """
        __self__.archive_uris = archive_uris
        __self__.args = args
        __self__.file_uris = file_uris
        __self__.jar_file_uris = jar_file_uris
        __self__.logging_config = logging_config
        __self__.main_class = main_class
        __self__.main_jar_file_uri = main_jar_file_uri
        __self__.properties = properties

@pulumi.input_type
class JobSparkConfigLoggingConfigArgs:
    driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]] = pulumi.input_property("driverLogLevels")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]]) -> None:
        __self__.driver_log_levels = driver_log_levels

@pulumi.input_type
class JobSparksqlConfigArgs:
    jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("jarFileUris")
    """
    HCFS URIs of jar files to be added to the Spark CLASSPATH.
    """
    logging_config: Optional[pulumi.Input['JobSparksqlConfigLoggingConfigArgs']] = pulumi.input_property("loggingConfig")
    properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("properties")
    """
    A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    """
    query_file_uri: Optional[pulumi.Input[str]] = pulumi.input_property("queryFileUri")
    """
    The HCFS URI of the script that contains SQL queries.
    Conflicts with `query_list`
    """
    query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("queryLists")
    """
    The list of SQL queries or statements to execute as part of the job.
    Conflicts with `query_file_uri`
    """
    script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("scriptVariables")
    """
    Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, jar_file_uris: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, logging_config: Optional[pulumi.Input['JobSparksqlConfigLoggingConfigArgs']] = None, properties: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, query_file_uri: Optional[pulumi.Input[str]] = None, query_lists: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, script_variables: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
               Conflicts with `query_list`
        :param pulumi.Input[List[pulumi.Input[str]]] query_lists: The list of SQL queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
        __self__.jar_file_uris = jar_file_uris
        __self__.logging_config = logging_config
        __self__.properties = properties
        __self__.query_file_uri = query_file_uri
        __self__.query_lists = query_lists
        __self__.script_variables = script_variables

@pulumi.input_type
class JobSparksqlConfigLoggingConfigArgs:
    driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]] = pulumi.input_property("driverLogLevels")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, driver_log_levels: pulumi.Input[Dict[str, pulumi.Input[str]]]) -> None:
        __self__.driver_log_levels = driver_log_levels

@pulumi.input_type
class JobStatusArgs:
    details: Optional[pulumi.Input[str]] = pulumi.input_property("details")
    state: Optional[pulumi.Input[str]] = pulumi.input_property("state")
    state_start_time: Optional[pulumi.Input[str]] = pulumi.input_property("stateStartTime")
    substate: Optional[pulumi.Input[str]] = pulumi.input_property("substate")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, details: Optional[pulumi.Input[str]] = None, state: Optional[pulumi.Input[str]] = None, state_start_time: Optional[pulumi.Input[str]] = None, substate: Optional[pulumi.Input[str]] = None) -> None:
        __self__.details = details
        __self__.state = state
        __self__.state_start_time = state_start_time
        __self__.substate = substate

