# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Any, Dict, List, Mapping, Optional, Tuple, Union
from .. import _utilities, _tables

__all__ = [
    'ClusterAddonsConfigArgs',
    'ClusterAddonsConfigCloudrunConfigArgs',
    'ClusterAddonsConfigConfigConnectorConfigArgs',
    'ClusterAddonsConfigDnsCacheConfigArgs',
    'ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs',
    'ClusterAddonsConfigHorizontalPodAutoscalingArgs',
    'ClusterAddonsConfigHttpLoadBalancingArgs',
    'ClusterAddonsConfigIstioConfigArgs',
    'ClusterAddonsConfigKalmConfigArgs',
    'ClusterAddonsConfigNetworkPolicyConfigArgs',
    'ClusterAuthenticatorGroupsConfigArgs',
    'ClusterClusterAutoscalingArgs',
    'ClusterClusterAutoscalingAutoProvisioningDefaultsArgs',
    'ClusterClusterAutoscalingResourceLimitArgs',
    'ClusterClusterTelemetryArgs',
    'ClusterDatabaseEncryptionArgs',
    'ClusterDefaultSnatStatusArgs',
    'ClusterIpAllocationPolicyArgs',
    'ClusterMaintenancePolicyArgs',
    'ClusterMaintenancePolicyDailyMaintenanceWindowArgs',
    'ClusterMaintenancePolicyRecurringWindowArgs',
    'ClusterMasterAuthArgs',
    'ClusterMasterAuthClientCertificateConfigArgs',
    'ClusterMasterAuthorizedNetworksConfigArgs',
    'ClusterMasterAuthorizedNetworksConfigCidrBlockArgs',
    'ClusterNetworkPolicyArgs',
    'ClusterNodeConfigArgs',
    'ClusterNodeConfigGuestAcceleratorArgs',
    'ClusterNodeConfigSandboxConfigArgs',
    'ClusterNodeConfigShieldedInstanceConfigArgs',
    'ClusterNodeConfigTaintArgs',
    'ClusterNodeConfigWorkloadMetadataConfigArgs',
    'ClusterNodePoolArgs',
    'ClusterNodePoolAutoscalingArgs',
    'ClusterNodePoolManagementArgs',
    'ClusterNodePoolNodeConfigArgs',
    'ClusterNodePoolNodeConfigGuestAcceleratorArgs',
    'ClusterNodePoolNodeConfigSandboxConfigArgs',
    'ClusterNodePoolNodeConfigShieldedInstanceConfigArgs',
    'ClusterNodePoolNodeConfigTaintArgs',
    'ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs',
    'ClusterNodePoolUpgradeSettingsArgs',
    'ClusterPodSecurityPolicyConfigArgs',
    'ClusterPrivateClusterConfigArgs',
    'ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs',
    'ClusterReleaseChannelArgs',
    'ClusterResourceUsageExportConfigArgs',
    'ClusterResourceUsageExportConfigBigqueryDestinationArgs',
    'ClusterVerticalPodAutoscalingArgs',
    'ClusterWorkloadIdentityConfigArgs',
    'NodePoolAutoscalingArgs',
    'NodePoolManagementArgs',
    'NodePoolNodeConfigArgs',
    'NodePoolNodeConfigGuestAcceleratorArgs',
    'NodePoolNodeConfigSandboxConfigArgs',
    'NodePoolNodeConfigShieldedInstanceConfigArgs',
    'NodePoolNodeConfigTaintArgs',
    'NodePoolNodeConfigWorkloadMetadataConfigArgs',
    'NodePoolUpgradeSettingsArgs',
]

@pulumi.input_type
class ClusterAddonsConfigArgs:
    def __init__(__self__, *,
                 cloudrun_config: Optional[pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs']] = None,
                 config_connector_config: Optional[pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs']] = None,
                 dns_cache_config: Optional[pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs']] = None,
                 gce_persistent_disk_csi_driver_config: Optional[pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs']] = None,
                 horizontal_pod_autoscaling: Optional[pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs']] = None,
                 http_load_balancing: Optional[pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs']] = None,
                 istio_config: Optional[pulumi.Input['ClusterAddonsConfigIstioConfigArgs']] = None,
                 kalm_config: Optional[pulumi.Input['ClusterAddonsConfigKalmConfigArgs']] = None,
                 network_policy_config: Optional[pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs']] = None):
        """
        :param pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs'] cloudrun_config: .
               The status of the CloudRun addon. It is disabled by default.
               Set `disabled = false` to enable.
        :param pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs'] config_connector_config: .
               The status of the ConfigConnector addon. It is disabled by default; Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs'] dns_cache_config: .
               The status of the NodeLocal DNSCache addon. It is disabled by default.
               Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs'] gce_persistent_disk_csi_driver_config: .
               Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Defaults to disabled; set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs'] horizontal_pod_autoscaling: The status of the Horizontal Pod Autoscaling
               addon, which increases or decreases the number of replica pods a replication controller
               has based on the resource usage of the existing pods.
               It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service.
               It is enabled by default;
               set `disabled = true` to disable.
        :param pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs'] http_load_balancing: The status of the HTTP (L7) load balancing
               controller addon, which makes it easy to set up HTTP load balancers for services in a
               cluster. It is enabled by default; set `disabled = true` to disable.
        :param pulumi.Input['ClusterAddonsConfigIstioConfigArgs'] istio_config: .
               Structure is documented below.
        :param pulumi.Input['ClusterAddonsConfigKalmConfigArgs'] kalm_config: .
               Configuration for the KALM addon, which manages the lifecycle of k8s. It is disabled by default; Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs'] network_policy_config: Whether we should enable the network policy addon
               for the master.  This must be enabled in order to enable network policy for the nodes.
               To enable this, you must also define a `network_policy` block,
               otherwise nothing will happen.
               It can only be disabled if the nodes already do not have network policies enabled.
               Defaults to disabled; set `disabled = false` to enable.
        """
        if cloudrun_config is not None:
            pulumi.set(__self__, "cloudrun_config", cloudrun_config)
        if config_connector_config is not None:
            pulumi.set(__self__, "config_connector_config", config_connector_config)
        if dns_cache_config is not None:
            pulumi.set(__self__, "dns_cache_config", dns_cache_config)
        if gce_persistent_disk_csi_driver_config is not None:
            pulumi.set(__self__, "gce_persistent_disk_csi_driver_config", gce_persistent_disk_csi_driver_config)
        if horizontal_pod_autoscaling is not None:
            pulumi.set(__self__, "horizontal_pod_autoscaling", horizontal_pod_autoscaling)
        if http_load_balancing is not None:
            pulumi.set(__self__, "http_load_balancing", http_load_balancing)
        if istio_config is not None:
            pulumi.set(__self__, "istio_config", istio_config)
        if kalm_config is not None:
            pulumi.set(__self__, "kalm_config", kalm_config)
        if network_policy_config is not None:
            pulumi.set(__self__, "network_policy_config", network_policy_config)

    @property
    @pulumi.getter(name="cloudrunConfig")
    def cloudrun_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs']]:
        """
        .
        The status of the CloudRun addon. It is disabled by default.
        Set `disabled = false` to enable.
        """
        ...

    @cloudrun_config.setter
    def cloudrun_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="configConnectorConfig")
    def config_connector_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs']]:
        """
        .
        The status of the ConfigConnector addon. It is disabled by default; Set `enabled = true` to enable.
        """
        ...

    @config_connector_config.setter
    def config_connector_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="dnsCacheConfig")
    def dns_cache_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs']]:
        """
        .
        The status of the NodeLocal DNSCache addon. It is disabled by default.
        Set `enabled = true` to enable.
        """
        ...

    @dns_cache_config.setter
    def dns_cache_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="gcePersistentDiskCsiDriverConfig")
    def gce_persistent_disk_csi_driver_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs']]:
        """
        .
        Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Defaults to disabled; set `enabled = true` to enable.
        """
        ...

    @gce_persistent_disk_csi_driver_config.setter
    def gce_persistent_disk_csi_driver_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="horizontalPodAutoscaling")
    def horizontal_pod_autoscaling(self) -> Optional[pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs']]:
        """
        The status of the Horizontal Pod Autoscaling
        addon, which increases or decreases the number of replica pods a replication controller
        has based on the resource usage of the existing pods.
        It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service.
        It is enabled by default;
        set `disabled = true` to disable.
        """
        ...

    @horizontal_pod_autoscaling.setter
    def horizontal_pod_autoscaling(self, value: Optional[pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs']]):
        ...

    @property
    @pulumi.getter(name="httpLoadBalancing")
    def http_load_balancing(self) -> Optional[pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs']]:
        """
        The status of the HTTP (L7) load balancing
        controller addon, which makes it easy to set up HTTP load balancers for services in a
        cluster. It is enabled by default; set `disabled = true` to disable.
        """
        ...

    @http_load_balancing.setter
    def http_load_balancing(self, value: Optional[pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs']]):
        ...

    @property
    @pulumi.getter(name="istioConfig")
    def istio_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigIstioConfigArgs']]:
        """
        .
        Structure is documented below.
        """
        ...

    @istio_config.setter
    def istio_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigIstioConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="kalmConfig")
    def kalm_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigKalmConfigArgs']]:
        """
        .
        Configuration for the KALM addon, which manages the lifecycle of k8s. It is disabled by default; Set `enabled = true` to enable.
        """
        ...

    @kalm_config.setter
    def kalm_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigKalmConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="networkPolicyConfig")
    def network_policy_config(self) -> Optional[pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs']]:
        """
        Whether we should enable the network policy addon
        for the master.  This must be enabled in order to enable network policy for the nodes.
        To enable this, you must also define a `network_policy` block,
        otherwise nothing will happen.
        It can only be disabled if the nodes already do not have network policies enabled.
        Defaults to disabled; set `disabled = false` to enable.
        """
        ...

    @network_policy_config.setter
    def network_policy_config(self, value: Optional[pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs']]):
        ...


@pulumi.input_type
class ClusterAddonsConfigCloudrunConfigArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        pulumi.set(__self__, "disabled", disabled)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigConfigConnectorConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigDnsCacheConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigHorizontalPodAutoscalingArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        pulumi.set(__self__, "disabled", disabled)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigHttpLoadBalancingArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        pulumi.set(__self__, "disabled", disabled)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigIstioConfigArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool],
                 auth: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        :param pulumi.Input[str] auth: The authentication type between services in Istio. Available options include `AUTH_MUTUAL_TLS`.
        """
        pulumi.set(__self__, "disabled", disabled)
        if auth is not None:
            pulumi.set(__self__, "auth", auth)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...

    @property
    @pulumi.getter
    def auth(self) -> Optional[pulumi.Input[str]]:
        """
        The authentication type between services in Istio. Available options include `AUTH_MUTUAL_TLS`.
        """
        ...

    @auth.setter
    def auth(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterAddonsConfigKalmConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAddonsConfigNetworkPolicyConfigArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        pulumi.set(__self__, "disabled", disabled)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterAuthenticatorGroupsConfigArgs:
    def __init__(__self__, *,
                 security_group: pulumi.Input[str]):
        """
        :param pulumi.Input[str] security_group: The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format `gke-security-groups@yourdomain.com`.
        """
        pulumi.set(__self__, "security_group", security_group)

    @property
    @pulumi.getter(name="securityGroup")
    def security_group(self) -> pulumi.Input[str]:
        """
        The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format `gke-security-groups@yourdomain.com`.
        """
        ...

    @security_group.setter
    def security_group(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterClusterAutoscalingArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool],
                 auto_provisioning_defaults: Optional[pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs']] = None,
                 autoscaling_profile: Optional[pulumi.Input[str]] = None,
                 resource_limits: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]]] = None):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        :param pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs'] auto_provisioning_defaults: Contains defaults for a node pool created by NAP.
               Structure is documented below.
        :param pulumi.Input[str] autoscaling_profile: ) Configuration
               options for the [Autoscaling profile](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles)
               feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability
               when deciding to remove nodes from a cluster. Can be `BALANCED` or `OPTIMIZE_UTILIZATION`. Defaults to `BALANCED`.
        :param pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]] resource_limits: Global constraints for machine resources in the
               cluster. Configuring the `cpu` and `memory` types is required if node
               auto-provisioning is enabled. These limits will apply to node pool autoscaling
               in addition to node auto-provisioning. Structure is documented below.
        """
        pulumi.set(__self__, "enabled", enabled)
        if auto_provisioning_defaults is not None:
            pulumi.set(__self__, "auto_provisioning_defaults", auto_provisioning_defaults)
        if autoscaling_profile is not None:
            pulumi.set(__self__, "autoscaling_profile", autoscaling_profile)
        if resource_limits is not None:
            pulumi.set(__self__, "resource_limits", resource_limits)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...

    @property
    @pulumi.getter(name="autoProvisioningDefaults")
    def auto_provisioning_defaults(self) -> Optional[pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs']]:
        """
        Contains defaults for a node pool created by NAP.
        Structure is documented below.
        """
        ...

    @auto_provisioning_defaults.setter
    def auto_provisioning_defaults(self, value: Optional[pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs']]):
        ...

    @property
    @pulumi.getter(name="autoscalingProfile")
    def autoscaling_profile(self) -> Optional[pulumi.Input[str]]:
        """
        ) Configuration
        options for the [Autoscaling profile](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles)
        feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability
        when deciding to remove nodes from a cluster. Can be `BALANCED` or `OPTIMIZE_UTILIZATION`. Defaults to `BALANCED`.
        """
        ...

    @autoscaling_profile.setter
    def autoscaling_profile(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="resourceLimits")
    def resource_limits(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]]]:
        """
        Global constraints for machine resources in the
        cluster. Configuring the `cpu` and `memory` types is required if node
        auto-provisioning is enabled. These limits will apply to node pool autoscaling
        in addition to node auto-provisioning. Structure is documented below.
        """
        ...

    @resource_limits.setter
    def resource_limits(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]]]):
        ...


@pulumi.input_type
class ClusterClusterAutoscalingAutoProvisioningDefaultsArgs:
    def __init__(__self__, *,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 service_account: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        """
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if oauth_scopes is not None:
            pulumi.set(__self__, "oauth_scopes", oauth_scopes)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Minimum CPU platform to be used by this instance.
        The instance may be scheduled on the specified or newer CPU platform. Applicable
        values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
        [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for more information.
        """
        ...

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="oauthScopes")
    def oauth_scopes(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The set of Google API scopes to be made available
        on all of the node VMs under the "default" service account. These can be
        either FQDNs, or scope aliases. The following scopes are necessary to ensure
        the correct functioning of the cluster:
        """
        ...

    @oauth_scopes.setter
    def oauth_scopes(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The service account to be used by the Node VMs.
        If not specified, the "default" service account is used.
        In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
        [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
        [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        """
        ...

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterClusterAutoscalingResourceLimitArgs:
    def __init__(__self__, *,
                 resource_type: pulumi.Input[str],
                 maximum: Optional[pulumi.Input[float]] = None,
                 minimum: Optional[pulumi.Input[float]] = None):
        """
        :param pulumi.Input[str] resource_type: The type of the resource. For example, `cpu` and
               `memory`.  See the [guide to using Node Auto-Provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)
               for a list of types.
        :param pulumi.Input[float] maximum: Maximum amount of the resource in the cluster.
        :param pulumi.Input[float] minimum: Minimum amount of the resource in the cluster.
        """
        pulumi.set(__self__, "resource_type", resource_type)
        if maximum is not None:
            pulumi.set(__self__, "maximum", maximum)
        if minimum is not None:
            pulumi.set(__self__, "minimum", minimum)

    @property
    @pulumi.getter(name="resourceType")
    def resource_type(self) -> pulumi.Input[str]:
        """
        The type of the resource. For example, `cpu` and
        `memory`.  See the [guide to using Node Auto-Provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)
        for a list of types.
        """
        ...

    @resource_type.setter
    def resource_type(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def maximum(self) -> Optional[pulumi.Input[float]]:
        """
        Maximum amount of the resource in the cluster.
        """
        ...

    @maximum.setter
    def maximum(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter
    def minimum(self) -> Optional[pulumi.Input[float]]:
        """
        Minimum amount of the resource in the cluster.
        """
        ...

    @minimum.setter
    def minimum(self, value: Optional[pulumi.Input[float]]):
        ...


@pulumi.input_type
class ClusterClusterTelemetryArgs:
    def __init__(__self__, *,
                 type: pulumi.Input[str]):
        """
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        ...

    @type.setter
    def type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterDatabaseEncryptionArgs:
    def __init__(__self__, *,
                 state: pulumi.Input[str],
                 key_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] state: `ENCRYPTED` or `DECRYPTED`
        :param pulumi.Input[str] key_name: the key to use to encrypt/decrypt secrets.  See the [DatabaseEncryption definition](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.DatabaseEncryption) for more information.
        """
        pulumi.set(__self__, "state", state)
        if key_name is not None:
            pulumi.set(__self__, "key_name", key_name)

    @property
    @pulumi.getter
    def state(self) -> pulumi.Input[str]:
        """
        `ENCRYPTED` or `DECRYPTED`
        """
        ...

    @state.setter
    def state(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter(name="keyName")
    def key_name(self) -> Optional[pulumi.Input[str]]:
        """
        the key to use to encrypt/decrypt secrets.  See the [DatabaseEncryption definition](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.DatabaseEncryption) for more information.
        """
        ...

    @key_name.setter
    def key_name(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterDefaultSnatStatusArgs:
    def __init__(__self__, *,
                 disabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        pulumi.set(__self__, "disabled", disabled)

    @property
    @pulumi.getter
    def disabled(self) -> pulumi.Input[bool]:
        """
        The status of the Istio addon, which makes it easy to set up Istio for services in a
        cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        ...

    @disabled.setter
    def disabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterIpAllocationPolicyArgs:
    def __init__(__self__, *,
                 cluster_ipv4_cidr_block: Optional[pulumi.Input[str]] = None,
                 cluster_secondary_range_name: Optional[pulumi.Input[str]] = None,
                 services_ipv4_cidr_block: Optional[pulumi.Input[str]] = None,
                 services_secondary_range_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cluster_ipv4_cidr_block: The IP address range for the cluster pod IPs.
               Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
               to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
               from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
               pick a specific range to use.
        :param pulumi.Input[str] cluster_secondary_range_name: The name of the existing secondary
               range in the cluster's subnetwork to use for pod IP addresses. Alternatively,
               `cluster_ipv4_cidr_block` can be used to automatically create a GKE-managed one.
        :param pulumi.Input[str] services_ipv4_cidr_block: The IP address range of the services IPs in this cluster.
               Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
               to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
               from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
               pick a specific range to use.
        :param pulumi.Input[str] services_secondary_range_name: The name of the existing
               secondary range in the cluster's subnetwork to use for service `ClusterIP`s.
               Alternatively, `services_ipv4_cidr_block` can be used to automatically create a
               GKE-managed one.
        """
        if cluster_ipv4_cidr_block is not None:
            pulumi.set(__self__, "cluster_ipv4_cidr_block", cluster_ipv4_cidr_block)
        if cluster_secondary_range_name is not None:
            pulumi.set(__self__, "cluster_secondary_range_name", cluster_secondary_range_name)
        if services_ipv4_cidr_block is not None:
            pulumi.set(__self__, "services_ipv4_cidr_block", services_ipv4_cidr_block)
        if services_secondary_range_name is not None:
            pulumi.set(__self__, "services_secondary_range_name", services_secondary_range_name)

    @property
    @pulumi.getter(name="clusterIpv4CidrBlock")
    def cluster_ipv4_cidr_block(self) -> Optional[pulumi.Input[str]]:
        """
        The IP address range for the cluster pod IPs.
        Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
        to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
        from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
        pick a specific range to use.
        """
        ...

    @cluster_ipv4_cidr_block.setter
    def cluster_ipv4_cidr_block(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="clusterSecondaryRangeName")
    def cluster_secondary_range_name(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the existing secondary
        range in the cluster's subnetwork to use for pod IP addresses. Alternatively,
        `cluster_ipv4_cidr_block` can be used to automatically create a GKE-managed one.
        """
        ...

    @cluster_secondary_range_name.setter
    def cluster_secondary_range_name(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="servicesIpv4CidrBlock")
    def services_ipv4_cidr_block(self) -> Optional[pulumi.Input[str]]:
        """
        The IP address range of the services IPs in this cluster.
        Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
        to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
        from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
        pick a specific range to use.
        """
        ...

    @services_ipv4_cidr_block.setter
    def services_ipv4_cidr_block(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="servicesSecondaryRangeName")
    def services_secondary_range_name(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the existing
        secondary range in the cluster's subnetwork to use for service `ClusterIP`s.
        Alternatively, `services_ipv4_cidr_block` can be used to automatically create a
        GKE-managed one.
        """
        ...

    @services_secondary_range_name.setter
    def services_secondary_range_name(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterMaintenancePolicyArgs:
    def __init__(__self__, *,
                 daily_maintenance_window: Optional[pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs']] = None,
                 recurring_window: Optional[pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs']] = None):
        """
        :param pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs'] daily_maintenance_window: Time window specified for daily maintenance operations.
               Specify `start_time` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM”,
               where HH : \[00-23\] and MM : \[00-59\] GMT. For example:
        :param pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs'] recurring_window: Time window for
               recurring maintenance operations.
        """
        if daily_maintenance_window is not None:
            pulumi.set(__self__, "daily_maintenance_window", daily_maintenance_window)
        if recurring_window is not None:
            pulumi.set(__self__, "recurring_window", recurring_window)

    @property
    @pulumi.getter(name="dailyMaintenanceWindow")
    def daily_maintenance_window(self) -> Optional[pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs']]:
        """
        Time window specified for daily maintenance operations.
        Specify `start_time` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM”,
        where HH : \[00-23\] and MM : \[00-59\] GMT. For example:
        """
        ...

    @daily_maintenance_window.setter
    def daily_maintenance_window(self, value: Optional[pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs']]):
        ...

    @property
    @pulumi.getter(name="recurringWindow")
    def recurring_window(self) -> Optional[pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs']]:
        """
        Time window for
        recurring maintenance operations.
        """
        ...

    @recurring_window.setter
    def recurring_window(self, value: Optional[pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs']]):
        ...


@pulumi.input_type
class ClusterMaintenancePolicyDailyMaintenanceWindowArgs:
    def __init__(__self__, *,
                 start_time: pulumi.Input[str],
                 duration: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "start_time", start_time)
        if duration is not None:
            pulumi.set(__self__, "duration", duration)

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> pulumi.Input[str]:
        ...

    @start_time.setter
    def start_time(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def duration(self) -> Optional[pulumi.Input[str]]:
        ...

    @duration.setter
    def duration(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterMaintenancePolicyRecurringWindowArgs:
    def __init__(__self__, *,
                 end_time: pulumi.Input[str],
                 recurrence: pulumi.Input[str],
                 start_time: pulumi.Input[str]):
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "recurrence", recurrence)
        pulumi.set(__self__, "start_time", start_time)

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> pulumi.Input[str]:
        ...

    @end_time.setter
    def end_time(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def recurrence(self) -> pulumi.Input[str]:
        ...

    @recurrence.setter
    def recurrence(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> pulumi.Input[str]:
        ...

    @start_time.setter
    def start_time(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterMasterAuthArgs:
    def __init__(__self__, *,
                 client_certificate: Optional[pulumi.Input[str]] = None,
                 client_certificate_config: Optional[pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs']] = None,
                 client_key: Optional[pulumi.Input[str]] = None,
                 cluster_ca_certificate: Optional[pulumi.Input[str]] = None,
                 password: Optional[pulumi.Input[str]] = None,
                 username: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs'] client_certificate_config: Whether client certificate authorization is enabled for this cluster.  For example:
        :param pulumi.Input[str] password: The password to use for HTTP basic authentication when accessing
               the Kubernetes master endpoint.
        :param pulumi.Input[str] username: The username to use for HTTP basic authentication when accessing
               the Kubernetes master endpoint. If not present basic auth will be disabled.
        """
        if client_certificate is not None:
            pulumi.set(__self__, "client_certificate", client_certificate)
        if client_certificate_config is not None:
            pulumi.set(__self__, "client_certificate_config", client_certificate_config)
        if client_key is not None:
            pulumi.set(__self__, "client_key", client_key)
        if cluster_ca_certificate is not None:
            pulumi.set(__self__, "cluster_ca_certificate", cluster_ca_certificate)
        if password is not None:
            pulumi.set(__self__, "password", password)
        if username is not None:
            pulumi.set(__self__, "username", username)

    @property
    @pulumi.getter(name="clientCertificate")
    def client_certificate(self) -> Optional[pulumi.Input[str]]:
        ...

    @client_certificate.setter
    def client_certificate(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="clientCertificateConfig")
    def client_certificate_config(self) -> Optional[pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs']]:
        """
        Whether client certificate authorization is enabled for this cluster.  For example:
        """
        ...

    @client_certificate_config.setter
    def client_certificate_config(self, value: Optional[pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="clientKey")
    def client_key(self) -> Optional[pulumi.Input[str]]:
        ...

    @client_key.setter
    def client_key(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="clusterCaCertificate")
    def cluster_ca_certificate(self) -> Optional[pulumi.Input[str]]:
        ...

    @cluster_ca_certificate.setter
    def cluster_ca_certificate(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def password(self) -> Optional[pulumi.Input[str]]:
        """
        The password to use for HTTP basic authentication when accessing
        the Kubernetes master endpoint.
        """
        ...

    @password.setter
    def password(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def username(self) -> Optional[pulumi.Input[str]]:
        """
        The username to use for HTTP basic authentication when accessing
        the Kubernetes master endpoint. If not present basic auth will be disabled.
        """
        ...

    @username.setter
    def username(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterMasterAuthClientCertificateConfigArgs:
    def __init__(__self__, *,
                 issue_client_certificate: pulumi.Input[bool]):
        pulumi.set(__self__, "issue_client_certificate", issue_client_certificate)

    @property
    @pulumi.getter(name="issueClientCertificate")
    def issue_client_certificate(self) -> pulumi.Input[bool]:
        ...

    @issue_client_certificate.setter
    def issue_client_certificate(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterMasterAuthorizedNetworksConfigArgs:
    def __init__(__self__, *,
                 cidr_blocks: Optional[pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]]] = None):
        """
        :param pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]] cidr_blocks: External networks that can access the
               Kubernetes cluster master through HTTPS.
        """
        if cidr_blocks is not None:
            pulumi.set(__self__, "cidr_blocks", cidr_blocks)

    @property
    @pulumi.getter(name="cidrBlocks")
    def cidr_blocks(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]]]:
        """
        External networks that can access the
        Kubernetes cluster master through HTTPS.
        """
        ...

    @cidr_blocks.setter
    def cidr_blocks(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]]]):
        ...


@pulumi.input_type
class ClusterMasterAuthorizedNetworksConfigCidrBlockArgs:
    def __init__(__self__, *,
                 cidr_block: pulumi.Input[str],
                 display_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cidr_block: External network that can access Kubernetes master through HTTPS.
               Must be specified in CIDR notation.
        :param pulumi.Input[str] display_name: Field for users to identify CIDR blocks.
        """
        pulumi.set(__self__, "cidr_block", cidr_block)
        if display_name is not None:
            pulumi.set(__self__, "display_name", display_name)

    @property
    @pulumi.getter(name="cidrBlock")
    def cidr_block(self) -> pulumi.Input[str]:
        """
        External network that can access Kubernetes master through HTTPS.
        Must be specified in CIDR notation.
        """
        ...

    @cidr_block.setter
    def cidr_block(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> Optional[pulumi.Input[str]]:
        """
        Field for users to identify CIDR blocks.
        """
        ...

    @display_name.setter
    def display_name(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterNetworkPolicyArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool],
                 provider: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        :param pulumi.Input[str] provider: The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
        """
        pulumi.set(__self__, "enabled", enabled)
        if provider is not None:
            pulumi.set(__self__, "provider", provider)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...

    @property
    @pulumi.getter
    def provider(self) -> Optional[pulumi.Input[str]]:
        """
        The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
        """
        ...

    @provider.setter
    def provider(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterNodeConfigArgs:
    def __init__(__self__, *,
                 boot_disk_kms_key: Optional[pulumi.Input[str]] = None,
                 disk_size_gb: Optional[pulumi.Input[float]] = None,
                 disk_type: Optional[pulumi.Input[str]] = None,
                 guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]]] = None,
                 image_type: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 local_ssd_count: Optional[pulumi.Input[float]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 metadata: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 preemptible: Optional[pulumi.Input[bool]] = None,
                 sandbox_config: Optional[pulumi.Input['ClusterNodeConfigSandboxConfigArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 shielded_instance_config: Optional[pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs']] = None,
                 tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]]] = None,
                 workload_metadata_config: Optional[pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs']] = None):
        """
        :param pulumi.Input[str] boot_disk_kms_key: The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        :param pulumi.Input[float] disk_size_gb: Size of the disk attached to each node, specified
               in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        :param pulumi.Input[str] disk_type: Type of the disk attached to each node
               (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        :param pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]] guest_accelerators: List of the type and count of accelerator cards attached to the instance.
               Structure documented below.
        :param pulumi.Input[str] image_type: The image type to use for this node. Note that changing the image type
               will delete and recreate all nodes in the node pool.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The Kubernetes labels (key/value pairs) to be applied to each node.
        :param pulumi.Input[float] local_ssd_count: The amount of local SSD disks that will be
               attached to each cluster node. Defaults to 0.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type.
               Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
               [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] metadata: The metadata key/value pairs assigned to instances in
               the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
               `true` by the API; if `metadata` is set but that default value is not
               included, the provider will attempt to unset the value. To avoid this, set the
               value in your config.
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[bool] preemptible: A boolean that represents whether or not the underlying node VMs
               are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
               for more information. Defaults to false.
        :param pulumi.Input['ClusterNodeConfigSandboxConfigArgs'] sandbox_config: [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
               Structure is documented below.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        :param pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance options. Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] tags: The list of instance tags applied to all nodes. Tags are used to identify
               valid sources or targets for network firewalls.
        :param pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]] taints: A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
               to apply to nodes. GKE's API can only set this field on cluster creation.
               However, GKE will add taints to your nodes if you enable certain features such
               as GPUs. If this field is set, any diffs on this field will cause the provider to
               recreate the underlying resource. Taint values can be updated safely in
               Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
               this field to manage taints. If you do, `lifecycle.ignore_changes` is
               recommended. Structure is documented below.
        :param pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs'] workload_metadata_config: Metadata configuration to expose to workloads on the node pool.
               Structure is documented below.
        """
        if boot_disk_kms_key is not None:
            pulumi.set(__self__, "boot_disk_kms_key", boot_disk_kms_key)
        if disk_size_gb is not None:
            pulumi.set(__self__, "disk_size_gb", disk_size_gb)
        if disk_type is not None:
            pulumi.set(__self__, "disk_type", disk_type)
        if guest_accelerators is not None:
            pulumi.set(__self__, "guest_accelerators", guest_accelerators)
        if image_type is not None:
            pulumi.set(__self__, "image_type", image_type)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if local_ssd_count is not None:
            pulumi.set(__self__, "local_ssd_count", local_ssd_count)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if oauth_scopes is not None:
            pulumi.set(__self__, "oauth_scopes", oauth_scopes)
        if preemptible is not None:
            pulumi.set(__self__, "preemptible", preemptible)
        if sandbox_config is not None:
            pulumi.set(__self__, "sandbox_config", sandbox_config)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if shielded_instance_config is not None:
            pulumi.set(__self__, "shielded_instance_config", shielded_instance_config)
        if tags is not None:
            pulumi.set(__self__, "tags", tags)
        if taints is not None:
            pulumi.set(__self__, "taints", taints)
        if workload_metadata_config is not None:
            pulumi.set(__self__, "workload_metadata_config", workload_metadata_config)

    @property
    @pulumi.getter(name="bootDiskKmsKey")
    def boot_disk_kms_key(self) -> Optional[pulumi.Input[str]]:
        """
        The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        """
        ...

    @boot_disk_kms_key.setter
    def boot_disk_kms_key(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="diskSizeGb")
    def disk_size_gb(self) -> Optional[pulumi.Input[float]]:
        """
        Size of the disk attached to each node, specified
        in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        """
        ...

    @disk_size_gb.setter
    def disk_size_gb(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="diskType")
    def disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the disk attached to each node
        (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        """
        ...

    @disk_type.setter
    def disk_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="guestAccelerators")
    def guest_accelerators(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]]]:
        """
        List of the type and count of accelerator cards attached to the instance.
        Structure documented below.
        """
        ...

    @guest_accelerators.setter
    def guest_accelerators(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]]]):
        ...

    @property
    @pulumi.getter(name="imageType")
    def image_type(self) -> Optional[pulumi.Input[str]]:
        """
        The image type to use for this node. Note that changing the image type
        will delete and recreate all nodes in the node pool.
        """
        ...

    @image_type.setter
    def image_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The Kubernetes labels (key/value pairs) to be applied to each node.
        """
        ...

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="localSsdCount")
    def local_ssd_count(self) -> Optional[pulumi.Input[float]]:
        """
        The amount of local SSD disks that will be
        attached to each cluster node. Defaults to 0.
        """
        ...

    @local_ssd_count.setter
    def local_ssd_count(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Google Compute Engine machine type.
        Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
        [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        """
        ...

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The metadata key/value pairs assigned to instances in
        the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
        `true` by the API; if `metadata` is set but that default value is not
        included, the provider will attempt to unset the value. To avoid this, set the
        value in your config.
        """
        ...

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Minimum CPU platform to be used by this instance.
        The instance may be scheduled on the specified or newer CPU platform. Applicable
        values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
        [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for more information.
        """
        ...

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="oauthScopes")
    def oauth_scopes(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The set of Google API scopes to be made available
        on all of the node VMs under the "default" service account. These can be
        either FQDNs, or scope aliases. The following scopes are necessary to ensure
        the correct functioning of the cluster:
        """
        ...

    @oauth_scopes.setter
    def oauth_scopes(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        A boolean that represents whether or not the underlying node VMs
        are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
        for more information. Defaults to false.
        """
        ...

    @preemptible.setter
    def preemptible(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="sandboxConfig")
    def sandbox_config(self) -> Optional[pulumi.Input['ClusterNodeConfigSandboxConfigArgs']]:
        """
        [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
        Structure is documented below.
        """
        ...

    @sandbox_config.setter
    def sandbox_config(self, value: Optional[pulumi.Input['ClusterNodeConfigSandboxConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The service account to be used by the Node VMs.
        If not specified, the "default" service account is used.
        In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
        [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
        [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        """
        ...

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="shieldedInstanceConfig")
    def shielded_instance_config(self) -> Optional[pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs']]:
        """
        Shielded Instance options. Structure is documented below.
        """
        ...

    @shielded_instance_config.setter
    def shielded_instance_config(self, value: Optional[pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs']]):
        ...

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The list of instance tags applied to all nodes. Tags are used to identify
        valid sources or targets for network firewalls.
        """
        ...

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def taints(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]]]:
        """
        A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
        to apply to nodes. GKE's API can only set this field on cluster creation.
        However, GKE will add taints to your nodes if you enable certain features such
        as GPUs. If this field is set, any diffs on this field will cause the provider to
        recreate the underlying resource. Taint values can be updated safely in
        Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
        this field to manage taints. If you do, `lifecycle.ignore_changes` is
        recommended. Structure is documented below.
        """
        ...

    @taints.setter
    def taints(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]]]):
        ...

    @property
    @pulumi.getter(name="workloadMetadataConfig")
    def workload_metadata_config(self) -> Optional[pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs']]:
        """
        Metadata configuration to expose to workloads on the node pool.
        Structure is documented below.
        """
        ...

    @workload_metadata_config.setter
    def workload_metadata_config(self, value: Optional[pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs']]):
        ...


@pulumi.input_type
class ClusterNodeConfigGuestAcceleratorArgs:
    def __init__(__self__, *,
                 count: pulumi.Input[float],
                 type: pulumi.Input[str]):
        """
        :param pulumi.Input[float] count: The number of the guest accelerator cards exposed to this instance.
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "count", count)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def count(self) -> pulumi.Input[float]:
        """
        The number of the guest accelerator cards exposed to this instance.
        """
        ...

    @count.setter
    def count(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        ...

    @type.setter
    def type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodeConfigSandboxConfigArgs:
    def __init__(__self__, *,
                 sandbox_type: pulumi.Input[str]):
        """
        :param pulumi.Input[str] sandbox_type: Which sandbox to use for pods in the node pool.
               Accepted values are:
        """
        pulumi.set(__self__, "sandbox_type", sandbox_type)

    @property
    @pulumi.getter(name="sandboxType")
    def sandbox_type(self) -> pulumi.Input[str]:
        """
        Which sandbox to use for pods in the node pool.
        Accepted values are:
        """
        ...

    @sandbox_type.setter
    def sandbox_type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodeConfigShieldedInstanceConfigArgs:
    def __init__(__self__, *,
                 enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None,
                 enable_secure_boot: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines if the instance has integrity monitoring enabled.
        :param pulumi.Input[bool] enable_secure_boot: Defines if the instance has Secure Boot enabled.
        """
        if enable_integrity_monitoring is not None:
            pulumi.set(__self__, "enable_integrity_monitoring", enable_integrity_monitoring)
        if enable_secure_boot is not None:
            pulumi.set(__self__, "enable_secure_boot", enable_secure_boot)

    @property
    @pulumi.getter(name="enableIntegrityMonitoring")
    def enable_integrity_monitoring(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines if the instance has integrity monitoring enabled.
        """
        ...

    @enable_integrity_monitoring.setter
    def enable_integrity_monitoring(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="enableSecureBoot")
    def enable_secure_boot(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines if the instance has Secure Boot enabled.
        """
        ...

    @enable_secure_boot.setter
    def enable_secure_boot(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class ClusterNodeConfigTaintArgs:
    def __init__(__self__, *,
                 effect: pulumi.Input[str],
                 key: pulumi.Input[str],
                 value: pulumi.Input[str]):
        """
        :param pulumi.Input[str] effect: Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        :param pulumi.Input[str] key: Key for taint.
        :param pulumi.Input[str] value: Value for taint.
        """
        pulumi.set(__self__, "effect", effect)
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def effect(self) -> pulumi.Input[str]:
        """
        Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        """
        ...

    @effect.setter
    def effect(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        Key for taint.
        """
        ...

    @key.setter
    def key(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[str]:
        """
        Value for taint.
        """
        ...

    @value.setter
    def value(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodeConfigWorkloadMetadataConfigArgs:
    def __init__(__self__, *,
                 node_metadata: pulumi.Input[str]):
        """
        :param pulumi.Input[str] node_metadata: How to expose the node metadata to the workload running on the node.
               Accepted values are:
               * UNSPECIFIED: Not Set
               * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
               * EXPOSE: Expose all VM metadata to pods.
               * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        pulumi.set(__self__, "node_metadata", node_metadata)

    @property
    @pulumi.getter(name="nodeMetadata")
    def node_metadata(self) -> pulumi.Input[str]:
        """
        How to expose the node metadata to the workload running on the node.
        Accepted values are:
        * UNSPECIFIED: Not Set
        * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
        * EXPOSE: Expose all VM metadata to pods.
        * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        ...

    @node_metadata.setter
    def node_metadata(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodePoolArgs:
    def __init__(__self__, *,
                 autoscaling: Optional[pulumi.Input['ClusterNodePoolAutoscalingArgs']] = None,
                 initial_node_count: Optional[pulumi.Input[float]] = None,
                 instance_group_urls: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 management: Optional[pulumi.Input['ClusterNodePoolManagementArgs']] = None,
                 max_pods_per_node: Optional[pulumi.Input[float]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 name_prefix: Optional[pulumi.Input[str]] = None,
                 node_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigArgs']] = None,
                 node_count: Optional[pulumi.Input[float]] = None,
                 node_locations: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 upgrade_settings: Optional[pulumi.Input['ClusterNodePoolUpgradeSettingsArgs']] = None,
                 version: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[float] initial_node_count: The number of nodes to create in this
               cluster's default node pool. In regional or multi-zonal clusters, this is the
               number of nodes per zone. Must be set if `node_pool` is not set. If you're using
               `container.NodePool` objects with no default node pool, you'll need to
               set this to a value of at least `1`, alongside setting
               `remove_default_node_pool` to `true`.
        :param pulumi.Input[List[pulumi.Input[str]]] instance_group_urls: List of instance group URLs which have been assigned
               to the cluster.
        :param pulumi.Input[str] name: The name of the cluster, unique within the project and
               location.
        :param pulumi.Input['ClusterNodePoolNodeConfigArgs'] node_config: Parameters used in creating the default node pool.
               Generally, this field should not be used at the same time as a
               `container.NodePool` or a `node_pool` block; this configuration
               manages the default node pool, which isn't recommended to be used.
               Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] node_locations: The list of zones in which the cluster's nodes
               are located. Nodes must be in the region of their regional cluster or in the
               same region as their cluster's zone for zonal clusters. If this is specified for
               a zonal cluster, omit the cluster's zone.
        """
        if autoscaling is not None:
            pulumi.set(__self__, "autoscaling", autoscaling)
        if initial_node_count is not None:
            pulumi.set(__self__, "initial_node_count", initial_node_count)
        if instance_group_urls is not None:
            pulumi.set(__self__, "instance_group_urls", instance_group_urls)
        if management is not None:
            pulumi.set(__self__, "management", management)
        if max_pods_per_node is not None:
            pulumi.set(__self__, "max_pods_per_node", max_pods_per_node)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if name_prefix is not None:
            pulumi.set(__self__, "name_prefix", name_prefix)
        if node_config is not None:
            pulumi.set(__self__, "node_config", node_config)
        if node_count is not None:
            pulumi.set(__self__, "node_count", node_count)
        if node_locations is not None:
            pulumi.set(__self__, "node_locations", node_locations)
        if upgrade_settings is not None:
            pulumi.set(__self__, "upgrade_settings", upgrade_settings)
        if version is not None:
            pulumi.set(__self__, "version", version)

    @property
    @pulumi.getter
    def autoscaling(self) -> Optional[pulumi.Input['ClusterNodePoolAutoscalingArgs']]:
        ...

    @autoscaling.setter
    def autoscaling(self, value: Optional[pulumi.Input['ClusterNodePoolAutoscalingArgs']]):
        ...

    @property
    @pulumi.getter(name="initialNodeCount")
    def initial_node_count(self) -> Optional[pulumi.Input[float]]:
        """
        The number of nodes to create in this
        cluster's default node pool. In regional or multi-zonal clusters, this is the
        number of nodes per zone. Must be set if `node_pool` is not set. If you're using
        `container.NodePool` objects with no default node pool, you'll need to
        set this to a value of at least `1`, alongside setting
        `remove_default_node_pool` to `true`.
        """
        ...

    @initial_node_count.setter
    def initial_node_count(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="instanceGroupUrls")
    def instance_group_urls(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        List of instance group URLs which have been assigned
        to the cluster.
        """
        ...

    @instance_group_urls.setter
    def instance_group_urls(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def management(self) -> Optional[pulumi.Input['ClusterNodePoolManagementArgs']]:
        ...

    @management.setter
    def management(self, value: Optional[pulumi.Input['ClusterNodePoolManagementArgs']]):
        ...

    @property
    @pulumi.getter(name="maxPodsPerNode")
    def max_pods_per_node(self) -> Optional[pulumi.Input[float]]:
        ...

    @max_pods_per_node.setter
    def max_pods_per_node(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the cluster, unique within the project and
        location.
        """
        ...

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="namePrefix")
    def name_prefix(self) -> Optional[pulumi.Input[str]]:
        ...

    @name_prefix.setter
    def name_prefix(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="nodeConfig")
    def node_config(self) -> Optional[pulumi.Input['ClusterNodePoolNodeConfigArgs']]:
        """
        Parameters used in creating the default node pool.
        Generally, this field should not be used at the same time as a
        `container.NodePool` or a `node_pool` block; this configuration
        manages the default node pool, which isn't recommended to be used.
        Structure is documented below.
        """
        ...

    @node_config.setter
    def node_config(self, value: Optional[pulumi.Input['ClusterNodePoolNodeConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="nodeCount")
    def node_count(self) -> Optional[pulumi.Input[float]]:
        ...

    @node_count.setter
    def node_count(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="nodeLocations")
    def node_locations(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The list of zones in which the cluster's nodes
        are located. Nodes must be in the region of their regional cluster or in the
        same region as their cluster's zone for zonal clusters. If this is specified for
        a zonal cluster, omit the cluster's zone.
        """
        ...

    @node_locations.setter
    def node_locations(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="upgradeSettings")
    def upgrade_settings(self) -> Optional[pulumi.Input['ClusterNodePoolUpgradeSettingsArgs']]:
        ...

    @upgrade_settings.setter
    def upgrade_settings(self, value: Optional[pulumi.Input['ClusterNodePoolUpgradeSettingsArgs']]):
        ...

    @property
    @pulumi.getter
    def version(self) -> Optional[pulumi.Input[str]]:
        ...

    @version.setter
    def version(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterNodePoolAutoscalingArgs:
    def __init__(__self__, *,
                 max_node_count: pulumi.Input[float],
                 min_node_count: pulumi.Input[float]):
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> pulumi.Input[float]:
        ...

    @max_node_count.setter
    def max_node_count(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[float]:
        ...

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[float]):
        ...


@pulumi.input_type
class ClusterNodePoolManagementArgs:
    def __init__(__self__, *,
                 auto_repair: Optional[pulumi.Input[bool]] = None,
                 auto_upgrade: Optional[pulumi.Input[bool]] = None):
        if auto_repair is not None:
            pulumi.set(__self__, "auto_repair", auto_repair)
        if auto_upgrade is not None:
            pulumi.set(__self__, "auto_upgrade", auto_upgrade)

    @property
    @pulumi.getter(name="autoRepair")
    def auto_repair(self) -> Optional[pulumi.Input[bool]]:
        ...

    @auto_repair.setter
    def auto_repair(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="autoUpgrade")
    def auto_upgrade(self) -> Optional[pulumi.Input[bool]]:
        ...

    @auto_upgrade.setter
    def auto_upgrade(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigArgs:
    def __init__(__self__, *,
                 boot_disk_kms_key: Optional[pulumi.Input[str]] = None,
                 disk_size_gb: Optional[pulumi.Input[float]] = None,
                 disk_type: Optional[pulumi.Input[str]] = None,
                 guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]]] = None,
                 image_type: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 local_ssd_count: Optional[pulumi.Input[float]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 metadata: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 preemptible: Optional[pulumi.Input[bool]] = None,
                 sandbox_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 shielded_instance_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs']] = None,
                 tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]]] = None,
                 workload_metadata_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs']] = None):
        """
        :param pulumi.Input[str] boot_disk_kms_key: The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        :param pulumi.Input[float] disk_size_gb: Size of the disk attached to each node, specified
               in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        :param pulumi.Input[str] disk_type: Type of the disk attached to each node
               (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        :param pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]] guest_accelerators: List of the type and count of accelerator cards attached to the instance.
               Structure documented below.
        :param pulumi.Input[str] image_type: The image type to use for this node. Note that changing the image type
               will delete and recreate all nodes in the node pool.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The Kubernetes labels (key/value pairs) to be applied to each node.
        :param pulumi.Input[float] local_ssd_count: The amount of local SSD disks that will be
               attached to each cluster node. Defaults to 0.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type.
               Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
               [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] metadata: The metadata key/value pairs assigned to instances in
               the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
               `true` by the API; if `metadata` is set but that default value is not
               included, the provider will attempt to unset the value. To avoid this, set the
               value in your config.
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[bool] preemptible: A boolean that represents whether or not the underlying node VMs
               are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
               for more information. Defaults to false.
        :param pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs'] sandbox_config: [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
               Structure is documented below.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        :param pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance options. Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] tags: The list of instance tags applied to all nodes. Tags are used to identify
               valid sources or targets for network firewalls.
        :param pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]] taints: A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
               to apply to nodes. GKE's API can only set this field on cluster creation.
               However, GKE will add taints to your nodes if you enable certain features such
               as GPUs. If this field is set, any diffs on this field will cause the provider to
               recreate the underlying resource. Taint values can be updated safely in
               Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
               this field to manage taints. If you do, `lifecycle.ignore_changes` is
               recommended. Structure is documented below.
        :param pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs'] workload_metadata_config: Metadata configuration to expose to workloads on the node pool.
               Structure is documented below.
        """
        if boot_disk_kms_key is not None:
            pulumi.set(__self__, "boot_disk_kms_key", boot_disk_kms_key)
        if disk_size_gb is not None:
            pulumi.set(__self__, "disk_size_gb", disk_size_gb)
        if disk_type is not None:
            pulumi.set(__self__, "disk_type", disk_type)
        if guest_accelerators is not None:
            pulumi.set(__self__, "guest_accelerators", guest_accelerators)
        if image_type is not None:
            pulumi.set(__self__, "image_type", image_type)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if local_ssd_count is not None:
            pulumi.set(__self__, "local_ssd_count", local_ssd_count)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if oauth_scopes is not None:
            pulumi.set(__self__, "oauth_scopes", oauth_scopes)
        if preemptible is not None:
            pulumi.set(__self__, "preemptible", preemptible)
        if sandbox_config is not None:
            pulumi.set(__self__, "sandbox_config", sandbox_config)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if shielded_instance_config is not None:
            pulumi.set(__self__, "shielded_instance_config", shielded_instance_config)
        if tags is not None:
            pulumi.set(__self__, "tags", tags)
        if taints is not None:
            pulumi.set(__self__, "taints", taints)
        if workload_metadata_config is not None:
            pulumi.set(__self__, "workload_metadata_config", workload_metadata_config)

    @property
    @pulumi.getter(name="bootDiskKmsKey")
    def boot_disk_kms_key(self) -> Optional[pulumi.Input[str]]:
        """
        The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        """
        ...

    @boot_disk_kms_key.setter
    def boot_disk_kms_key(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="diskSizeGb")
    def disk_size_gb(self) -> Optional[pulumi.Input[float]]:
        """
        Size of the disk attached to each node, specified
        in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        """
        ...

    @disk_size_gb.setter
    def disk_size_gb(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="diskType")
    def disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the disk attached to each node
        (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        """
        ...

    @disk_type.setter
    def disk_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="guestAccelerators")
    def guest_accelerators(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]]]:
        """
        List of the type and count of accelerator cards attached to the instance.
        Structure documented below.
        """
        ...

    @guest_accelerators.setter
    def guest_accelerators(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]]]):
        ...

    @property
    @pulumi.getter(name="imageType")
    def image_type(self) -> Optional[pulumi.Input[str]]:
        """
        The image type to use for this node. Note that changing the image type
        will delete and recreate all nodes in the node pool.
        """
        ...

    @image_type.setter
    def image_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The Kubernetes labels (key/value pairs) to be applied to each node.
        """
        ...

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="localSsdCount")
    def local_ssd_count(self) -> Optional[pulumi.Input[float]]:
        """
        The amount of local SSD disks that will be
        attached to each cluster node. Defaults to 0.
        """
        ...

    @local_ssd_count.setter
    def local_ssd_count(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Google Compute Engine machine type.
        Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
        [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        """
        ...

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The metadata key/value pairs assigned to instances in
        the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
        `true` by the API; if `metadata` is set but that default value is not
        included, the provider will attempt to unset the value. To avoid this, set the
        value in your config.
        """
        ...

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Minimum CPU platform to be used by this instance.
        The instance may be scheduled on the specified or newer CPU platform. Applicable
        values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
        [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for more information.
        """
        ...

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="oauthScopes")
    def oauth_scopes(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The set of Google API scopes to be made available
        on all of the node VMs under the "default" service account. These can be
        either FQDNs, or scope aliases. The following scopes are necessary to ensure
        the correct functioning of the cluster:
        """
        ...

    @oauth_scopes.setter
    def oauth_scopes(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        A boolean that represents whether or not the underlying node VMs
        are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
        for more information. Defaults to false.
        """
        ...

    @preemptible.setter
    def preemptible(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="sandboxConfig")
    def sandbox_config(self) -> Optional[pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs']]:
        """
        [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
        Structure is documented below.
        """
        ...

    @sandbox_config.setter
    def sandbox_config(self, value: Optional[pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The service account to be used by the Node VMs.
        If not specified, the "default" service account is used.
        In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
        [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
        [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        """
        ...

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="shieldedInstanceConfig")
    def shielded_instance_config(self) -> Optional[pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs']]:
        """
        Shielded Instance options. Structure is documented below.
        """
        ...

    @shielded_instance_config.setter
    def shielded_instance_config(self, value: Optional[pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs']]):
        ...

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        """
        The list of instance tags applied to all nodes. Tags are used to identify
        valid sources or targets for network firewalls.
        """
        ...

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def taints(self) -> Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]]]:
        """
        A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
        to apply to nodes. GKE's API can only set this field on cluster creation.
        However, GKE will add taints to your nodes if you enable certain features such
        as GPUs. If this field is set, any diffs on this field will cause the provider to
        recreate the underlying resource. Taint values can be updated safely in
        Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
        this field to manage taints. If you do, `lifecycle.ignore_changes` is
        recommended. Structure is documented below.
        """
        ...

    @taints.setter
    def taints(self, value: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]]]):
        ...

    @property
    @pulumi.getter(name="workloadMetadataConfig")
    def workload_metadata_config(self) -> Optional[pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs']]:
        """
        Metadata configuration to expose to workloads on the node pool.
        Structure is documented below.
        """
        ...

    @workload_metadata_config.setter
    def workload_metadata_config(self, value: Optional[pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs']]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigGuestAcceleratorArgs:
    def __init__(__self__, *,
                 count: pulumi.Input[float],
                 type: pulumi.Input[str]):
        """
        :param pulumi.Input[float] count: The number of the guest accelerator cards exposed to this instance.
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "count", count)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def count(self) -> pulumi.Input[float]:
        """
        The number of the guest accelerator cards exposed to this instance.
        """
        ...

    @count.setter
    def count(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        ...

    @type.setter
    def type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigSandboxConfigArgs:
    def __init__(__self__, *,
                 sandbox_type: pulumi.Input[str]):
        """
        :param pulumi.Input[str] sandbox_type: Which sandbox to use for pods in the node pool.
               Accepted values are:
        """
        pulumi.set(__self__, "sandbox_type", sandbox_type)

    @property
    @pulumi.getter(name="sandboxType")
    def sandbox_type(self) -> pulumi.Input[str]:
        """
        Which sandbox to use for pods in the node pool.
        Accepted values are:
        """
        ...

    @sandbox_type.setter
    def sandbox_type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigShieldedInstanceConfigArgs:
    def __init__(__self__, *,
                 enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None,
                 enable_secure_boot: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines if the instance has integrity monitoring enabled.
        :param pulumi.Input[bool] enable_secure_boot: Defines if the instance has Secure Boot enabled.
        """
        if enable_integrity_monitoring is not None:
            pulumi.set(__self__, "enable_integrity_monitoring", enable_integrity_monitoring)
        if enable_secure_boot is not None:
            pulumi.set(__self__, "enable_secure_boot", enable_secure_boot)

    @property
    @pulumi.getter(name="enableIntegrityMonitoring")
    def enable_integrity_monitoring(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines if the instance has integrity monitoring enabled.
        """
        ...

    @enable_integrity_monitoring.setter
    def enable_integrity_monitoring(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="enableSecureBoot")
    def enable_secure_boot(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines if the instance has Secure Boot enabled.
        """
        ...

    @enable_secure_boot.setter
    def enable_secure_boot(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigTaintArgs:
    def __init__(__self__, *,
                 effect: pulumi.Input[str],
                 key: pulumi.Input[str],
                 value: pulumi.Input[str]):
        """
        :param pulumi.Input[str] effect: Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        :param pulumi.Input[str] key: Key for taint.
        :param pulumi.Input[str] value: Value for taint.
        """
        pulumi.set(__self__, "effect", effect)
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def effect(self) -> pulumi.Input[str]:
        """
        Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        """
        ...

    @effect.setter
    def effect(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        Key for taint.
        """
        ...

    @key.setter
    def key(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[str]:
        """
        Value for taint.
        """
        ...

    @value.setter
    def value(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs:
    def __init__(__self__, *,
                 node_metadata: pulumi.Input[str]):
        """
        :param pulumi.Input[str] node_metadata: How to expose the node metadata to the workload running on the node.
               Accepted values are:
               * UNSPECIFIED: Not Set
               * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
               * EXPOSE: Expose all VM metadata to pods.
               * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        pulumi.set(__self__, "node_metadata", node_metadata)

    @property
    @pulumi.getter(name="nodeMetadata")
    def node_metadata(self) -> pulumi.Input[str]:
        """
        How to expose the node metadata to the workload running on the node.
        Accepted values are:
        * UNSPECIFIED: Not Set
        * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
        * EXPOSE: Expose all VM metadata to pods.
        * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        ...

    @node_metadata.setter
    def node_metadata(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterNodePoolUpgradeSettingsArgs:
    def __init__(__self__, *,
                 max_surge: pulumi.Input[float],
                 max_unavailable: pulumi.Input[float]):
        pulumi.set(__self__, "max_surge", max_surge)
        pulumi.set(__self__, "max_unavailable", max_unavailable)

    @property
    @pulumi.getter(name="maxSurge")
    def max_surge(self) -> pulumi.Input[float]:
        ...

    @max_surge.setter
    def max_surge(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter(name="maxUnavailable")
    def max_unavailable(self) -> pulumi.Input[float]:
        ...

    @max_unavailable.setter
    def max_unavailable(self, value: pulumi.Input[float]):
        ...


@pulumi.input_type
class ClusterPodSecurityPolicyConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterPrivateClusterConfigArgs:
    def __init__(__self__, *,
                 enable_private_endpoint: pulumi.Input[bool],
                 enable_private_nodes: Optional[pulumi.Input[bool]] = None,
                 master_global_access_config: Optional[pulumi.Input['ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs']] = None,
                 master_ipv4_cidr_block: Optional[pulumi.Input[str]] = None,
                 peering_name: Optional[pulumi.Input[str]] = None,
                 private_endpoint: Optional[pulumi.Input[str]] = None,
                 public_endpoint: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[bool] enable_private_endpoint: When `true`, the cluster's private
               endpoint is used as the cluster endpoint and access through the public endpoint
               is disabled. When `false`, either endpoint can be used. This field only applies
               to private clusters, when `enable_private_nodes` is `true`.
        :param pulumi.Input[bool] enable_private_nodes: Enables the private cluster feature,
               creating a private endpoint on the cluster. In a private cluster, nodes only
               have RFC 1918 private addresses and communicate with the master's private
               endpoint via private networking.
        :param pulumi.Input[str] master_ipv4_cidr_block: The IP range in CIDR notation to use for
               the hosted master network. This range will be used for assigning private IP
               addresses to the cluster master(s) and the ILB VIP. This range must not overlap
               with any other ranges in use within the cluster's network, and it must be a /28
               subnet. See [Private Cluster Limitations](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#limitations)
               for more details. This field only applies to private clusters, when
               `enable_private_nodes` is `true`.
        :param pulumi.Input[str] peering_name: The name of the peering between this cluster and the Google owned VPC.
        :param pulumi.Input[str] private_endpoint: The internal IP address of this cluster's master endpoint.
        :param pulumi.Input[str] public_endpoint: The external IP address of this cluster's master endpoint.
        """
        pulumi.set(__self__, "enable_private_endpoint", enable_private_endpoint)
        if enable_private_nodes is not None:
            pulumi.set(__self__, "enable_private_nodes", enable_private_nodes)
        if master_global_access_config is not None:
            pulumi.set(__self__, "master_global_access_config", master_global_access_config)
        if master_ipv4_cidr_block is not None:
            pulumi.set(__self__, "master_ipv4_cidr_block", master_ipv4_cidr_block)
        if peering_name is not None:
            pulumi.set(__self__, "peering_name", peering_name)
        if private_endpoint is not None:
            pulumi.set(__self__, "private_endpoint", private_endpoint)
        if public_endpoint is not None:
            pulumi.set(__self__, "public_endpoint", public_endpoint)

    @property
    @pulumi.getter(name="enablePrivateEndpoint")
    def enable_private_endpoint(self) -> pulumi.Input[bool]:
        """
        When `true`, the cluster's private
        endpoint is used as the cluster endpoint and access through the public endpoint
        is disabled. When `false`, either endpoint can be used. This field only applies
        to private clusters, when `enable_private_nodes` is `true`.
        """
        ...

    @enable_private_endpoint.setter
    def enable_private_endpoint(self, value: pulumi.Input[bool]):
        ...

    @property
    @pulumi.getter(name="enablePrivateNodes")
    def enable_private_nodes(self) -> Optional[pulumi.Input[bool]]:
        """
        Enables the private cluster feature,
        creating a private endpoint on the cluster. In a private cluster, nodes only
        have RFC 1918 private addresses and communicate with the master's private
        endpoint via private networking.
        """
        ...

    @enable_private_nodes.setter
    def enable_private_nodes(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="masterGlobalAccessConfig")
    def master_global_access_config(self) -> Optional[pulumi.Input['ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs']]:
        ...

    @master_global_access_config.setter
    def master_global_access_config(self, value: Optional[pulumi.Input['ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="masterIpv4CidrBlock")
    def master_ipv4_cidr_block(self) -> Optional[pulumi.Input[str]]:
        """
        The IP range in CIDR notation to use for
        the hosted master network. This range will be used for assigning private IP
        addresses to the cluster master(s) and the ILB VIP. This range must not overlap
        with any other ranges in use within the cluster's network, and it must be a /28
        subnet. See [Private Cluster Limitations](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#limitations)
        for more details. This field only applies to private clusters, when
        `enable_private_nodes` is `true`.
        """
        ...

    @master_ipv4_cidr_block.setter
    def master_ipv4_cidr_block(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="peeringName")
    def peering_name(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the peering between this cluster and the Google owned VPC.
        """
        ...

    @peering_name.setter
    def peering_name(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="privateEndpoint")
    def private_endpoint(self) -> Optional[pulumi.Input[str]]:
        """
        The internal IP address of this cluster's master endpoint.
        """
        ...

    @private_endpoint.setter
    def private_endpoint(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="publicEndpoint")
    def public_endpoint(self) -> Optional[pulumi.Input[str]]:
        """
        The external IP address of this cluster's master endpoint.
        """
        ...

    @public_endpoint.setter
    def public_endpoint(self, value: Optional[pulumi.Input[str]]):
        ...


@pulumi.input_type
class ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterReleaseChannelArgs:
    def __init__(__self__, *,
                 channel: pulumi.Input[str]):
        """
        :param pulumi.Input[str] channel: The selected release channel.
               Accepted values are:
               * UNSPECIFIED: Not set.
               * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
               * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
               * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
        """
        pulumi.set(__self__, "channel", channel)

    @property
    @pulumi.getter
    def channel(self) -> pulumi.Input[str]:
        """
        The selected release channel.
        Accepted values are:
        * UNSPECIFIED: Not set.
        * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
        * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
        * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
        """
        ...

    @channel.setter
    def channel(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterResourceUsageExportConfigArgs:
    def __init__(__self__, *,
                 bigquery_destination: pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs'],
                 enable_network_egress_metering: Optional[pulumi.Input[bool]] = None,
                 enable_resource_consumption_metering: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs'] bigquery_destination: Parameters for using BigQuery as the destination of resource usage export.
        :param pulumi.Input[bool] enable_network_egress_metering: Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created
               in the cluster to meter network egress traffic.
        :param pulumi.Input[bool] enable_resource_consumption_metering: Whether to enable resource
               consumption metering on this cluster. When enabled, a table will be created in
               the resource export BigQuery dataset to store resource consumption data. The
               resulting table can be joined with the resource usage table or with BigQuery
               billing export. Defaults to `true`.
        """
        pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        if enable_network_egress_metering is not None:
            pulumi.set(__self__, "enable_network_egress_metering", enable_network_egress_metering)
        if enable_resource_consumption_metering is not None:
            pulumi.set(__self__, "enable_resource_consumption_metering", enable_resource_consumption_metering)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs']:
        """
        Parameters for using BigQuery as the destination of resource usage export.
        """
        ...

    @bigquery_destination.setter
    def bigquery_destination(self, value: pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs']):
        ...

    @property
    @pulumi.getter(name="enableNetworkEgressMetering")
    def enable_network_egress_metering(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created
        in the cluster to meter network egress traffic.
        """
        ...

    @enable_network_egress_metering.setter
    def enable_network_egress_metering(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="enableResourceConsumptionMetering")
    def enable_resource_consumption_metering(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to enable resource
        consumption metering on this cluster. When enabled, a table will be created in
        the resource export BigQuery dataset to store resource consumption data. The
        resulting table can be joined with the resource usage table or with BigQuery
        billing export. Defaults to `true`.
        """
        ...

    @enable_resource_consumption_metering.setter
    def enable_resource_consumption_metering(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class ClusterResourceUsageExportConfigBigqueryDestinationArgs:
    def __init__(__self__, *,
                 dataset_id: pulumi.Input[str]):
        pulumi.set(__self__, "dataset_id", dataset_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> pulumi.Input[str]:
        ...

    @dataset_id.setter
    def dataset_id(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class ClusterVerticalPodAutoscalingArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Enable the PodSecurityPolicy controller for this cluster.
        If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        ...

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        ...


@pulumi.input_type
class ClusterWorkloadIdentityConfigArgs:
    def __init__(__self__, *,
                 identity_namespace: pulumi.Input[str]):
        """
        :param pulumi.Input[str] identity_namespace: Currently, the only supported identity namespace is the project's default.
        """
        pulumi.set(__self__, "identity_namespace", identity_namespace)

    @property
    @pulumi.getter(name="identityNamespace")
    def identity_namespace(self) -> pulumi.Input[str]:
        """
        Currently, the only supported identity namespace is the project's default.
        """
        ...

    @identity_namespace.setter
    def identity_namespace(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class NodePoolAutoscalingArgs:
    def __init__(__self__, *,
                 max_node_count: pulumi.Input[float],
                 min_node_count: pulumi.Input[float]):
        """
        :param pulumi.Input[float] max_node_count: Maximum number of nodes in the NodePool. Must be >= min_node_count.
        :param pulumi.Input[float] min_node_count: Minimum number of nodes in the NodePool. Must be >=0 and
               <= `max_node_count`.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> pulumi.Input[float]:
        """
        Maximum number of nodes in the NodePool. Must be >= min_node_count.
        """
        ...

    @max_node_count.setter
    def max_node_count(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[float]:
        """
        Minimum number of nodes in the NodePool. Must be >=0 and
        <= `max_node_count`.
        """
        ...

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[float]):
        ...


@pulumi.input_type
class NodePoolManagementArgs:
    def __init__(__self__, *,
                 auto_repair: Optional[pulumi.Input[bool]] = None,
                 auto_upgrade: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] auto_repair: Whether the nodes will be automatically repaired.
        :param pulumi.Input[bool] auto_upgrade: Whether the nodes will be automatically upgraded.
        """
        if auto_repair is not None:
            pulumi.set(__self__, "auto_repair", auto_repair)
        if auto_upgrade is not None:
            pulumi.set(__self__, "auto_upgrade", auto_upgrade)

    @property
    @pulumi.getter(name="autoRepair")
    def auto_repair(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether the nodes will be automatically repaired.
        """
        ...

    @auto_repair.setter
    def auto_repair(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="autoUpgrade")
    def auto_upgrade(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether the nodes will be automatically upgraded.
        """
        ...

    @auto_upgrade.setter
    def auto_upgrade(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class NodePoolNodeConfigArgs:
    def __init__(__self__, *,
                 boot_disk_kms_key: Optional[pulumi.Input[str]] = None,
                 disk_size_gb: Optional[pulumi.Input[float]] = None,
                 disk_type: Optional[pulumi.Input[str]] = None,
                 guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigGuestAcceleratorArgs']]]] = None,
                 image_type: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 local_ssd_count: Optional[pulumi.Input[float]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 metadata: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 preemptible: Optional[pulumi.Input[bool]] = None,
                 sandbox_config: Optional[pulumi.Input['NodePoolNodeConfigSandboxConfigArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 shielded_instance_config: Optional[pulumi.Input['NodePoolNodeConfigShieldedInstanceConfigArgs']] = None,
                 tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None,
                 taints: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigTaintArgs']]]] = None,
                 workload_metadata_config: Optional[pulumi.Input['NodePoolNodeConfigWorkloadMetadataConfigArgs']] = None):
        if boot_disk_kms_key is not None:
            pulumi.set(__self__, "boot_disk_kms_key", boot_disk_kms_key)
        if disk_size_gb is not None:
            pulumi.set(__self__, "disk_size_gb", disk_size_gb)
        if disk_type is not None:
            pulumi.set(__self__, "disk_type", disk_type)
        if guest_accelerators is not None:
            pulumi.set(__self__, "guest_accelerators", guest_accelerators)
        if image_type is not None:
            pulumi.set(__self__, "image_type", image_type)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if local_ssd_count is not None:
            pulumi.set(__self__, "local_ssd_count", local_ssd_count)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if oauth_scopes is not None:
            pulumi.set(__self__, "oauth_scopes", oauth_scopes)
        if preemptible is not None:
            pulumi.set(__self__, "preemptible", preemptible)
        if sandbox_config is not None:
            pulumi.set(__self__, "sandbox_config", sandbox_config)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if shielded_instance_config is not None:
            pulumi.set(__self__, "shielded_instance_config", shielded_instance_config)
        if tags is not None:
            pulumi.set(__self__, "tags", tags)
        if taints is not None:
            pulumi.set(__self__, "taints", taints)
        if workload_metadata_config is not None:
            pulumi.set(__self__, "workload_metadata_config", workload_metadata_config)

    @property
    @pulumi.getter(name="bootDiskKmsKey")
    def boot_disk_kms_key(self) -> Optional[pulumi.Input[str]]:
        ...

    @boot_disk_kms_key.setter
    def boot_disk_kms_key(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="diskSizeGb")
    def disk_size_gb(self) -> Optional[pulumi.Input[float]]:
        ...

    @disk_size_gb.setter
    def disk_size_gb(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="diskType")
    def disk_type(self) -> Optional[pulumi.Input[str]]:
        ...

    @disk_type.setter
    def disk_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="guestAccelerators")
    def guest_accelerators(self) -> Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigGuestAcceleratorArgs']]]]:
        ...

    @guest_accelerators.setter
    def guest_accelerators(self, value: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigGuestAcceleratorArgs']]]]):
        ...

    @property
    @pulumi.getter(name="imageType")
    def image_type(self) -> Optional[pulumi.Input[str]]:
        ...

    @image_type.setter
    def image_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        ...

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="localSsdCount")
    def local_ssd_count(self) -> Optional[pulumi.Input[float]]:
        ...

    @local_ssd_count.setter
    def local_ssd_count(self, value: Optional[pulumi.Input[float]]):
        ...

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        ...

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        ...

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        ...

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="oauthScopes")
    def oauth_scopes(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        ...

    @oauth_scopes.setter
    def oauth_scopes(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def preemptible(self) -> Optional[pulumi.Input[bool]]:
        ...

    @preemptible.setter
    def preemptible(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="sandboxConfig")
    def sandbox_config(self) -> Optional[pulumi.Input['NodePoolNodeConfigSandboxConfigArgs']]:
        ...

    @sandbox_config.setter
    def sandbox_config(self, value: Optional[pulumi.Input['NodePoolNodeConfigSandboxConfigArgs']]):
        ...

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        ...

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        ...

    @property
    @pulumi.getter(name="shieldedInstanceConfig")
    def shielded_instance_config(self) -> Optional[pulumi.Input['NodePoolNodeConfigShieldedInstanceConfigArgs']]:
        ...

    @shielded_instance_config.setter
    def shielded_instance_config(self, value: Optional[pulumi.Input['NodePoolNodeConfigShieldedInstanceConfigArgs']]):
        ...

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[List[pulumi.Input[str]]]]:
        ...

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[List[pulumi.Input[str]]]]):
        ...

    @property
    @pulumi.getter
    def taints(self) -> Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigTaintArgs']]]]:
        ...

    @taints.setter
    def taints(self, value: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigTaintArgs']]]]):
        ...

    @property
    @pulumi.getter(name="workloadMetadataConfig")
    def workload_metadata_config(self) -> Optional[pulumi.Input['NodePoolNodeConfigWorkloadMetadataConfigArgs']]:
        ...

    @workload_metadata_config.setter
    def workload_metadata_config(self, value: Optional[pulumi.Input['NodePoolNodeConfigWorkloadMetadataConfigArgs']]):
        ...


@pulumi.input_type
class NodePoolNodeConfigGuestAcceleratorArgs:
    def __init__(__self__, *,
                 count: pulumi.Input[float],
                 type: pulumi.Input[str]):
        pulumi.set(__self__, "count", count)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def count(self) -> pulumi.Input[float]:
        ...

    @count.setter
    def count(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        ...

    @type.setter
    def type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class NodePoolNodeConfigSandboxConfigArgs:
    def __init__(__self__, *,
                 sandbox_type: pulumi.Input[str]):
        pulumi.set(__self__, "sandbox_type", sandbox_type)

    @property
    @pulumi.getter(name="sandboxType")
    def sandbox_type(self) -> pulumi.Input[str]:
        ...

    @sandbox_type.setter
    def sandbox_type(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class NodePoolNodeConfigShieldedInstanceConfigArgs:
    def __init__(__self__, *,
                 enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None,
                 enable_secure_boot: Optional[pulumi.Input[bool]] = None):
        if enable_integrity_monitoring is not None:
            pulumi.set(__self__, "enable_integrity_monitoring", enable_integrity_monitoring)
        if enable_secure_boot is not None:
            pulumi.set(__self__, "enable_secure_boot", enable_secure_boot)

    @property
    @pulumi.getter(name="enableIntegrityMonitoring")
    def enable_integrity_monitoring(self) -> Optional[pulumi.Input[bool]]:
        ...

    @enable_integrity_monitoring.setter
    def enable_integrity_monitoring(self, value: Optional[pulumi.Input[bool]]):
        ...

    @property
    @pulumi.getter(name="enableSecureBoot")
    def enable_secure_boot(self) -> Optional[pulumi.Input[bool]]:
        ...

    @enable_secure_boot.setter
    def enable_secure_boot(self, value: Optional[pulumi.Input[bool]]):
        ...


@pulumi.input_type
class NodePoolNodeConfigTaintArgs:
    def __init__(__self__, *,
                 effect: pulumi.Input[str],
                 key: pulumi.Input[str],
                 value: pulumi.Input[str]):
        pulumi.set(__self__, "effect", effect)
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def effect(self) -> pulumi.Input[str]:
        ...

    @effect.setter
    def effect(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        ...

    @key.setter
    def key(self, value: pulumi.Input[str]):
        ...

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[str]:
        ...

    @value.setter
    def value(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class NodePoolNodeConfigWorkloadMetadataConfigArgs:
    def __init__(__self__, *,
                 node_metadata: pulumi.Input[str]):
        pulumi.set(__self__, "node_metadata", node_metadata)

    @property
    @pulumi.getter(name="nodeMetadata")
    def node_metadata(self) -> pulumi.Input[str]:
        ...

    @node_metadata.setter
    def node_metadata(self, value: pulumi.Input[str]):
        ...


@pulumi.input_type
class NodePoolUpgradeSettingsArgs:
    def __init__(__self__, *,
                 max_surge: pulumi.Input[float],
                 max_unavailable: pulumi.Input[float]):
        """
        :param pulumi.Input[float] max_surge: The number of additional nodes that can be added to the node pool during
               an upgrade. Increasing `max_surge` raises the number of nodes that can be upgraded simultaneously.
               Can be set to 0 or greater.
        :param pulumi.Input[float] max_unavailable: The number of nodes that can be simultaneously unavailable during
               an upgrade. Increasing `max_unavailable` raises the number of nodes that can be upgraded in
               parallel. Can be set to 0 or greater.
        """
        pulumi.set(__self__, "max_surge", max_surge)
        pulumi.set(__self__, "max_unavailable", max_unavailable)

    @property
    @pulumi.getter(name="maxSurge")
    def max_surge(self) -> pulumi.Input[float]:
        """
        The number of additional nodes that can be added to the node pool during
        an upgrade. Increasing `max_surge` raises the number of nodes that can be upgraded simultaneously.
        Can be set to 0 or greater.
        """
        ...

    @max_surge.setter
    def max_surge(self, value: pulumi.Input[float]):
        ...

    @property
    @pulumi.getter(name="maxUnavailable")
    def max_unavailable(self) -> pulumi.Input[float]:
        """
        The number of nodes that can be simultaneously unavailable during
        an upgrade. Increasing `max_unavailable` raises the number of nodes that can be upgraded in
        parallel. Can be set to 0 or greater.
        """
        ...

    @max_unavailable.setter
    def max_unavailable(self, value: pulumi.Input[float]):
        ...


