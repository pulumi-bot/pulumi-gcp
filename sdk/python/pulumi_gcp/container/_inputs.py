# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Any, Dict, List, Optional, Tuple, Union
from .. import _utilities, _tables
from ._inputs import *
from . import outputs

@pulumi.input_type
class ClusterAddonsConfigArgs:
    cloudrun_config: Optional[pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs']] = pulumi.input_property("cloudrunConfig")
    """
    .
    The status of the CloudRun addon. It is disabled by default.
    Set `disabled = false` to enable.
    """
    config_connector_config: Optional[pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs']] = pulumi.input_property("configConnectorConfig")
    """
    .
    The status of the ConfigConnector addon. It is disabled by default; Set `enabled = true` to enable.
    """
    dns_cache_config: Optional[pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs']] = pulumi.input_property("dnsCacheConfig")
    """
    .
    The status of the NodeLocal DNSCache addon. It is disabled by default.
    Set `enabled = true` to enable.
    """
    gce_persistent_disk_csi_driver_config: Optional[pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs']] = pulumi.input_property("gcePersistentDiskCsiDriverConfig")
    """
    .
    Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Defaults to disabled; set `enabled = true` to enable.
    """
    horizontal_pod_autoscaling: Optional[pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs']] = pulumi.input_property("horizontalPodAutoscaling")
    """
    The status of the Horizontal Pod Autoscaling
    addon, which increases or decreases the number of replica pods a replication controller
    has based on the resource usage of the existing pods.
    It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service.
    It is enabled by default;
    set `disabled = true` to disable.
    """
    http_load_balancing: Optional[pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs']] = pulumi.input_property("httpLoadBalancing")
    """
    The status of the HTTP (L7) load balancing
    controller addon, which makes it easy to set up HTTP load balancers for services in a
    cluster. It is enabled by default; set `disabled = true` to disable.
    """
    istio_config: Optional[pulumi.Input['ClusterAddonsConfigIstioConfigArgs']] = pulumi.input_property("istioConfig")
    """
    .
    Structure is documented below.
    """
    kalm_config: Optional[pulumi.Input['ClusterAddonsConfigKalmConfigArgs']] = pulumi.input_property("kalmConfig")
    """
    .
    Configuration for the KALM addon, which manages the lifecycle of k8s. It is disabled by default; Set `enabled = true` to enable.
    """
    network_policy_config: Optional[pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs']] = pulumi.input_property("networkPolicyConfig")
    """
    Whether we should enable the network policy addon
    for the master.  This must be enabled in order to enable network policy for the nodes.
    To enable this, you must also define a `network_policy` block,
    otherwise nothing will happen.
    It can only be disabled if the nodes already do not have network policies enabled.
    Defaults to disabled; set `disabled = false` to enable.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, cloudrun_config: Optional[pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs']] = None, config_connector_config: Optional[pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs']] = None, dns_cache_config: Optional[pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs']] = None, gce_persistent_disk_csi_driver_config: Optional[pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs']] = None, horizontal_pod_autoscaling: Optional[pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs']] = None, http_load_balancing: Optional[pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs']] = None, istio_config: Optional[pulumi.Input['ClusterAddonsConfigIstioConfigArgs']] = None, kalm_config: Optional[pulumi.Input['ClusterAddonsConfigKalmConfigArgs']] = None, network_policy_config: Optional[pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs']] = None) -> None:
        """
        :param pulumi.Input['ClusterAddonsConfigCloudrunConfigArgs'] cloudrun_config: .
               The status of the CloudRun addon. It is disabled by default.
               Set `disabled = false` to enable.
        :param pulumi.Input['ClusterAddonsConfigConfigConnectorConfigArgs'] config_connector_config: .
               The status of the ConfigConnector addon. It is disabled by default; Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigDnsCacheConfigArgs'] dns_cache_config: .
               The status of the NodeLocal DNSCache addon. It is disabled by default.
               Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs'] gce_persistent_disk_csi_driver_config: .
               Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Defaults to disabled; set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigHorizontalPodAutoscalingArgs'] horizontal_pod_autoscaling: The status of the Horizontal Pod Autoscaling
               addon, which increases or decreases the number of replica pods a replication controller
               has based on the resource usage of the existing pods.
               It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service.
               It is enabled by default;
               set `disabled = true` to disable.
        :param pulumi.Input['ClusterAddonsConfigHttpLoadBalancingArgs'] http_load_balancing: The status of the HTTP (L7) load balancing
               controller addon, which makes it easy to set up HTTP load balancers for services in a
               cluster. It is enabled by default; set `disabled = true` to disable.
        :param pulumi.Input['ClusterAddonsConfigIstioConfigArgs'] istio_config: .
               Structure is documented below.
        :param pulumi.Input['ClusterAddonsConfigKalmConfigArgs'] kalm_config: .
               Configuration for the KALM addon, which manages the lifecycle of k8s. It is disabled by default; Set `enabled = true` to enable.
        :param pulumi.Input['ClusterAddonsConfigNetworkPolicyConfigArgs'] network_policy_config: Whether we should enable the network policy addon
               for the master.  This must be enabled in order to enable network policy for the nodes.
               To enable this, you must also define a `network_policy` block,
               otherwise nothing will happen.
               It can only be disabled if the nodes already do not have network policies enabled.
               Defaults to disabled; set `disabled = false` to enable.
        """
        __self__.cloudrun_config = cloudrun_config
        __self__.config_connector_config = config_connector_config
        __self__.dns_cache_config = dns_cache_config
        __self__.gce_persistent_disk_csi_driver_config = gce_persistent_disk_csi_driver_config
        __self__.horizontal_pod_autoscaling = horizontal_pod_autoscaling
        __self__.http_load_balancing = http_load_balancing
        __self__.istio_config = istio_config
        __self__.kalm_config = kalm_config
        __self__.network_policy_config = network_policy_config

@pulumi.input_type
class ClusterAddonsConfigCloudrunConfigArgs:
    disabled: pulumi.Input[bool] = pulumi.input_property("disabled")
    """
    The status of the Istio addon, which makes it easy to set up Istio for services in a
    cluster. It is disabled by default. Set `disabled = false` to enable.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        __self__.disabled = disabled

@pulumi.input_type
class ClusterAddonsConfigConfigConnectorConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterAddonsConfigDnsCacheConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterAddonsConfigGcePersistentDiskCsiDriverConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterAddonsConfigHorizontalPodAutoscalingArgs:
    disabled: pulumi.Input[bool] = pulumi.input_property("disabled")
    """
    The status of the Istio addon, which makes it easy to set up Istio for services in a
    cluster. It is disabled by default. Set `disabled = false` to enable.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        __self__.disabled = disabled

@pulumi.input_type
class ClusterAddonsConfigHttpLoadBalancingArgs:
    disabled: pulumi.Input[bool] = pulumi.input_property("disabled")
    """
    The status of the Istio addon, which makes it easy to set up Istio for services in a
    cluster. It is disabled by default. Set `disabled = false` to enable.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        __self__.disabled = disabled

@pulumi.input_type
class ClusterAddonsConfigIstioConfigArgs:
    disabled: pulumi.Input[bool] = pulumi.input_property("disabled")
    """
    The status of the Istio addon, which makes it easy to set up Istio for services in a
    cluster. It is disabled by default. Set `disabled = false` to enable.
    """
    auth: Optional[pulumi.Input[str]] = pulumi.input_property("auth")
    """
    The authentication type between services in Istio. Available options include `AUTH_MUTUAL_TLS`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disabled: pulumi.Input[bool], auth: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        :param pulumi.Input[str] auth: The authentication type between services in Istio. Available options include `AUTH_MUTUAL_TLS`.
        """
        __self__.disabled = disabled
        __self__.auth = auth

@pulumi.input_type
class ClusterAddonsConfigKalmConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterAddonsConfigNetworkPolicyConfigArgs:
    disabled: pulumi.Input[bool] = pulumi.input_property("disabled")
    """
    The status of the Istio addon, which makes it easy to set up Istio for services in a
    cluster. It is disabled by default. Set `disabled = false` to enable.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, disabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] disabled: The status of the Istio addon, which makes it easy to set up Istio for services in a
               cluster. It is disabled by default. Set `disabled = false` to enable.
        """
        __self__.disabled = disabled

@pulumi.input_type
class ClusterAuthenticatorGroupsConfigArgs:
    security_group: pulumi.Input[str] = pulumi.input_property("securityGroup")
    """
    The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format `gke-security-groups@yourdomain.com`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, security_group: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] security_group: The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format `gke-security-groups@yourdomain.com`.
        """
        __self__.security_group = security_group

@pulumi.input_type
class ClusterClusterAutoscalingArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """
    auto_provisioning_defaults: Optional[pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs']] = pulumi.input_property("autoProvisioningDefaults")
    """
    Contains defaults for a node pool created by NAP.
    Structure is documented below.
    """
    autoscaling_profile: Optional[pulumi.Input[str]] = pulumi.input_property("autoscalingProfile")
    """
    ) Configuration
    options for the [Autoscaling profile](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles)
    feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability
    when deciding to remove nodes from a cluster. Can be `BALANCED` or `OPTIMIZE_UTILIZATION`. Defaults to `BALANCED`.
    """
    resource_limits: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]]] = pulumi.input_property("resourceLimits")
    """
    Global constraints for machine resources in the
    cluster. Configuring the `cpu` and `memory` types is required if node
    auto-provisioning is enabled. These limits will apply to node pool autoscaling
    in addition to node auto-provisioning. Structure is documented below.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool], auto_provisioning_defaults: Optional[pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs']] = None, autoscaling_profile: Optional[pulumi.Input[str]] = None, resource_limits: Optional[pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]]] = None) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        :param pulumi.Input['ClusterClusterAutoscalingAutoProvisioningDefaultsArgs'] auto_provisioning_defaults: Contains defaults for a node pool created by NAP.
               Structure is documented below.
        :param pulumi.Input[str] autoscaling_profile: ) Configuration
               options for the [Autoscaling profile](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles)
               feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability
               when deciding to remove nodes from a cluster. Can be `BALANCED` or `OPTIMIZE_UTILIZATION`. Defaults to `BALANCED`.
        :param pulumi.Input[List[pulumi.Input['ClusterClusterAutoscalingResourceLimitArgs']]] resource_limits: Global constraints for machine resources in the
               cluster. Configuring the `cpu` and `memory` types is required if node
               auto-provisioning is enabled. These limits will apply to node pool autoscaling
               in addition to node auto-provisioning. Structure is documented below.
        """
        __self__.enabled = enabled
        __self__.auto_provisioning_defaults = auto_provisioning_defaults
        __self__.autoscaling_profile = autoscaling_profile
        __self__.resource_limits = resource_limits

@pulumi.input_type
class ClusterClusterAutoscalingAutoProvisioningDefaultsArgs:
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    """
    Minimum CPU platform to be used by this instance.
    The instance may be scheduled on the specified or newer CPU platform. Applicable
    values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
    [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for more information.
    """
    oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("oauthScopes")
    """
    The set of Google API scopes to be made available
    on all of the node VMs under the "default" service account. These can be
    either FQDNs, or scope aliases. The following scopes are necessary to ensure
    the correct functioning of the cluster:
    """
    service_account: Optional[pulumi.Input[str]] = pulumi.input_property("serviceAccount")
    """
    The service account to be used by the Node VMs.
    If not specified, the "default" service account is used.
    In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
    [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
    [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, min_cpu_platform: Optional[pulumi.Input[str]] = None, oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, service_account: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        """
        __self__.min_cpu_platform = min_cpu_platform
        __self__.oauth_scopes = oauth_scopes
        __self__.service_account = service_account

@pulumi.input_type
class ClusterClusterAutoscalingResourceLimitArgs:
    resource_type: pulumi.Input[str] = pulumi.input_property("resourceType")
    """
    The type of the resource. For example, `cpu` and
    `memory`.  See the [guide to using Node Auto-Provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)
    for a list of types.
    """
    maximum: Optional[pulumi.Input[float]] = pulumi.input_property("maximum")
    """
    Maximum amount of the resource in the cluster.
    """
    minimum: Optional[pulumi.Input[float]] = pulumi.input_property("minimum")
    """
    Minimum amount of the resource in the cluster.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, resource_type: pulumi.Input[str], maximum: Optional[pulumi.Input[float]] = None, minimum: Optional[pulumi.Input[float]] = None) -> None:
        """
        :param pulumi.Input[str] resource_type: The type of the resource. For example, `cpu` and
               `memory`.  See the [guide to using Node Auto-Provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)
               for a list of types.
        :param pulumi.Input[float] maximum: Maximum amount of the resource in the cluster.
        :param pulumi.Input[float] minimum: Minimum amount of the resource in the cluster.
        """
        __self__.resource_type = resource_type
        __self__.maximum = maximum
        __self__.minimum = minimum

@pulumi.input_type
class ClusterClusterTelemetryArgs:
    type: pulumi.Input[str] = pulumi.input_property("type")
    """
    The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        __self__.type = type

@pulumi.input_type
class ClusterDatabaseEncryptionArgs:
    state: pulumi.Input[str] = pulumi.input_property("state")
    """
    `ENCRYPTED` or `DECRYPTED`
    """
    key_name: Optional[pulumi.Input[str]] = pulumi.input_property("keyName")
    """
    the key to use to encrypt/decrypt secrets.  See the [DatabaseEncryption definition](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.DatabaseEncryption) for more information.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, state: pulumi.Input[str], key_name: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] state: `ENCRYPTED` or `DECRYPTED`
        :param pulumi.Input[str] key_name: the key to use to encrypt/decrypt secrets.  See the [DatabaseEncryption definition](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.DatabaseEncryption) for more information.
        """
        __self__.state = state
        __self__.key_name = key_name

@pulumi.input_type
class ClusterIpAllocationPolicyArgs:
    cluster_ipv4_cidr_block: Optional[pulumi.Input[str]] = pulumi.input_property("clusterIpv4CidrBlock")
    """
    The IP address range for the cluster pod IPs.
    Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
    to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
    from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
    pick a specific range to use.
    """
    cluster_secondary_range_name: Optional[pulumi.Input[str]] = pulumi.input_property("clusterSecondaryRangeName")
    """
    The name of the existing secondary
    range in the cluster's subnetwork to use for pod IP addresses. Alternatively,
    `cluster_ipv4_cidr_block` can be used to automatically create a GKE-managed one.
    """
    services_ipv4_cidr_block: Optional[pulumi.Input[str]] = pulumi.input_property("servicesIpv4CidrBlock")
    """
    The IP address range of the services IPs in this cluster.
    Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
    to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
    from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
    pick a specific range to use.
    """
    services_secondary_range_name: Optional[pulumi.Input[str]] = pulumi.input_property("servicesSecondaryRangeName")
    """
    The name of the existing
    secondary range in the cluster's subnetwork to use for service `ClusterIP`s.
    Alternatively, `services_ipv4_cidr_block` can be used to automatically create a
    GKE-managed one.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, cluster_ipv4_cidr_block: Optional[pulumi.Input[str]] = None, cluster_secondary_range_name: Optional[pulumi.Input[str]] = None, services_ipv4_cidr_block: Optional[pulumi.Input[str]] = None, services_secondary_range_name: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] cluster_ipv4_cidr_block: The IP address range for the cluster pod IPs.
               Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
               to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
               from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
               pick a specific range to use.
        :param pulumi.Input[str] cluster_secondary_range_name: The name of the existing secondary
               range in the cluster's subnetwork to use for pod IP addresses. Alternatively,
               `cluster_ipv4_cidr_block` can be used to automatically create a GKE-managed one.
        :param pulumi.Input[str] services_ipv4_cidr_block: The IP address range of the services IPs in this cluster.
               Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
               to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
               from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
               pick a specific range to use.
        :param pulumi.Input[str] services_secondary_range_name: The name of the existing
               secondary range in the cluster's subnetwork to use for service `ClusterIP`s.
               Alternatively, `services_ipv4_cidr_block` can be used to automatically create a
               GKE-managed one.
        """
        __self__.cluster_ipv4_cidr_block = cluster_ipv4_cidr_block
        __self__.cluster_secondary_range_name = cluster_secondary_range_name
        __self__.services_ipv4_cidr_block = services_ipv4_cidr_block
        __self__.services_secondary_range_name = services_secondary_range_name

@pulumi.input_type
class ClusterMaintenancePolicyArgs:
    daily_maintenance_window: Optional[pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs']] = pulumi.input_property("dailyMaintenanceWindow")
    """
    Time window specified for daily maintenance operations.
    Specify `start_time` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM”,
    where HH : \[00-23\] and MM : \[00-59\] GMT. For example:
    """
    recurring_window: Optional[pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs']] = pulumi.input_property("recurringWindow")
    """
    Time window for
    recurring maintenance operations.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, daily_maintenance_window: Optional[pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs']] = None, recurring_window: Optional[pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs']] = None) -> None:
        """
        :param pulumi.Input['ClusterMaintenancePolicyDailyMaintenanceWindowArgs'] daily_maintenance_window: Time window specified for daily maintenance operations.
               Specify `start_time` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM”,
               where HH : \[00-23\] and MM : \[00-59\] GMT. For example:
        :param pulumi.Input['ClusterMaintenancePolicyRecurringWindowArgs'] recurring_window: Time window for
               recurring maintenance operations.
        """
        __self__.daily_maintenance_window = daily_maintenance_window
        __self__.recurring_window = recurring_window

@pulumi.input_type
class ClusterMaintenancePolicyDailyMaintenanceWindowArgs:
    start_time: pulumi.Input[str] = pulumi.input_property("startTime")
    duration: Optional[pulumi.Input[str]] = pulumi.input_property("duration")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, start_time: pulumi.Input[str], duration: Optional[pulumi.Input[str]] = None) -> None:
        __self__.start_time = start_time
        __self__.duration = duration

@pulumi.input_type
class ClusterMaintenancePolicyRecurringWindowArgs:
    end_time: pulumi.Input[str] = pulumi.input_property("endTime")
    recurrence: pulumi.Input[str] = pulumi.input_property("recurrence")
    start_time: pulumi.Input[str] = pulumi.input_property("startTime")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, end_time: pulumi.Input[str], recurrence: pulumi.Input[str], start_time: pulumi.Input[str]) -> None:
        __self__.end_time = end_time
        __self__.recurrence = recurrence
        __self__.start_time = start_time

@pulumi.input_type
class ClusterMasterAuthArgs:
    client_certificate: Optional[pulumi.Input[str]] = pulumi.input_property("clientCertificate")
    client_certificate_config: Optional[pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs']] = pulumi.input_property("clientCertificateConfig")
    """
    Whether client certificate authorization is enabled for this cluster.  For example:
    """
    client_key: Optional[pulumi.Input[str]] = pulumi.input_property("clientKey")
    cluster_ca_certificate: Optional[pulumi.Input[str]] = pulumi.input_property("clusterCaCertificate")
    password: Optional[pulumi.Input[str]] = pulumi.input_property("password")
    """
    The password to use for HTTP basic authentication when accessing
    the Kubernetes master endpoint.
    """
    username: Optional[pulumi.Input[str]] = pulumi.input_property("username")
    """
    The username to use for HTTP basic authentication when accessing
    the Kubernetes master endpoint. If not present basic auth will be disabled.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, client_certificate: Optional[pulumi.Input[str]] = None, client_certificate_config: Optional[pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs']] = None, client_key: Optional[pulumi.Input[str]] = None, cluster_ca_certificate: Optional[pulumi.Input[str]] = None, password: Optional[pulumi.Input[str]] = None, username: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input['ClusterMasterAuthClientCertificateConfigArgs'] client_certificate_config: Whether client certificate authorization is enabled for this cluster.  For example:
        :param pulumi.Input[str] password: The password to use for HTTP basic authentication when accessing
               the Kubernetes master endpoint.
        :param pulumi.Input[str] username: The username to use for HTTP basic authentication when accessing
               the Kubernetes master endpoint. If not present basic auth will be disabled.
        """
        __self__.client_certificate = client_certificate
        __self__.client_certificate_config = client_certificate_config
        __self__.client_key = client_key
        __self__.cluster_ca_certificate = cluster_ca_certificate
        __self__.password = password
        __self__.username = username

@pulumi.input_type
class ClusterMasterAuthClientCertificateConfigArgs:
    issue_client_certificate: pulumi.Input[bool] = pulumi.input_property("issueClientCertificate")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, issue_client_certificate: pulumi.Input[bool]) -> None:
        __self__.issue_client_certificate = issue_client_certificate

@pulumi.input_type
class ClusterMasterAuthorizedNetworksConfigArgs:
    cidr_blocks: Optional[pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]]] = pulumi.input_property("cidrBlocks")
    """
    External networks that can access the
    Kubernetes cluster master through HTTPS.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, cidr_blocks: Optional[pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]]] = None) -> None:
        """
        :param pulumi.Input[List[pulumi.Input['ClusterMasterAuthorizedNetworksConfigCidrBlockArgs']]] cidr_blocks: External networks that can access the
               Kubernetes cluster master through HTTPS.
        """
        __self__.cidr_blocks = cidr_blocks

@pulumi.input_type
class ClusterMasterAuthorizedNetworksConfigCidrBlockArgs:
    cidr_block: pulumi.Input[str] = pulumi.input_property("cidrBlock")
    """
    External network that can access Kubernetes master through HTTPS.
    Must be specified in CIDR notation.
    """
    display_name: Optional[pulumi.Input[str]] = pulumi.input_property("displayName")
    """
    Field for users to identify CIDR blocks.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, cidr_block: pulumi.Input[str], display_name: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[str] cidr_block: External network that can access Kubernetes master through HTTPS.
               Must be specified in CIDR notation.
        :param pulumi.Input[str] display_name: Field for users to identify CIDR blocks.
        """
        __self__.cidr_block = cidr_block
        __self__.display_name = display_name

@pulumi.input_type
class ClusterNetworkPolicyArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """
    provider: Optional[pulumi.Input[str]] = pulumi.input_property("provider")
    """
    The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool], provider: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        :param pulumi.Input[str] provider: The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
        """
        __self__.enabled = enabled
        __self__.provider = provider

@pulumi.input_type
class ClusterNodeConfigArgs:
    boot_disk_kms_key: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskKmsKey")
    """
    The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
    """
    disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("diskSizeGb")
    """
    Size of the disk attached to each node, specified
    in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
    """
    disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("diskType")
    """
    Type of the disk attached to each node
    (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
    """
    guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]]] = pulumi.input_property("guestAccelerators")
    """
    List of the type and count of accelerator cards attached to the instance.
    Structure documented below.
    """
    image_type: Optional[pulumi.Input[str]] = pulumi.input_property("imageType")
    """
    The image type to use for this node. Note that changing the image type
    will delete and recreate all nodes in the node pool.
    """
    labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("labels")
    """
    The Kubernetes labels (key/value pairs) to be applied to each node.
    """
    local_ssd_count: Optional[pulumi.Input[float]] = pulumi.input_property("localSsdCount")
    """
    The amount of local SSD disks that will be
    attached to each cluster node. Defaults to 0.
    """
    machine_type: Optional[pulumi.Input[str]] = pulumi.input_property("machineType")
    """
    The name of a Google Compute Engine machine type.
    Defaults to `n1-standard-1`. To create a custom machine type, value should be set as specified
    [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
    """
    metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("metadata")
    """
    The metadata key/value pairs assigned to instances in
    the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
    `true` by the API; if `metadata` is set but that default value is not
    included, the provider will attempt to unset the value. To avoid this, set the
    value in your config.
    """
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    """
    Minimum CPU platform to be used by this instance.
    The instance may be scheduled on the specified or newer CPU platform. Applicable
    values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
    [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for more information.
    """
    oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("oauthScopes")
    """
    The set of Google API scopes to be made available
    on all of the node VMs under the "default" service account. These can be
    either FQDNs, or scope aliases. The following scopes are necessary to ensure
    the correct functioning of the cluster:
    """
    preemptible: Optional[pulumi.Input[bool]] = pulumi.input_property("preemptible")
    """
    A boolean that represents whether or not the underlying node VMs
    are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
    for more information. Defaults to false.
    """
    sandbox_config: Optional[pulumi.Input['ClusterNodeConfigSandboxConfigArgs']] = pulumi.input_property("sandboxConfig")
    """
    [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
    Structure is documented below.
    """
    service_account: Optional[pulumi.Input[str]] = pulumi.input_property("serviceAccount")
    """
    The service account to be used by the Node VMs.
    If not specified, the "default" service account is used.
    In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
    [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
    [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
    """
    shielded_instance_config: Optional[pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs']] = pulumi.input_property("shieldedInstanceConfig")
    """
    Shielded Instance options. Structure is documented below.
    """
    tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("tags")
    """
    The list of instance tags applied to all nodes. Tags are used to identify
    valid sources or targets for network firewalls.
    """
    taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]]] = pulumi.input_property("taints")
    """
    A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
    to apply to nodes. GKE's API can only set this field on cluster creation.
    However, GKE will add taints to your nodes if you enable certain features such
    as GPUs. If this field is set, any diffs on this field will cause the provider to
    recreate the underlying resource. Taint values can be updated safely in
    Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
    this field to manage taints. If you do, `lifecycle.ignore_changes` is
    recommended. Structure is documented below.
    """
    workload_metadata_config: Optional[pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs']] = pulumi.input_property("workloadMetadataConfig")
    """
    Metadata configuration to expose to workloads on the node pool.
    Structure is documented below.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_kms_key: Optional[pulumi.Input[str]] = None, disk_size_gb: Optional[pulumi.Input[float]] = None, disk_type: Optional[pulumi.Input[str]] = None, guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]]] = None, image_type: Optional[pulumi.Input[str]] = None, labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, local_ssd_count: Optional[pulumi.Input[float]] = None, machine_type: Optional[pulumi.Input[str]] = None, metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, min_cpu_platform: Optional[pulumi.Input[str]] = None, oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, preemptible: Optional[pulumi.Input[bool]] = None, sandbox_config: Optional[pulumi.Input['ClusterNodeConfigSandboxConfigArgs']] = None, service_account: Optional[pulumi.Input[str]] = None, shielded_instance_config: Optional[pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs']] = None, tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]]] = None, workload_metadata_config: Optional[pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs']] = None) -> None:
        """
        :param pulumi.Input[str] boot_disk_kms_key: The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        :param pulumi.Input[float] disk_size_gb: Size of the disk attached to each node, specified
               in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        :param pulumi.Input[str] disk_type: Type of the disk attached to each node
               (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        :param pulumi.Input[List[pulumi.Input['ClusterNodeConfigGuestAcceleratorArgs']]] guest_accelerators: List of the type and count of accelerator cards attached to the instance.
               Structure documented below.
        :param pulumi.Input[str] image_type: The image type to use for this node. Note that changing the image type
               will delete and recreate all nodes in the node pool.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] labels: The Kubernetes labels (key/value pairs) to be applied to each node.
        :param pulumi.Input[float] local_ssd_count: The amount of local SSD disks that will be
               attached to each cluster node. Defaults to 0.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type.
               Defaults to `n1-standard-1`. To create a custom machine type, value should be set as specified
               [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] metadata: The metadata key/value pairs assigned to instances in
               the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
               `true` by the API; if `metadata` is set but that default value is not
               included, the provider will attempt to unset the value. To avoid this, set the
               value in your config.
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[bool] preemptible: A boolean that represents whether or not the underlying node VMs
               are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
               for more information. Defaults to false.
        :param pulumi.Input['ClusterNodeConfigSandboxConfigArgs'] sandbox_config: [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
               Structure is documented below.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        :param pulumi.Input['ClusterNodeConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance options. Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] tags: The list of instance tags applied to all nodes. Tags are used to identify
               valid sources or targets for network firewalls.
        :param pulumi.Input[List[pulumi.Input['ClusterNodeConfigTaintArgs']]] taints: A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
               to apply to nodes. GKE's API can only set this field on cluster creation.
               However, GKE will add taints to your nodes if you enable certain features such
               as GPUs. If this field is set, any diffs on this field will cause the provider to
               recreate the underlying resource. Taint values can be updated safely in
               Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
               this field to manage taints. If you do, `lifecycle.ignore_changes` is
               recommended. Structure is documented below.
        :param pulumi.Input['ClusterNodeConfigWorkloadMetadataConfigArgs'] workload_metadata_config: Metadata configuration to expose to workloads on the node pool.
               Structure is documented below.
        """
        __self__.boot_disk_kms_key = boot_disk_kms_key
        __self__.disk_size_gb = disk_size_gb
        __self__.disk_type = disk_type
        __self__.guest_accelerators = guest_accelerators
        __self__.image_type = image_type
        __self__.labels = labels
        __self__.local_ssd_count = local_ssd_count
        __self__.machine_type = machine_type
        __self__.metadata = metadata
        __self__.min_cpu_platform = min_cpu_platform
        __self__.oauth_scopes = oauth_scopes
        __self__.preemptible = preemptible
        __self__.sandbox_config = sandbox_config
        __self__.service_account = service_account
        __self__.shielded_instance_config = shielded_instance_config
        __self__.tags = tags
        __self__.taints = taints
        __self__.workload_metadata_config = workload_metadata_config

@pulumi.input_type
class ClusterNodeConfigGuestAcceleratorArgs:
    count: pulumi.Input[float] = pulumi.input_property("count")
    """
    The number of the guest accelerator cards exposed to this instance.
    """
    type: pulumi.Input[str] = pulumi.input_property("type")
    """
    The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, count: pulumi.Input[float], type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[float] count: The number of the guest accelerator cards exposed to this instance.
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        __self__.count = count
        __self__.type = type

@pulumi.input_type
class ClusterNodeConfigSandboxConfigArgs:
    sandbox_type: pulumi.Input[str] = pulumi.input_property("sandboxType")
    """
    Which sandbox to use for pods in the node pool.
    Accepted values are:
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, sandbox_type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] sandbox_type: Which sandbox to use for pods in the node pool.
               Accepted values are:
        """
        __self__.sandbox_type = sandbox_type

@pulumi.input_type
class ClusterNodeConfigShieldedInstanceConfigArgs:
    enable_integrity_monitoring: Optional[pulumi.Input[bool]] = pulumi.input_property("enableIntegrityMonitoring")
    """
    Defines if the instance has integrity monitoring enabled.
    """
    enable_secure_boot: Optional[pulumi.Input[bool]] = pulumi.input_property("enableSecureBoot")
    """
    Defines if the instance has Secure Boot enabled.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None, enable_secure_boot: Optional[pulumi.Input[bool]] = None) -> None:
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines if the instance has integrity monitoring enabled.
        :param pulumi.Input[bool] enable_secure_boot: Defines if the instance has Secure Boot enabled.
        """
        __self__.enable_integrity_monitoring = enable_integrity_monitoring
        __self__.enable_secure_boot = enable_secure_boot

@pulumi.input_type
class ClusterNodeConfigTaintArgs:
    effect: pulumi.Input[str] = pulumi.input_property("effect")
    """
    Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
    """
    key: pulumi.Input[str] = pulumi.input_property("key")
    """
    Key for taint.
    """
    value: pulumi.Input[str] = pulumi.input_property("value")
    """
    Value for taint.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, effect: pulumi.Input[str], key: pulumi.Input[str], value: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] effect: Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        :param pulumi.Input[str] key: Key for taint.
        :param pulumi.Input[str] value: Value for taint.
        """
        __self__.effect = effect
        __self__.key = key
        __self__.value = value

@pulumi.input_type
class ClusterNodeConfigWorkloadMetadataConfigArgs:
    node_metadata: pulumi.Input[str] = pulumi.input_property("nodeMetadata")
    """
    How to expose the node metadata to the workload running on the node.
    Accepted values are:
    * UNSPECIFIED: Not Set
    * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
    * EXPOSE: Expose all VM metadata to pods.
    * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, node_metadata: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] node_metadata: How to expose the node metadata to the workload running on the node.
               Accepted values are:
               * UNSPECIFIED: Not Set
               * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
               * EXPOSE: Expose all VM metadata to pods.
               * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        __self__.node_metadata = node_metadata

@pulumi.input_type
class ClusterNodePoolArgs:
    autoscaling: Optional[pulumi.Input['ClusterNodePoolAutoscalingArgs']] = pulumi.input_property("autoscaling")
    initial_node_count: Optional[pulumi.Input[float]] = pulumi.input_property("initialNodeCount")
    """
    The number of nodes to create in this
    cluster's default node pool. In regional or multi-zonal clusters, this is the
    number of nodes per zone. Must be set if `node_pool` is not set. If you're using
    `container.NodePool` objects with no default node pool, you'll need to
    set this to a value of at least `1`, alongside setting
    `remove_default_node_pool` to `true`.
    """
    instance_group_urls: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("instanceGroupUrls")
    """
    List of instance group URLs which have been assigned
    to the cluster.
    """
    management: Optional[pulumi.Input['ClusterNodePoolManagementArgs']] = pulumi.input_property("management")
    max_pods_per_node: Optional[pulumi.Input[float]] = pulumi.input_property("maxPodsPerNode")
    name: Optional[pulumi.Input[str]] = pulumi.input_property("name")
    """
    The name of the cluster, unique within the project and
    location.
    """
    name_prefix: Optional[pulumi.Input[str]] = pulumi.input_property("namePrefix")
    node_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigArgs']] = pulumi.input_property("nodeConfig")
    """
    Parameters used in creating the default node pool.
    Generally, this field should not be used at the same time as a
    `container.NodePool` or a `node_pool` block; this configuration
    manages the default node pool, which isn't recommended to be used.
    Structure is documented below.
    """
    node_count: Optional[pulumi.Input[float]] = pulumi.input_property("nodeCount")
    node_locations: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("nodeLocations")
    """
    The list of zones in which the cluster's nodes
    are located. Nodes must be in the region of their regional cluster or in the
    same region as their cluster's zone for zonal clusters. If this is specified for
    a zonal cluster, omit the cluster's zone.
    """
    upgrade_settings: Optional[pulumi.Input['ClusterNodePoolUpgradeSettingsArgs']] = pulumi.input_property("upgradeSettings")
    version: Optional[pulumi.Input[str]] = pulumi.input_property("version")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, autoscaling: Optional[pulumi.Input['ClusterNodePoolAutoscalingArgs']] = None, initial_node_count: Optional[pulumi.Input[float]] = None, instance_group_urls: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, management: Optional[pulumi.Input['ClusterNodePoolManagementArgs']] = None, max_pods_per_node: Optional[pulumi.Input[float]] = None, name: Optional[pulumi.Input[str]] = None, name_prefix: Optional[pulumi.Input[str]] = None, node_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigArgs']] = None, node_count: Optional[pulumi.Input[float]] = None, node_locations: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, upgrade_settings: Optional[pulumi.Input['ClusterNodePoolUpgradeSettingsArgs']] = None, version: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[float] initial_node_count: The number of nodes to create in this
               cluster's default node pool. In regional or multi-zonal clusters, this is the
               number of nodes per zone. Must be set if `node_pool` is not set. If you're using
               `container.NodePool` objects with no default node pool, you'll need to
               set this to a value of at least `1`, alongside setting
               `remove_default_node_pool` to `true`.
        :param pulumi.Input[List[pulumi.Input[str]]] instance_group_urls: List of instance group URLs which have been assigned
               to the cluster.
        :param pulumi.Input[str] name: The name of the cluster, unique within the project and
               location.
        :param pulumi.Input['ClusterNodePoolNodeConfigArgs'] node_config: Parameters used in creating the default node pool.
               Generally, this field should not be used at the same time as a
               `container.NodePool` or a `node_pool` block; this configuration
               manages the default node pool, which isn't recommended to be used.
               Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] node_locations: The list of zones in which the cluster's nodes
               are located. Nodes must be in the region of their regional cluster or in the
               same region as their cluster's zone for zonal clusters. If this is specified for
               a zonal cluster, omit the cluster's zone.
        """
        __self__.autoscaling = autoscaling
        __self__.initial_node_count = initial_node_count
        __self__.instance_group_urls = instance_group_urls
        __self__.management = management
        __self__.max_pods_per_node = max_pods_per_node
        __self__.name = name
        __self__.name_prefix = name_prefix
        __self__.node_config = node_config
        __self__.node_count = node_count
        __self__.node_locations = node_locations
        __self__.upgrade_settings = upgrade_settings
        __self__.version = version

@pulumi.input_type
class ClusterNodePoolAutoscalingArgs:
    max_node_count: pulumi.Input[float] = pulumi.input_property("maxNodeCount")
    min_node_count: pulumi.Input[float] = pulumi.input_property("minNodeCount")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_node_count: pulumi.Input[float], min_node_count: pulumi.Input[float]) -> None:
        __self__.max_node_count = max_node_count
        __self__.min_node_count = min_node_count

@pulumi.input_type
class ClusterNodePoolManagementArgs:
    auto_repair: Optional[pulumi.Input[bool]] = pulumi.input_property("autoRepair")
    auto_upgrade: Optional[pulumi.Input[bool]] = pulumi.input_property("autoUpgrade")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, auto_repair: Optional[pulumi.Input[bool]] = None, auto_upgrade: Optional[pulumi.Input[bool]] = None) -> None:
        __self__.auto_repair = auto_repair
        __self__.auto_upgrade = auto_upgrade

@pulumi.input_type
class ClusterNodePoolNodeConfigArgs:
    boot_disk_kms_key: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskKmsKey")
    """
    The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
    """
    disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("diskSizeGb")
    """
    Size of the disk attached to each node, specified
    in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
    """
    disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("diskType")
    """
    Type of the disk attached to each node
    (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
    """
    guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]]] = pulumi.input_property("guestAccelerators")
    """
    List of the type and count of accelerator cards attached to the instance.
    Structure documented below.
    """
    image_type: Optional[pulumi.Input[str]] = pulumi.input_property("imageType")
    """
    The image type to use for this node. Note that changing the image type
    will delete and recreate all nodes in the node pool.
    """
    labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("labels")
    """
    The Kubernetes labels (key/value pairs) to be applied to each node.
    """
    local_ssd_count: Optional[pulumi.Input[float]] = pulumi.input_property("localSsdCount")
    """
    The amount of local SSD disks that will be
    attached to each cluster node. Defaults to 0.
    """
    machine_type: Optional[pulumi.Input[str]] = pulumi.input_property("machineType")
    """
    The name of a Google Compute Engine machine type.
    Defaults to `n1-standard-1`. To create a custom machine type, value should be set as specified
    [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
    """
    metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("metadata")
    """
    The metadata key/value pairs assigned to instances in
    the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
    `true` by the API; if `metadata` is set but that default value is not
    included, the provider will attempt to unset the value. To avoid this, set the
    value in your config.
    """
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    """
    Minimum CPU platform to be used by this instance.
    The instance may be scheduled on the specified or newer CPU platform. Applicable
    values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
    [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
    for more information.
    """
    oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("oauthScopes")
    """
    The set of Google API scopes to be made available
    on all of the node VMs under the "default" service account. These can be
    either FQDNs, or scope aliases. The following scopes are necessary to ensure
    the correct functioning of the cluster:
    """
    preemptible: Optional[pulumi.Input[bool]] = pulumi.input_property("preemptible")
    """
    A boolean that represents whether or not the underlying node VMs
    are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
    for more information. Defaults to false.
    """
    sandbox_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs']] = pulumi.input_property("sandboxConfig")
    """
    [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
    Structure is documented below.
    """
    service_account: Optional[pulumi.Input[str]] = pulumi.input_property("serviceAccount")
    """
    The service account to be used by the Node VMs.
    If not specified, the "default" service account is used.
    In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
    [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
    [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
    """
    shielded_instance_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs']] = pulumi.input_property("shieldedInstanceConfig")
    """
    Shielded Instance options. Structure is documented below.
    """
    tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("tags")
    """
    The list of instance tags applied to all nodes. Tags are used to identify
    valid sources or targets for network firewalls.
    """
    taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]]] = pulumi.input_property("taints")
    """
    A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
    to apply to nodes. GKE's API can only set this field on cluster creation.
    However, GKE will add taints to your nodes if you enable certain features such
    as GPUs. If this field is set, any diffs on this field will cause the provider to
    recreate the underlying resource. Taint values can be updated safely in
    Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
    this field to manage taints. If you do, `lifecycle.ignore_changes` is
    recommended. Structure is documented below.
    """
    workload_metadata_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs']] = pulumi.input_property("workloadMetadataConfig")
    """
    Metadata configuration to expose to workloads on the node pool.
    Structure is documented below.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_kms_key: Optional[pulumi.Input[str]] = None, disk_size_gb: Optional[pulumi.Input[float]] = None, disk_type: Optional[pulumi.Input[str]] = None, guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]]] = None, image_type: Optional[pulumi.Input[str]] = None, labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, local_ssd_count: Optional[pulumi.Input[float]] = None, machine_type: Optional[pulumi.Input[str]] = None, metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, min_cpu_platform: Optional[pulumi.Input[str]] = None, oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, preemptible: Optional[pulumi.Input[bool]] = None, sandbox_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs']] = None, service_account: Optional[pulumi.Input[str]] = None, shielded_instance_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs']] = None, tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, taints: Optional[pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]]] = None, workload_metadata_config: Optional[pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs']] = None) -> None:
        """
        :param pulumi.Input[str] boot_disk_kms_key: The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
        :param pulumi.Input[float] disk_size_gb: Size of the disk attached to each node, specified
               in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
        :param pulumi.Input[str] disk_type: Type of the disk attached to each node
               (e.g. 'pd-standard' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
        :param pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigGuestAcceleratorArgs']]] guest_accelerators: List of the type and count of accelerator cards attached to the instance.
               Structure documented below.
        :param pulumi.Input[str] image_type: The image type to use for this node. Note that changing the image type
               will delete and recreate all nodes in the node pool.
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] labels: The Kubernetes labels (key/value pairs) to be applied to each node.
        :param pulumi.Input[float] local_ssd_count: The amount of local SSD disks that will be
               attached to each cluster node. Defaults to 0.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type.
               Defaults to `n1-standard-1`. To create a custom machine type, value should be set as specified
               [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
        :param pulumi.Input[Dict[str, pulumi.Input[str]]] metadata: The metadata key/value pairs assigned to instances in
               the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
               `true` by the API; if `metadata` is set but that default value is not
               included, the provider will attempt to unset the value. To avoid this, set the
               value in your config.
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance.
               The instance may be scheduled on the specified or newer CPU platform. Applicable
               values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
               [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for more information.
        :param pulumi.Input[List[pulumi.Input[str]]] oauth_scopes: The set of Google API scopes to be made available
               on all of the node VMs under the "default" service account. These can be
               either FQDNs, or scope aliases. The following scopes are necessary to ensure
               the correct functioning of the cluster:
        :param pulumi.Input[bool] preemptible: A boolean that represents whether or not the underlying node VMs
               are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
               for more information. Defaults to false.
        :param pulumi.Input['ClusterNodePoolNodeConfigSandboxConfigArgs'] sandbox_config: [GKE Sandbox](https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods) configuration. When enabling this feature you must specify `image_type = "COS_CONTAINERD"` and `node_version = "1.12.7-gke.17"` or later to use it.
               Structure is documented below.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
               In order to use the configured `oauth_scopes` for logging and monitoring, the service account being used needs the
               [roles/logging.logWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_logging_roles) and
               [roles/monitoring.metricWriter](https://cloud.google.com/iam/docs/understanding-roles#stackdriver_monitoring_roles) roles.
        :param pulumi.Input['ClusterNodePoolNodeConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance options. Structure is documented below.
        :param pulumi.Input[List[pulumi.Input[str]]] tags: The list of instance tags applied to all nodes. Tags are used to identify
               valid sources or targets for network firewalls.
        :param pulumi.Input[List[pulumi.Input['ClusterNodePoolNodeConfigTaintArgs']]] taints: A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
               to apply to nodes. GKE's API can only set this field on cluster creation.
               However, GKE will add taints to your nodes if you enable certain features such
               as GPUs. If this field is set, any diffs on this field will cause the provider to
               recreate the underlying resource. Taint values can be updated safely in
               Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
               this field to manage taints. If you do, `lifecycle.ignore_changes` is
               recommended. Structure is documented below.
        :param pulumi.Input['ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs'] workload_metadata_config: Metadata configuration to expose to workloads on the node pool.
               Structure is documented below.
        """
        __self__.boot_disk_kms_key = boot_disk_kms_key
        __self__.disk_size_gb = disk_size_gb
        __self__.disk_type = disk_type
        __self__.guest_accelerators = guest_accelerators
        __self__.image_type = image_type
        __self__.labels = labels
        __self__.local_ssd_count = local_ssd_count
        __self__.machine_type = machine_type
        __self__.metadata = metadata
        __self__.min_cpu_platform = min_cpu_platform
        __self__.oauth_scopes = oauth_scopes
        __self__.preemptible = preemptible
        __self__.sandbox_config = sandbox_config
        __self__.service_account = service_account
        __self__.shielded_instance_config = shielded_instance_config
        __self__.tags = tags
        __self__.taints = taints
        __self__.workload_metadata_config = workload_metadata_config

@pulumi.input_type
class ClusterNodePoolNodeConfigGuestAcceleratorArgs:
    count: pulumi.Input[float] = pulumi.input_property("count")
    """
    The number of the guest accelerator cards exposed to this instance.
    """
    type: pulumi.Input[str] = pulumi.input_property("type")
    """
    The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, count: pulumi.Input[float], type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[float] count: The number of the guest accelerator cards exposed to this instance.
        :param pulumi.Input[str] type: The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
        """
        __self__.count = count
        __self__.type = type

@pulumi.input_type
class ClusterNodePoolNodeConfigSandboxConfigArgs:
    sandbox_type: pulumi.Input[str] = pulumi.input_property("sandboxType")
    """
    Which sandbox to use for pods in the node pool.
    Accepted values are:
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, sandbox_type: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] sandbox_type: Which sandbox to use for pods in the node pool.
               Accepted values are:
        """
        __self__.sandbox_type = sandbox_type

@pulumi.input_type
class ClusterNodePoolNodeConfigShieldedInstanceConfigArgs:
    enable_integrity_monitoring: Optional[pulumi.Input[bool]] = pulumi.input_property("enableIntegrityMonitoring")
    """
    Defines if the instance has integrity monitoring enabled.
    """
    enable_secure_boot: Optional[pulumi.Input[bool]] = pulumi.input_property("enableSecureBoot")
    """
    Defines if the instance has Secure Boot enabled.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None, enable_secure_boot: Optional[pulumi.Input[bool]] = None) -> None:
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines if the instance has integrity monitoring enabled.
        :param pulumi.Input[bool] enable_secure_boot: Defines if the instance has Secure Boot enabled.
        """
        __self__.enable_integrity_monitoring = enable_integrity_monitoring
        __self__.enable_secure_boot = enable_secure_boot

@pulumi.input_type
class ClusterNodePoolNodeConfigTaintArgs:
    effect: pulumi.Input[str] = pulumi.input_property("effect")
    """
    Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
    """
    key: pulumi.Input[str] = pulumi.input_property("key")
    """
    Key for taint.
    """
    value: pulumi.Input[str] = pulumi.input_property("value")
    """
    Value for taint.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, effect: pulumi.Input[str], key: pulumi.Input[str], value: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] effect: Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
        :param pulumi.Input[str] key: Key for taint.
        :param pulumi.Input[str] value: Value for taint.
        """
        __self__.effect = effect
        __self__.key = key
        __self__.value = value

@pulumi.input_type
class ClusterNodePoolNodeConfigWorkloadMetadataConfigArgs:
    node_metadata: pulumi.Input[str] = pulumi.input_property("nodeMetadata")
    """
    How to expose the node metadata to the workload running on the node.
    Accepted values are:
    * UNSPECIFIED: Not Set
    * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
    * EXPOSE: Expose all VM metadata to pods.
    * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, node_metadata: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] node_metadata: How to expose the node metadata to the workload running on the node.
               Accepted values are:
               * UNSPECIFIED: Not Set
               * SECURE: Prevent workloads not in hostNetwork from accessing certain VM metadata, specifically kube-env, which contains Kubelet credentials, and the instance identity token. See [Metadata Concealment](https://cloud.google.com/kubernetes-engine/docs/how-to/metadata-proxy) documentation.
               * EXPOSE: Expose all VM metadata to pods.
               * GKE_METADATA_SERVER: Enables [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) on the node.
        """
        __self__.node_metadata = node_metadata

@pulumi.input_type
class ClusterNodePoolUpgradeSettingsArgs:
    max_surge: pulumi.Input[float] = pulumi.input_property("maxSurge")
    max_unavailable: pulumi.Input[float] = pulumi.input_property("maxUnavailable")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_surge: pulumi.Input[float], max_unavailable: pulumi.Input[float]) -> None:
        __self__.max_surge = max_surge
        __self__.max_unavailable = max_unavailable

@pulumi.input_type
class ClusterPodSecurityPolicyConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterPrivateClusterConfigArgs:
    enable_private_endpoint: pulumi.Input[bool] = pulumi.input_property("enablePrivateEndpoint")
    """
    When `true`, the cluster's private
    endpoint is used as the cluster endpoint and access through the public endpoint
    is disabled. When `false`, either endpoint can be used. This field only applies
    to private clusters, when `enable_private_nodes` is `true`.
    """
    enable_private_nodes: Optional[pulumi.Input[bool]] = pulumi.input_property("enablePrivateNodes")
    """
    Enables the private cluster feature,
    creating a private endpoint on the cluster. In a private cluster, nodes only
    have RFC 1918 private addresses and communicate with the master's private
    endpoint via private networking.
    """
    master_global_access_config: Optional[pulumi.Input['ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs']] = pulumi.input_property("masterGlobalAccessConfig")
    master_ipv4_cidr_block: Optional[pulumi.Input[str]] = pulumi.input_property("masterIpv4CidrBlock")
    """
    The IP range in CIDR notation to use for
    the hosted master network. This range will be used for assigning private IP
    addresses to the cluster master(s) and the ILB VIP. This range must not overlap
    with any other ranges in use within the cluster's network, and it must be a /28
    subnet. See [Private Cluster Limitations](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#limitations)
    for more details. This field only applies to private clusters, when
    `enable_private_nodes` is `true`.
    """
    peering_name: Optional[pulumi.Input[str]] = pulumi.input_property("peeringName")
    """
    The name of the peering between this cluster and the Google owned VPC.
    """
    private_endpoint: Optional[pulumi.Input[str]] = pulumi.input_property("privateEndpoint")
    """
    The internal IP address of this cluster's master endpoint.
    """
    public_endpoint: Optional[pulumi.Input[str]] = pulumi.input_property("publicEndpoint")
    """
    The external IP address of this cluster's master endpoint.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enable_private_endpoint: pulumi.Input[bool], enable_private_nodes: Optional[pulumi.Input[bool]] = None, master_global_access_config: Optional[pulumi.Input['ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs']] = None, master_ipv4_cidr_block: Optional[pulumi.Input[str]] = None, peering_name: Optional[pulumi.Input[str]] = None, private_endpoint: Optional[pulumi.Input[str]] = None, public_endpoint: Optional[pulumi.Input[str]] = None) -> None:
        """
        :param pulumi.Input[bool] enable_private_endpoint: When `true`, the cluster's private
               endpoint is used as the cluster endpoint and access through the public endpoint
               is disabled. When `false`, either endpoint can be used. This field only applies
               to private clusters, when `enable_private_nodes` is `true`.
        :param pulumi.Input[bool] enable_private_nodes: Enables the private cluster feature,
               creating a private endpoint on the cluster. In a private cluster, nodes only
               have RFC 1918 private addresses and communicate with the master's private
               endpoint via private networking.
        :param pulumi.Input[str] master_ipv4_cidr_block: The IP range in CIDR notation to use for
               the hosted master network. This range will be used for assigning private IP
               addresses to the cluster master(s) and the ILB VIP. This range must not overlap
               with any other ranges in use within the cluster's network, and it must be a /28
               subnet. See [Private Cluster Limitations](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#limitations)
               for more details. This field only applies to private clusters, when
               `enable_private_nodes` is `true`.
        :param pulumi.Input[str] peering_name: The name of the peering between this cluster and the Google owned VPC.
        :param pulumi.Input[str] private_endpoint: The internal IP address of this cluster's master endpoint.
        :param pulumi.Input[str] public_endpoint: The external IP address of this cluster's master endpoint.
        """
        __self__.enable_private_endpoint = enable_private_endpoint
        __self__.enable_private_nodes = enable_private_nodes
        __self__.master_global_access_config = master_global_access_config
        __self__.master_ipv4_cidr_block = master_ipv4_cidr_block
        __self__.peering_name = peering_name
        __self__.private_endpoint = private_endpoint
        __self__.public_endpoint = public_endpoint

@pulumi.input_type
class ClusterPrivateClusterConfigMasterGlobalAccessConfigArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterReleaseChannelArgs:
    channel: pulumi.Input[str] = pulumi.input_property("channel")
    """
    The selected release channel.
    Accepted values are:
    * UNSPECIFIED: Not set.
    * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
    * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
    * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, channel: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] channel: The selected release channel.
               Accepted values are:
               * UNSPECIFIED: Not set.
               * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
               * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
               * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
        """
        __self__.channel = channel

@pulumi.input_type
class ClusterResourceUsageExportConfigArgs:
    bigquery_destination: pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs'] = pulumi.input_property("bigqueryDestination")
    """
    Parameters for using BigQuery as the destination of resource usage export.
    """
    enable_network_egress_metering: Optional[pulumi.Input[bool]] = pulumi.input_property("enableNetworkEgressMetering")
    """
    Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created
    in the cluster to meter network egress traffic.
    """
    enable_resource_consumption_metering: Optional[pulumi.Input[bool]] = pulumi.input_property("enableResourceConsumptionMetering")
    """
    Whether to enable resource
    consumption metering on this cluster. When enabled, a table will be created in
    the resource export BigQuery dataset to store resource consumption data. The
    resulting table can be joined with the resource usage table or with BigQuery
    billing export. Defaults to `true`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, bigquery_destination: pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs'], enable_network_egress_metering: Optional[pulumi.Input[bool]] = None, enable_resource_consumption_metering: Optional[pulumi.Input[bool]] = None) -> None:
        """
        :param pulumi.Input['ClusterResourceUsageExportConfigBigqueryDestinationArgs'] bigquery_destination: Parameters for using BigQuery as the destination of resource usage export.
        :param pulumi.Input[bool] enable_network_egress_metering: Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created
               in the cluster to meter network egress traffic.
        :param pulumi.Input[bool] enable_resource_consumption_metering: Whether to enable resource
               consumption metering on this cluster. When enabled, a table will be created in
               the resource export BigQuery dataset to store resource consumption data. The
               resulting table can be joined with the resource usage table or with BigQuery
               billing export. Defaults to `true`.
        """
        __self__.bigquery_destination = bigquery_destination
        __self__.enable_network_egress_metering = enable_network_egress_metering
        __self__.enable_resource_consumption_metering = enable_resource_consumption_metering

@pulumi.input_type
class ClusterResourceUsageExportConfigBigqueryDestinationArgs:
    dataset_id: pulumi.Input[str] = pulumi.input_property("datasetId")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, dataset_id: pulumi.Input[str]) -> None:
        __self__.dataset_id = dataset_id

@pulumi.input_type
class ClusterVerticalPodAutoscalingArgs:
    enabled: pulumi.Input[bool] = pulumi.input_property("enabled")
    """
    Enable the PodSecurityPolicy controller for this cluster.
    If enabled, pods must be valid under a PodSecurityPolicy to be created.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enabled: pulumi.Input[bool]) -> None:
        """
        :param pulumi.Input[bool] enabled: Enable the PodSecurityPolicy controller for this cluster.
               If enabled, pods must be valid under a PodSecurityPolicy to be created.
        """
        __self__.enabled = enabled

@pulumi.input_type
class ClusterWorkloadIdentityConfigArgs:
    identity_namespace: pulumi.Input[str] = pulumi.input_property("identityNamespace")
    """
    Currently, the only supported identity namespace is the project's default.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, identity_namespace: pulumi.Input[str]) -> None:
        """
        :param pulumi.Input[str] identity_namespace: Currently, the only supported identity namespace is the project's default.
        """
        __self__.identity_namespace = identity_namespace

@pulumi.input_type
class NodePoolAutoscalingArgs:
    max_node_count: pulumi.Input[float] = pulumi.input_property("maxNodeCount")
    """
    Maximum number of nodes in the NodePool. Must be >= min_node_count.
    """
    min_node_count: pulumi.Input[float] = pulumi.input_property("minNodeCount")
    """
    Minimum number of nodes in the NodePool. Must be >=0 and
    <= `max_node_count`.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_node_count: pulumi.Input[float], min_node_count: pulumi.Input[float]) -> None:
        """
        :param pulumi.Input[float] max_node_count: Maximum number of nodes in the NodePool. Must be >= min_node_count.
        :param pulumi.Input[float] min_node_count: Minimum number of nodes in the NodePool. Must be >=0 and
               <= `max_node_count`.
        """
        __self__.max_node_count = max_node_count
        __self__.min_node_count = min_node_count

@pulumi.input_type
class NodePoolManagementArgs:
    auto_repair: Optional[pulumi.Input[bool]] = pulumi.input_property("autoRepair")
    """
    Whether the nodes will be automatically repaired.
    """
    auto_upgrade: Optional[pulumi.Input[bool]] = pulumi.input_property("autoUpgrade")
    """
    Whether the nodes will be automatically upgraded.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, auto_repair: Optional[pulumi.Input[bool]] = None, auto_upgrade: Optional[pulumi.Input[bool]] = None) -> None:
        """
        :param pulumi.Input[bool] auto_repair: Whether the nodes will be automatically repaired.
        :param pulumi.Input[bool] auto_upgrade: Whether the nodes will be automatically upgraded.
        """
        __self__.auto_repair = auto_repair
        __self__.auto_upgrade = auto_upgrade

@pulumi.input_type
class NodePoolNodeConfigArgs:
    boot_disk_kms_key: Optional[pulumi.Input[str]] = pulumi.input_property("bootDiskKmsKey")
    disk_size_gb: Optional[pulumi.Input[float]] = pulumi.input_property("diskSizeGb")
    disk_type: Optional[pulumi.Input[str]] = pulumi.input_property("diskType")
    guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigGuestAcceleratorArgs']]]] = pulumi.input_property("guestAccelerators")
    image_type: Optional[pulumi.Input[str]] = pulumi.input_property("imageType")
    labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("labels")
    local_ssd_count: Optional[pulumi.Input[float]] = pulumi.input_property("localSsdCount")
    machine_type: Optional[pulumi.Input[str]] = pulumi.input_property("machineType")
    metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = pulumi.input_property("metadata")
    min_cpu_platform: Optional[pulumi.Input[str]] = pulumi.input_property("minCpuPlatform")
    oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("oauthScopes")
    preemptible: Optional[pulumi.Input[bool]] = pulumi.input_property("preemptible")
    sandbox_config: Optional[pulumi.Input['NodePoolNodeConfigSandboxConfigArgs']] = pulumi.input_property("sandboxConfig")
    service_account: Optional[pulumi.Input[str]] = pulumi.input_property("serviceAccount")
    shielded_instance_config: Optional[pulumi.Input['NodePoolNodeConfigShieldedInstanceConfigArgs']] = pulumi.input_property("shieldedInstanceConfig")
    tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = pulumi.input_property("tags")
    taints: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigTaintArgs']]]] = pulumi.input_property("taints")
    workload_metadata_config: Optional[pulumi.Input['NodePoolNodeConfigWorkloadMetadataConfigArgs']] = pulumi.input_property("workloadMetadataConfig")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, boot_disk_kms_key: Optional[pulumi.Input[str]] = None, disk_size_gb: Optional[pulumi.Input[float]] = None, disk_type: Optional[pulumi.Input[str]] = None, guest_accelerators: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigGuestAcceleratorArgs']]]] = None, image_type: Optional[pulumi.Input[str]] = None, labels: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, local_ssd_count: Optional[pulumi.Input[float]] = None, machine_type: Optional[pulumi.Input[str]] = None, metadata: Optional[pulumi.Input[Dict[str, pulumi.Input[str]]]] = None, min_cpu_platform: Optional[pulumi.Input[str]] = None, oauth_scopes: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, preemptible: Optional[pulumi.Input[bool]] = None, sandbox_config: Optional[pulumi.Input['NodePoolNodeConfigSandboxConfigArgs']] = None, service_account: Optional[pulumi.Input[str]] = None, shielded_instance_config: Optional[pulumi.Input['NodePoolNodeConfigShieldedInstanceConfigArgs']] = None, tags: Optional[pulumi.Input[List[pulumi.Input[str]]]] = None, taints: Optional[pulumi.Input[List[pulumi.Input['NodePoolNodeConfigTaintArgs']]]] = None, workload_metadata_config: Optional[pulumi.Input['NodePoolNodeConfigWorkloadMetadataConfigArgs']] = None) -> None:
        __self__.boot_disk_kms_key = boot_disk_kms_key
        __self__.disk_size_gb = disk_size_gb
        __self__.disk_type = disk_type
        __self__.guest_accelerators = guest_accelerators
        __self__.image_type = image_type
        __self__.labels = labels
        __self__.local_ssd_count = local_ssd_count
        __self__.machine_type = machine_type
        __self__.metadata = metadata
        __self__.min_cpu_platform = min_cpu_platform
        __self__.oauth_scopes = oauth_scopes
        __self__.preemptible = preemptible
        __self__.sandbox_config = sandbox_config
        __self__.service_account = service_account
        __self__.shielded_instance_config = shielded_instance_config
        __self__.tags = tags
        __self__.taints = taints
        __self__.workload_metadata_config = workload_metadata_config

@pulumi.input_type
class NodePoolNodeConfigGuestAcceleratorArgs:
    count: pulumi.Input[float] = pulumi.input_property("count")
    type: pulumi.Input[str] = pulumi.input_property("type")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, count: pulumi.Input[float], type: pulumi.Input[str]) -> None:
        __self__.count = count
        __self__.type = type

@pulumi.input_type
class NodePoolNodeConfigSandboxConfigArgs:
    sandbox_type: pulumi.Input[str] = pulumi.input_property("sandboxType")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, sandbox_type: pulumi.Input[str]) -> None:
        __self__.sandbox_type = sandbox_type

@pulumi.input_type
class NodePoolNodeConfigShieldedInstanceConfigArgs:
    enable_integrity_monitoring: Optional[pulumi.Input[bool]] = pulumi.input_property("enableIntegrityMonitoring")
    enable_secure_boot: Optional[pulumi.Input[bool]] = pulumi.input_property("enableSecureBoot")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None, enable_secure_boot: Optional[pulumi.Input[bool]] = None) -> None:
        __self__.enable_integrity_monitoring = enable_integrity_monitoring
        __self__.enable_secure_boot = enable_secure_boot

@pulumi.input_type
class NodePoolNodeConfigTaintArgs:
    effect: pulumi.Input[str] = pulumi.input_property("effect")
    key: pulumi.Input[str] = pulumi.input_property("key")
    value: pulumi.Input[str] = pulumi.input_property("value")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, effect: pulumi.Input[str], key: pulumi.Input[str], value: pulumi.Input[str]) -> None:
        __self__.effect = effect
        __self__.key = key
        __self__.value = value

@pulumi.input_type
class NodePoolNodeConfigWorkloadMetadataConfigArgs:
    node_metadata: pulumi.Input[str] = pulumi.input_property("nodeMetadata")

    # pylint: disable=no-self-argument
    def __init__(__self__, *, node_metadata: pulumi.Input[str]) -> None:
        __self__.node_metadata = node_metadata

@pulumi.input_type
class NodePoolUpgradeSettingsArgs:
    max_surge: pulumi.Input[float] = pulumi.input_property("maxSurge")
    """
    The number of additional nodes that can be added to the node pool during
    an upgrade. Increasing `max_surge` raises the number of nodes that can be upgraded simultaneously.
    Can be set to 0 or greater.
    """
    max_unavailable: pulumi.Input[float] = pulumi.input_property("maxUnavailable")
    """
    The number of nodes that can be simultaneously unavailable during
    an upgrade. Increasing `max_unavailable` raises the number of nodes that can be upgraded in
    parallel. Can be set to 0 or greater.
    """

    # pylint: disable=no-self-argument
    def __init__(__self__, *, max_surge: pulumi.Input[float], max_unavailable: pulumi.Input[float]) -> None:
        """
        :param pulumi.Input[float] max_surge: The number of additional nodes that can be added to the node pool during
               an upgrade. Increasing `max_surge` raises the number of nodes that can be upgraded simultaneously.
               Can be set to 0 or greater.
        :param pulumi.Input[float] max_unavailable: The number of nodes that can be simultaneously unavailable during
               an upgrade. Increasing `max_unavailable` raises the number of nodes that can be upgraded in
               parallel. Can be set to 0 or greater.
        """
        __self__.max_surge = max_surge
        __self__.max_unavailable = max_unavailable

